<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhichengkou.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhicheng">
<meta property="og:url" content="http://zhichengkou.github.io/index.html">
<meta property="og:site_name" content="Zhicheng">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zhicheng">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://zhichengkou.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Zhicheng</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhicheng</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="post-title-link" itemprop="url">仿人机器人</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-09 11:39:01 / 修改时间：11:39:21" itemprop="dateCreated datePublished" datetime="2025-04-09T11:39:01+08:00">2025-04-09</time>
            </span>

          
            <span id="/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="post-meta-item leancloud_visitors" data-flag-title="仿人机器人" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">强化学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-08 19:17:22 / 修改时间：19:22:25" itemprop="dateCreated datePublished" datetime="2025-04-08T19:17:22+08:00">2025-04-08</time>
            </span>

          
            <span id="/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="rl研究方向">RL研究方向</h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 40%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr>
<th>研究方向</th>
<th>原理关键词</th>
<th>典型应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>深度强化学习</td>
<td>深度网络 + Q/V/策略逼近</td>
<td>游戏 AI、自动驾驶、机器人</td>
</tr>
<tr>
<td>模型预测 RL</td>
<td>学习环境模型 + 规划</td>
<td>控制系统、医疗、能源优化</td>
</tr>
<tr>
<td>多智能体 RL</td>
<td>协同/博弈学习</td>
<td>群体机器人、博弈系统</td>
</tr>
<tr>
<td>离线 RL</td>
<td>静态数据训练 + 分布偏差处理</td>
<td>医疗、金融、推荐系统</td>
</tr>
<tr>
<td>元强化学习</td>
<td>快速适应新任务</td>
<td>跨任务机器人、个性化推荐</td>
</tr>
<tr>
<td>模仿/逆强化学习</td>
<td>专家示范学习 / 奖励反推</td>
<td>自动驾驶、人类行为建模</td>
</tr>
<tr>
<td>安全强化学习</td>
<td>加入风险或硬约束优化目标</td>
<td>自动驾驶、电力安全、飞行器</td>
</tr>
<tr>
<td>RL + 控制</td>
<td>稳定性分析 + 策略优化</td>
<td>动态系统控制、智能制造</td>
</tr>
<tr>
<td>RLHF</td>
<td>人类反馈建模 + 策略微调</td>
<td>对话系统、生成式 AI 优化</td>
</tr>
</tbody>
</table>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">强化学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-07 19:48:01" itemprop="dateCreated datePublished" datetime="2025-04-07T19:48:01+08:00">2025-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-09 10:16:00" itemprop="dateModified" datetime="2025-04-09T10:16:00+08:00">2025-04-09</time>
              </span>

          
            <span id="/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="on-policy-和-off-policy-的区别">On-policy 和 Off-policy
的区别</h2>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 37%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr>
<th>特征</th>
<th><strong>On-policy (SARSA)</strong></th>
<th><strong>Off-policy (Q-learning)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>行为策略与目标策略的关系</strong></td>
<td>行为策略与目标策略相同</td>
<td>行为策略和目标策略不同</td>
</tr>
<tr>
<td><strong>策略选择依据</strong></td>
<td>代理在每个状态下根据当前策略选择动作</td>
<td>代理根据探索性策略选择动作，但更新时使用目标策略（贪婪策略）</td>
</tr>
<tr>
<td><strong>学习更新</strong></td>
<td>通过选择的动作 ( A ) 和下一个动作 ( A' ) 来更新 Q 值</td>
<td>通过选择的动作 ( A ) 和下一个状态的最优动作来更新 Q 值</td>
</tr>
<tr>
<td><strong>例子</strong></td>
<td>使用 <strong>(
)-贪婪策略</strong>，代理每次选择当前策略下的动作，并在下一个状态上选择动作
( A' ) 来更新 Q 值。</td>
<td>使用 <strong>( )-贪婪策略</strong>
进行探索，但更新时始终选择最优动作来更新 Q 值。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="三种方法对比总结"><strong>三种方法对比总结</strong></h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>更新方式</th>
<th>适用场景</th>
<th>是否需要环境模型</th>
<th>计算量</th>
<th>收敛速度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>蒙特卡洛</strong></td>
<td>只有回合结束后更新</td>
<td>适用于无模型、离线学习</td>
<td><strong>不需要</strong></td>
<td><strong>高</strong></td>
<td><strong>慢</strong></td>
</tr>
<tr>
<td><strong>时序差分（TD）</strong></td>
<td>每一步都更新</td>
<td>适用于无模型、在线学习</td>
<td><strong>不需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
<tr>
<td><strong>动态规划（DP）</strong></td>
<td>通过转移概率计算更新</td>
<td>适用于已知环境</td>
<td><strong>需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
</tbody>
</table>
<ol type="1">
<li><strong>蒙特卡洛（MC）</strong>：必须 <strong>等待回合结束</strong>
才能更新，不适用于实时学习。</li>
<li><strong>时序差分（TD）</strong>：<strong>每一步都更新</strong>，比蒙特卡洛更快，适用于
<strong>在线学习</strong>。</li>
<li><strong>动态规划（DP）</strong>：需要
<strong>环境模型</strong>（转移概率），但计算最精确，适用于
<strong>已知环境</strong>。</li>
</ol>
<p>在强化学习中，<strong>TD
学习是最常用的，因为它既能在线更新，又不依赖于环境模型</strong>。蒙特卡洛主要用于
<strong>离线学习</strong>，而动态规划适用于
<strong>已知环境</strong>（如象棋等确定性环境）。</p>
<hr />
<h2 id="同轨策略-vs.-离轨策略">同轨策略 vs. 离轨策略</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>同轨策略（On-policy）</strong></th>
<th><strong>离轨策略（Off-policy）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数据来源</strong></td>
<td>当前策略</td>
<td>其他策略</td>
</tr>
<tr>
<td><strong>更新的策略</strong></td>
<td>使用同一个策略更新</td>
<td>使用不同策略更新</td>
</tr>
<tr>
<td><strong>典型算法</strong></td>
<td>SARSA、Actor-Critic、REINFORCE</td>
<td>Q-learning、DQN</td>
</tr>
<tr>
<td><strong>探索方式</strong></td>
<td>需要手动引入探索（如 ε-贪心）</td>
<td>训练目标本身包含最优策略</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>相对较慢</td>
<td>相对较快</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>同轨策略（On-policy）</strong> 更像是一个人在
<strong>边学边做</strong>，不断根据自己的经验来调整行为。</li>
<li><strong>离轨策略（Off-policy）</strong> 更像是一个人
<strong>看着别人的经验</strong> 来学习，并且能更快收敛到最优解。</li>
</ul>
<hr />
<table>
<colgroup>
<col style="width: 4%" />
<col style="width: 45%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>更新公式</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>[ V(s_t) V(s_t) + ]</td>
<td>通过当前状态的价值和下一个状态的价值来更新，估计状态的价值。仅依赖下一个状态的值来进行更新，适用于
<strong>状态价值估计</strong>。</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>使用当前策略进行学习，依赖于当前的动作选择，<strong>当前状态和动作</strong>以及<strong>下一个状态和动作</strong>共同更新
Q 值，适用于 <strong>策略学习</strong>。</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>通过选择最大 Q
值的动作进行学习，无论当前策略如何，只关心未来的最佳动作，适用于
<strong>最优策略学习</strong>。</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>结合了 TD(0)
和蒙特卡洛方法，回溯多步更新，适用于长期学习，特别是在复杂任务中，考虑到
<strong>多步未来的反馈</strong>。</td>
</tr>
</tbody>
</table>
<h3
id="为什么sarsa进行的是策略学习而非状态价值估计">为什么SARSA进行的是策略学习而非状态价值估计？</h3>
<ol type="1">
<li><p><strong>动作价值函数（( Q(s, a) )）的学习</strong>：
SARSA的目标是学习动作价值函数 ( Q(s, a) )，而不是单独学习状态价值函数 (
V(s)
)。这是因为在强化学习中，我们关心的通常是<strong>在某个状态下采取某个动作后能获得的回报</strong>，而不仅仅是状态本身的“价值”。动作价值函数能够直接反映在某个状态下采取特定动作的好坏。</p></li>
<li><p><strong>SARSA是基于策略的学习</strong>：
SARSA算法通过采取当前策略下的动作，并根据当前策略的行为来估计未来的回报。这意味着，SARSA本质上是在<strong>同轨策略</strong>下学习，并通过当前策略来选择动作。因此，它不仅关注状态本身，而是通过状态-动作对（(
s_t, a_t )）来进行学习。</p>
<ul>
<li>在每个时间步，SARSA都会通过当前策略选择动作（比如 (
)-贪婪策略），然后更新相应的 ( Q(s, a) )。</li>
<li>而对于<strong>状态价值函数</strong>（( V(s)
)），它只是通过所有可能动作的加权平均来得到的，因此，如果我们只学习状态价值而不学习动作价值，我们就无法通过具体的动作决策来直接优化策略。</li>
</ul></li>
<li><p><strong>策略学习与状态价值的关系</strong>： 状态价值函数 ( V(s) )
只能在策略已知的情况下计算，而在SARSA中，策略本身是通过<strong>持续更新的动作价值函数</strong>来间接优化的。SARSA不直接估计状态价值函数，而是通过(
Q(s, a) )来间接影响策略的改进。</p></li>
<li><p><strong>为什么动作价值比状态价值更有意义？</strong>：
在强化学习中，我们的目标通常是<strong>最大化回报</strong>，而回报的最大化依赖于在每个状态下选择合适的动作。学习<strong>状态价值</strong>只是帮助我们知道一个状态本身的好坏，但我们仍然需要知道在每个状态下应该采取什么样的动作来优化回报。这就是为什么SARSA（以及其他基于值的策略学习算法）会直接学习<strong>动作价值函数</strong>。</p>
<ul>
<li>在<strong>策略迭代</strong>或<strong>值迭代</strong>中，策略是通过状态值函数来更新的，但SARSA通过动作值函数来优化策略，因为动作选择比状态值更直接影响回报。</li>
</ul></li>
</ol>
<h3 id="sarsa与状态价值估计的关系">SARSA与状态价值估计的关系</h3>
<p>尽管SARSA本身不直接估计状态价值函数 ( V(s)
)，但通过学习<strong>动作价值函数</strong>（( Q(s, a)
)），我们可以间接地得到状态价值函数 ( V(s)
)。实际上，状态价值函数可以通过对所有可能动作的加权平均来计算： [ V(s) =
_a (a|s) Q(s, a) ] 其中，( (a|s) ) 是在状态 ( s ) 下采取动作 ( a )
的概率。通过这种方式，尽管SARSA不直接估计状态价值函数，但它间接提供了可以计算状态价值函数的所有信息。</p>
<h3 id="总结">总结：</h3>
<p>SARSA算法专注于学习<strong>动作价值函数</strong>（( Q(s, a)
)），而不是直接估计状态价值函数 ( V(s)
)。这是因为在强化学习中，目标是优化策略，通过学习每个状态-动作对的价值来选择最佳动作。SARSA通过直接估计<strong>每个状态下的动作价值</strong>，而不是仅仅估计状态本身的价值，从而能够更有效地学习和改进策略。</p>
<h3 id="常用-td-方法的区别"><strong>常用 TD 方法的区别</strong></h3>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 27%" />
<col style="width: 18%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>策略类型</th>
<th>更新方式</th>
<th>目标</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>同轨</td>
<td>仅依赖当前状态的价值和下一个状态的价值进行更新</td>
<td>估计状态价值函数</td>
<td>在线实时学习，适用于估计状态值</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>同轨</td>
<td>当前动作和下一个动作共同更新 Q 值</td>
<td>学习当前策略的 Q 值</td>
<td>行为安全，如机器人、自动驾驶</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>离轨</td>
<td>最大化下一个状态的 Q 值进行更新</td>
<td>学习最优策略的 Q 值</td>
<td>追求最优策略，游戏、围棋 AI</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>同轨</td>
<td>结合 TD(0) 和蒙特卡洛方法，回溯多步更新</td>
<td>估计状态-动作值函数</td>
<td>长期学习，适用于复杂任务</td>
</tr>
</tbody>
</table>
<h3 id="对比说明"><strong>对比说明：</strong></h3>
<ul>
<li><strong>TD(0)</strong>：只考虑一个时间步的更新，适用于简单的状态价值估计。</li>
<li><strong>SARSA</strong>：基于当前策略，更新状态-动作值 Q(s,
a)，适用于<strong>在线学习</strong>和<strong>行为安全</strong>。</li>
<li><strong>Q-learning</strong>：选择未来最大 Q
值的动作更新状态-动作值，适用于追求<strong>最优策略</strong>。</li>
<li><strong>TD(λ)</strong>：通过多个时间步回溯来更新，结合了 TD(0)
和蒙特卡洛方法，适用于 <strong>长期学习</strong>和
<strong>复杂任务</strong>。</li>
</ul>
<hr />
<p>Dyna-Q 算法家族是基于强化学习中
<strong>将真实经验与模型模拟经验结合</strong>
的一类算法。它们融合了<strong>模型学习（Model-based）</strong>与<strong>价值函数更新（如
Q-learning）</strong>，具有较高的样本效率。</p>
<p>以下是 Dyna-Q
及其常见变体的对比表格，包括其核心思路、改进点、是否探索模型变化等关键特征。</p>
<hr />
<h2 id="dyna-q-及其主要变体对比表">Dyna-Q 及其主要变体对比表</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 24%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr>
<th>算法名称</th>
<th>核心思想</th>
<th>是否使用模型</th>
<th>是否增强探索</th>
<th>是否处理非定常环境</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dyna-Q</strong></td>
<td>基础版本，将 Q-learning 与模拟经验结合</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>学习效率高、结构简单</td>
<td>模型质量差时可能误导学习</td>
</tr>
<tr>
<td><strong>Dyna-Q+</strong></td>
<td>在模型中加入“探索奖励”，鼓励模拟冷门状态-动作对</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>增强探索能力、适应非定常性</td>
<td>增加超参数（探索奖励函数）</td>
</tr>
<tr>
<td><strong>Prioritized Dyna-Q</strong></td>
<td>使用优先级队列优先更新影响大的状态</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>❌ 否</td>
<td>更高效率、优先更新重要经验</td>
<td>实现复杂度稍高</td>
</tr>
<tr>
<td><strong>Dyna-2</strong></td>
<td>在模型中使用多个估计模型进行模拟（不确定建模）</td>
<td>✅ 多模型</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>对不确定性建模，有利于探索与适应</td>
<td>存储和计算量大</td>
</tr>
<tr>
<td><strong>Dyna-Q with Linear Function Approximation</strong></td>
<td>使用线性函数逼近代替 Q 表</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>✅ 视模型</td>
<td>可扩展到大状态空间</td>
<td>精度依赖函数逼近器质量</td>
</tr>
<tr>
<td><strong>Dyna-Q with Neural Networks</strong></td>
<td>用神经网络逼近 Q 值和环境模型</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>可用于连续空间、复杂环境</td>
<td>容易不稳定，需要技巧调参</td>
</tr>
</tbody>
</table>
<h3 id="dyna-q">1. <strong>Dyna-Q</strong></h3>
<ul>
<li>基础版本，结合真实交互 + 模拟经验。</li>
<li>每次从模型中随机选择过往 (s, a)，进行“虚拟更新”。</li>
</ul>
<h3 id="dyna-q-1">2. <strong>Dyna-Q+</strong></h3>
<ul>
<li>解决 Dyna-Q 早期探索不足的问题。</li>
<li>给模型中<strong>长时间未访问的 (s, a)</strong> 加一个额外奖励：</li>
</ul>
<p>[ = k ]</p>
<p>其中 τ 是未访问的时间步数，k 是超参数。</p>
<h3 id="prioritized-dyna-q">3. <strong>Prioritized Dyna-Q</strong></h3>
<ul>
<li>模拟更新不再随机，而是基于“状态更新的重要性”进行排序更新（如 TD
误差越大，越优先）。</li>
<li>使用优先队列实现，更高效。</li>
</ul>
<h3 id="dyna-2">4. <strong>Dyna-2</strong></h3>
<ul>
<li>使用多个不同的模型（表示环境可能的变化），每次模拟时从这些模型中采样。</li>
<li>更适合非定常或不确定环境（如博弈、对手变换等）。</li>
</ul>
<h3 id="函数逼近版本线性-神经网络">5. <strong>函数逼近版本（线性 /
神经网络）</strong></h3>
<ul>
<li>替代表格型 Q 值，用函数逼近器（如神经网络）逼近 Q
值或环境动态。</li>
<li>适用于大规模/连续状态空间的环境。</li>
</ul>
<h2 id="总结推荐不同需求">总结推荐（不同需求）</h2>
<table>
<thead>
<tr>
<th>使用场景</th>
<th>推荐算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>小型离散环境</td>
<td>Dyna-Q / Dyna-Q+</td>
</tr>
<tr>
<td>希望强化探索、避免早收敛</td>
<td>Dyna-Q+</td>
</tr>
<tr>
<td>数据有限，希望学习快</td>
<td>Prioritized Dyna-Q</td>
</tr>
<tr>
<td>非定常环境（如对手策略变化）</td>
<td>Dyna-Q+ 或 Dyna-2</td>
</tr>
<tr>
<td>状态空间较大 / 连续</td>
<td>函数逼近版 Dyna-Q</td>
</tr>
<tr>
<td>有强大计算资源+深度学习</td>
<td>Dyna-Q with Neural Networks</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="第一部分总结">第一部分总结</h3>
<p>在强化学习中，<strong>时间差分（TD）</strong>方法、<strong>动态规划（DP）</strong>方法和<strong>蒙特卡洛（MC）</strong>方法是三种常见的策略评估和优化技术。每种方法都有其适用的场景，并且它们的优缺点也有所不同。下面是对这三种方法的简要概述，以及它们分别适合的场景。</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 30%" />
<col style="width: 30%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD（时间差分）</strong></td>
<td>在线学习、实时学习、大规模状态空间、部分可观察的环境</td>
<td>能够在线更新，适合实时学习，计算效率高</td>
<td>收敛慢，对初始值敏感，可能产生偏差</td>
</tr>
<tr>
<td><strong>DP（动态规划）</strong></td>
<td>已知完整环境模型、离线学习、有限状态空间</td>
<td>能找到最优策略和最优值函数，理论精确</td>
<td>需要完整的环境模型，不适用于大规模状态空间</td>
</tr>
<tr>
<td><strong>MC（蒙特卡洛）</strong></td>
<td>完全可观察的环境、回合结束后更新、离线学习</td>
<td>不需要环境模型，只需回报进行更新，适合复杂环境</td>
<td>需要等待回合结束，不适合在线学习，计算低效（尤其状态空间大时）</td>
</tr>
</tbody>
</table>
<hr />
<p>在强化学习中，<strong>基于函数逼近的同轨策略预测</strong>（On-Policy
Prediction with Function
Approximation）是指在使用<strong>函数逼近</strong>来估计值函数时，采用与当前策略相同的轨迹来进行学习。这种方法通常是为了处理大规模或连续的状态空间，其中使用表格型方法来表示值函数变得不再可行。</p>
<h3 id="同轨策略on-policy">1.
<strong>同轨策略（On-Policy）</strong>：</h3>
<ul>
<li><strong>同轨策略</strong>意味着学习过程中使用的策略（通常是行为策略）和用于估计状态值函数的策略是相同的。在这种方法中，智能体根据当前策略与环境进行交互并收集经验，而使用这些经验来更新策略的值函数。</li>
</ul>
<h3 id="函数逼近function-approximation">2. <strong>函数逼近（Function
Approximation）</strong>：</h3>
<ul>
<li><strong>函数逼近</strong>是为了处理大规模状态空间（或者连续空间），通过使用参数化函数（如线性函数、神经网络等）来逼近值函数（如状态值函数
( V(s) ) 或动作值函数 ( Q(s, a)
)），避免使用表格形式存储所有可能的状态或动作。</li>
<li>函数逼近可以帮助将值函数从一个巨大的状态空间简化为一个相对较小的参数空间，从而能够在更大的环境中进行有效的学习。</li>
</ul>
<h3 id="同轨策略预测的内容">3.
<strong>同轨策略预测的内容</strong>：</h3>
<p>在基于函数逼近的同轨策略预测中，主要包括以下几个方面：</p>
<h4 id="使用td时间差分方法进行同轨策略预测">1.
<strong>使用TD（时间差分）方法进行同轨策略预测</strong>：</h4>
<ul>
<li><p>同轨策略预测的经典方法是使用<strong>TD方法</strong>，例如<strong>TD(0)</strong>，在这种方法中，值函数是通过逐步更新的。</p></li>
<li><p>通过<strong>TD(0)</strong> 或
<strong>SARSA</strong>（基于同轨策略的TD方法），智能体根据其当前策略进行状态转移并计算每一步的回报。随着时间的推移，智能体不断根据回报对其值函数进行更新。</p>
<p>典型的更新公式为： [ V(s_t) V(s_t) + ] 其中：</p>
<ul>
<li>( s_t ) 是当前状态，</li>
<li>( r_{t+1} ) 是当前奖励，</li>
<li>( V(s_{t+1}) ) 是下一个状态的估计值，</li>
<li>( ) 是折扣因子，</li>
<li>( ) 是学习率。</li>
</ul></li>
</ul>
<h4 id="线性函数逼近">2. <strong>线性函数逼近</strong>：</h4>
<ul>
<li>在使用<strong>线性函数逼近</strong>时，值函数 ( V(s) )
被表示为状态特征的线性组合： [ V(s) = ^T (s) ] 其中 ( ) 是参数向量，(
(s) ) 是状态 ( s ) 的特征向量。</li>
<li>通过<strong>梯度下降</strong>或其他优化方法，智能体学习合适的参数 (
) 来逼近状态的值函数。</li>
</ul>
<h4 id="非线性函数逼近例如神经网络">3.
<strong>非线性函数逼近</strong>（例如神经网络）：</h4>
<ul>
<li>对于更加复杂的任务，尤其是当状态空间非常大且不容易用线性函数表示时，可以使用<strong>神经网络</strong>作为函数逼近器。神经网络可以通过多层结构捕捉状态和价值之间的复杂关系。</li>
<li><strong>深度Q网络（DQN）</strong>是一个经典的例子，它使用深度神经网络来逼近动作值函数
( Q(s, a) )，并通过同轨策略进行学习。</li>
</ul>
<h4 id="策略评估和更新">4. <strong>策略评估和更新</strong>：</h4>
<ul>
<li>在基于函数逼近的同轨策略预测中，智能体不断评估当前策略的表现并使用TD方法更新其值函数。同时，策略也会基于这些估计进行调整，以改善智能体的表现。</li>
</ul>
<h4 id="基于梯度的更新">5. <strong>基于梯度的更新</strong>：</h4>
<ul>
<li><p>对于非线性逼近（如神经网络），值函数的更新通常通过<strong>梯度下降</strong>进行。智能体根据策略评估的结果（即对状态值函数或动作值函数的估计），通过反向传播算法来更新函数逼近器的参数。</p>
<p>例如，在使用神经网络时，参数 ( )
会根据损失函数的梯度进行调整，以最小化预测值与真实回报之间的差距。</p></li>
</ul>
<h3 id="同轨策略预测的经典算法">4.
<strong>同轨策略预测的经典算法</strong>：</h3>
<p>以下是一些常见的使用函数逼近的同轨策略预测算法：</p>
<h4 id="sarsaλ">(1) <strong>SARSA(λ)</strong>：</h4>
<ul>
<li><strong>SARSA(λ)</strong>
是一种结合了<strong>时间差分学习（TD）</strong>和<strong>跟踪误差</strong>的算法，使用λ-回溯来进行更长期的估计。</li>
<li>使用类似于TD(0)的更新规则，但通过加权历史的TD误差来改进更新过程，从而获得更快的收敛。</li>
</ul>
<h4 id="深度q学习dqn">(2) <strong>深度Q学习（DQN）</strong>：</h4>
<ul>
<li><strong>DQN</strong>
是一种结合了<strong>Q-learning</strong>和<strong>深度神经网络</strong>的方法，使用神经网络逼近动作值函数
( Q(s, a) )。</li>
<li>在DQN中，策略是基于贪心策略（即选择最大Q值的动作），并且通过训练神经网络来更新Q值。</li>
</ul>
<h4 id="a3c异步优势演员-评论家">(3)
<strong>A3C（异步优势演员-评论家）</strong>：</h4>
<ul>
<li><strong>A3C</strong>
是一种基于<strong>策略梯度</strong>的强化学习算法，使用两个网络（演员和评论家）来估计策略和状态值函数。A3C采用了多线程训练，在多个环境中同时训练，来提高学习效率。</li>
</ul>
<h3 id="优缺点分析">5. <strong>优缺点分析</strong>：</h3>
<h4 id="优点">优点：</h4>
<ul>
<li><strong>大规模问题</strong>：基于函数逼近的方法可以处理大规模或连续的状态空间，不需要每个状态都保存一个值。</li>
<li><strong>适用动态环境</strong>：与表格型方法不同，基于函数逼近的方法可以在动态和复杂的环境中工作。</li>
<li><strong>灵活性</strong>：可以使用线性和非线性函数逼近（如神经网络），提供了更大的灵活性来应对各种环境。</li>
</ul>
<h4 id="缺点">缺点：</h4>
<ul>
<li><strong>收敛性问题</strong>：当使用非线性函数逼近时（例如神经网络），可能会面临不稳定性和收敛性问题。</li>
<li><strong>过拟合</strong>：如果模型过于复杂，可能会过拟合训练数据，导致泛化能力差。</li>
<li><strong>需要大量计算</strong>：使用深度学习等方法时，计算成本和训练时间可能非常高。</li>
</ul>
<h3 id="总结-1">总结：</h3>
<p><strong>基于函数逼近的同轨策略预测</strong>利用当前策略下的经验来更新值函数或策略，适用于大规模或连续的状态空间，并使用函数逼近（如线性函数或神经网络）来解决表格型方法无法处理的问题。常见的算法包括SARSA(λ)、DQN和A3C等。</p>
<hr />
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 36%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>对比维度</th>
<th>TD(0)（时序差分）</th>
<th>LSTD（最小二乘TD）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方法类型</strong></td>
<td>增量式更新算法</td>
<td>批量方法 / 解线性方程组</td>
</tr>
<tr>
<td><strong>是否使用采样</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>是否使用bootstrapping</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>慢，需多次遍历</td>
<td>快，一次遍历即可</td>
</tr>
<tr>
<td><strong>收敛稳定性</strong></td>
<td>对学习率敏感，较不稳定</td>
<td>稳定性高</td>
</tr>
<tr>
<td><strong>是否需要学习率（α）</strong></td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td><strong>是否需要存储所有数据</strong></td>
<td>否（可在线更新）</td>
<td>是（需存储一批数据计算矩阵）</td>
</tr>
<tr>
<td><strong>估计误差类型</strong></td>
<td>噪声大，估计不准确</td>
<td>最小化TD误差平方，估计精确</td>
</tr>
<tr>
<td><strong>函数逼近类型</strong></td>
<td>支持线性与非线性</td>
<td>仅适用于线性函数逼近</td>
</tr>
<tr>
<td><strong>计算代价</strong></td>
<td>低（每步更新）</td>
<td>高（求解矩阵逆）</td>
</tr>
<tr>
<td><strong>是否可在线使用</strong></td>
<td>是</td>
<td>否（需收集一批样本）</td>
</tr>
<tr>
<td><strong>是否适合大规模状态空间</strong></td>
<td>是</td>
<td>否（矩阵求逆开销大，维度高时困难）</td>
</tr>
<tr>
<td><strong>常见用途</strong></td>
<td>在线学习、环境交互</td>
<td>批量预测、策略评估</td>
</tr>
<tr>
<td><strong>结果可解释性</strong></td>
<td>随机性大，结果随步长波动</td>
<td>更平滑、可解释性好</td>
</tr>
</tbody>
</table>
<h3 id="总结推荐">总结推荐：</h3>
<ul>
<li><strong>TD(0)</strong>：适合在线学习、环境交互、资源有限时使用。算法简单，适合实时任务。</li>
<li><strong>LSTD</strong>：适合批量数据分析、预测任务，或者需要快速稳定收敛的场景，但计算复杂度高，适合线性逼近小维度问题。</li>
</ul>
<hr />
<h1
id="第九章基于函数逼近的同轨策略预测">第九章：基于函数逼近的同轨策略预测</h1>
<h2 id="核心背景为什么要函数逼近">核心背景：为什么要函数逼近？</h2>
<blockquote>
<p>如果希望强化学习系统能够应用于人工智能和大型工程应用中，则系统必须有能力进行<strong>泛化</strong>。</p>
</blockquote>
<p>强化学习传统方法（比如表格型
Q-learning）依赖于将每个状态或状态-动作对显式地存储在表格中。当状态空间很大或是连续的时候（比如自动驾驶、围棋、股票市场），这种方法就无法胜任了。</p>
<p>因此我们需要<strong>函数逼近器（function
approximator）</strong>，来对价值函数进行泛化，比如： - 线性模型 -
多项式函数 - 神经网络（即深度学习） - RBF（径向基函数） - 瓦片编码（tile
coding）</p>
<hr />
<h2
id="核心思想用监督学习方法逼近值函数">核心思想：用监督学习方法逼近值函数</h2>
<blockquote>
<p>只要将每次更新时涉及的二元组 <code>(s → g)</code>
作为训练样本就可以了。</p>
</blockquote>
<p>在强化学习中，我们在每一步都可以得到一个样本： <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state s → 目标值 g</span><br></pre></td></tr></table></figure>
其中目标值 g 可能是： -
蒙特卡洛返回：<code>G = r1 + r2 + ... + rn</code> - TD
返回：<code>r + γ * V(s')</code></p>
<p>这些“输入→目标值”本质上和<strong>监督学习</strong>里的训练对没什么不同。</p>
<p>所以我们可以借用监督学习的工具（比如神经网络训练）来拟合这个过程。</p>
<hr />
<h2 id="什么是同轨策略on-policy">💡 什么是同轨策略（on-policy）？</h2>
<blockquote>
<p>我们定义均方价值误差 VE(w) 为对使用权值向量 w 的近似价值函数
<code>v̂_w(s)</code> 的加权误差。</p>
</blockquote>
<p>在强化学习中，<strong>同轨策略</strong>是指： - 使用的策略 π
既用于<strong>生成数据</strong>（交互环境），又用于<strong>评估/学习</strong>。</p>
<p>相比之下，<strong>异轨策略 off-policy</strong>（比如
Q-learning）是指用一个行为策略采样，但学习的是另一个目标策略。</p>
<p><code>VE(w)</code> 衡量的是：近似函数 <code>v̂_w(s)</code>
相对于真实状态值 <code>v_π(s)</code> 的误差，通常定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VE(w) = ∑ μ(s) * (v̂_w(s) - v_π(s))²</span><br></pre></td></tr></table></figure>
<p>其中 μ(s) 是状态的访问概率（stationary distribution）。</p>
<hr />
<h2 id="如何优化这个误差">📉 如何优化这个误差？</h2>
<blockquote>
<p>最常见的方法是随机梯度下降（SGD）及其变种</p>
</blockquote>
<p>我们希望通过调整参数 <code>w</code> 来让 <code>VE(w)</code>
尽可能小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w ← w - α * ∇VE(w)</span><br></pre></td></tr></table></figure>
<p>但问题是，我们<strong>不知道</strong> <code>v_π(s)</code>
的真实值，所以不能直接计算 ∇VE(w)。</p>
<p>于是我们使用<strong>TD目标或蒙特卡洛目标来估计</strong>误差，从而构造所谓的<strong>半梯度算法（semi-gradient
methods）</strong>。</p>
<hr />
<h2 id="什么是半梯度方法为什么叫这个名">🧮
什么是“半梯度方法”？为什么叫这个名？</h2>
<blockquote>
<p>在这种自举法（如DP）中，权值向量在更新目标中出现，但计算梯度时不考虑它</p>
</blockquote>
<p>“半梯度”意思是：你在 TD 学习中更新的目标 <code>r + γ * v̂_w(s')</code>
是带有 <code>w</code>
的，但你却<strong>不对这个目标求梯度</strong>。你只对
<code>v̂_w(s)</code> 这一部分求梯度。</p>
<h3 id="示例">示例：</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delta = r + γ * v̂_w(s<span class="string">&#x27;) - v̂_w(s)   # TD误差</span></span><br><span class="line"><span class="string">w ← w + α * delta * ∇v̂_w(s)</span></span><br></pre></td></tr></table></figure>
<p>注意：目标中有
<code>v̂_w(s')</code>，但我们更新时<strong>不对它求导</strong>，所以叫“半梯度”。</p>
<hr />
<h2 id="线性函数逼近最重要的一个基类">🔢
线性函数逼近（最重要的一个基类）</h2>
<blockquote>
<p>线性函数逼近中，值估计是特征加权求和：<code>v̂(s) = φ(s)^T w</code></p>
</blockquote>
<p>这是强化学习中最容易分析和收敛性最好的逼近方式。</p>
<ul>
<li><code>φ(s)</code> 是状态的特征向量（比如 one-hot
编码、瓦片编码、RBF等）</li>
<li><code>w</code> 是权值向量（参数）</li>
<li>优点：分析简单，收敛性好，计算快</li>
</ul>
<hr />
<h2 id="特征选择的策略">🎨 特征选择的策略？</h2>
<blockquote>
<p>选择好的特征是加先验知识的一种重要方式</p>
</blockquote>
<p>特征越好，泛化效果越好。 - <strong>多项式特征</strong>（如
<code>1, x, x²</code>）通常不太适合RL -
<strong>傅立叶基</strong>：适合周期性问题（比如控制摆动） -
<strong>瓦片编码（Tile
coding）</strong>：非常适合离散近似连续空间，计算快 -
<strong>径向基函数（RBF）</strong>：适用于平滑响应的低维问题</p>
<hr />
<h2 id="不同方法对比">📚 不同方法对比</h2>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
<th>数据效率</th>
<th>计算复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>半梯度 TD(0)</strong></td>
<td>单步更新，自举</td>
<td>中等</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>n步TD</strong></td>
<td>使用 n 步回报</td>
<td>高</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>蒙特卡洛（MC）</strong></td>
<td>用整幕回报更新</td>
<td>高</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>LSTD</strong></td>
<td>最数据高效，最精确</td>
<td>高</td>
<td>O(d²)</td>
</tr>
<tr>
<td><strong>非线性逼近（NN）</strong></td>
<td>深度RL，适合复杂任务</td>
<td>取决于模型</td>
<td>高</td>
</tr>
</tbody>
</table>
<blockquote>
<p>d：权值数量（特征维度）</p>
</blockquote>
<hr />
<h2 id="深度强化学习非线性逼近">🚀 深度强化学习：非线性逼近</h2>
<blockquote>
<p>用神经网络表示价值函数或策略，常见于 DQN、Actor-Critic 等方法。</p>
</blockquote>
<p>训练方法： - 使用反向传播算法 - 通常使用经验回放 + 目标网络等机制</p>
<hr />
<h2 id="总结核心点">✅ 总结核心点</h2>
<ol type="1">
<li>大状态空间 → 必须使用函数逼近</li>
<li>同轨策略 → 策略评估任务中，常用半梯度方法</li>
<li>线性函数逼近 → 理论最清晰、收敛性最强</li>
<li>特征选择 → 是模型性能的核心关键</li>
<li>深度强化学习 → 实现非线性逼近，适用于复杂任务</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/29/KKT%E6%9D%A1%E4%BB%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/29/KKT%E6%9D%A1%E4%BB%B6/" class="post-title-link" itemprop="url">KKT条件</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-29 23:16:47" itemprop="dateCreated datePublished" datetime="2025-03-29T23:16:47+08:00">2025-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-07 19:53:11" itemprop="dateModified" datetime="2025-04-07T19:53:11+08:00">2025-04-07</time>
              </span>

          
            <span id="/2025/03/29/KKT%E6%9D%A1%E4%BB%B6/" class="post-meta-item leancloud_visitors" data-flag-title="KKT条件" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>KKT（Karush-Kuhn-Tucker）条件是非线性优化中最重要的最优性条件之一，主要用于<strong>求解约束优化问题</strong>。它是<strong>拉格朗日乘子法</strong>的推广，适用于<strong>不等式约束</strong>的情况。</p>
<hr />
<h2 id="kkt-条件的作用"><strong>1. KKT 条件的作用</strong></h2>
<p>KKT 条件的主要作用包括：</p>
<h3 id="判断最优解"><strong>（1）判断最优解</strong></h3>
<ul>
<li>任何一个可行解如果满足 KKT
条件，并且某些附加条件（如凸性条件）成立，则该解是<strong>最优解</strong>。</li>
<li>在优化问题中，满足 KKT 条件的点是局部最优解的候选点。</li>
</ul>
<h3
id="转化约束优化为无约束优化"><strong>（2）转化约束优化为无约束优化</strong></h3>
<ul>
<li>通过引入<strong>拉格朗日乘子</strong>，KKT
条件将<strong>有约束问题</strong>转化为<strong>等式条件和不等式条件的组合</strong>，从而使问题更易于求解。</li>
</ul>
<h3
id="广泛应用于机器学习和控制领域"><strong>（3）广泛应用于机器学习和控制领域</strong></h3>
<ul>
<li><strong>支持向量机（SVM）</strong>：在对偶优化中使用 KKT
条件来找到最优超平面。</li>
<li><strong>最优控制</strong>：KKT
条件用于求解优化控制问题，特别是在<strong>MPC（模型预测控制）</strong>和<strong>强化学习</strong>中常见。</li>
<li><strong>经济学和运筹学</strong>：用于资源分配、均衡分析等。</li>
</ul>
<hr />
<h2 id="kkt-条件的数学表达"><strong>2. KKT 条件的数学表达</strong></h2>
<p>假设要优化的问题如下：</p>
<p>[ _{x} f(x) ]</p>
<p><strong>约束条件：</strong> [ g_i(x) , i = 1, , m ] [ h_j(x) = 0, j =
1, , p ]</p>
<p>其中： - ( f(x) ) 是目标函数 - ( g_i(x) ) 是不等式约束 - ( h_j(x) )
是等式约束</p>
<h3 id="kkt-条件由以下四部分组成"><strong>KKT
条件由以下四部分组成：</strong></h3>
<h4 id="可行性条件primal-feasibility"><strong>（1）可行性条件（Primal
Feasibility）</strong></h4>
<p>[ g_i(x^<em>) , h_j(x^</em>) = 0 ] 即，最优解 ( x^* )
必须满足约束。</p>
<h4
id="梯度条件stationarity"><strong>（2）梯度条件（Stationarity）</strong></h4>
<p>[ f(x^*) + _{i=1}^{m} _i g_i(x^*) + _{j=1}^{p} _j h_j(x^*) = 0 ]
即，最优解的梯度（导数）必须满足拉格朗日条件。</p>
<h4
id="互补松弛条件complementary-slackness"><strong>（3）互补松弛条件（Complementary
Slackness）</strong></h4>
<p>[ _i g_i(x^<em>) = 0, i ] - 如果 ( g_i(x^</em>) &lt; 0
)（严格小于），则 ( _i = 0 )（对应的拉格朗日乘子必须为 0）。 - 如果 (
g_i(x^*) = 0 )（刚好等于），则 ( _i ) 可以非零。</p>
<p>这个条件保证了只有<strong>激活的约束（等于 0
的不等式约束）</strong>才会影响最优解。</p>
<h4
id="对偶可行性条件dual-feasibility"><strong>（4）对偶可行性条件（Dual
Feasibility）</strong></h4>
<p>[ _i , i ] 即，拉格朗日乘子必须是非负的。</p>
<hr />
<h2 id="kkt-条件的直观理解"><strong>3. KKT 条件的直观理解</strong></h2>
<ul>
<li><strong>KKT 约束可行性：</strong> 解决方案必须满足原始约束。</li>
<li><strong>KKT 站点条件：</strong>
目标函数在最优点的梯度必须与约束的梯度平衡。</li>
<li><strong>KKT 互补松弛：</strong>
如果某个不等式约束没有紧贴（即严格小于 0），那么它不会影响优化（乘子为
0）。</li>
<li><strong>KKT 对偶可行性：</strong>
约束的拉格朗日乘子必须是非负的。</li>
</ul>
<p>可以把 KKT 条件想象成一个“拉扯”系统： - 目标函数 ( f(x) )
想要往最优方向移动 - 约束 ( g(x) ) 和 ( h(x) ) 就像“绳子”一样在拉住它 -
KKT 条件确保这些力达到平衡，找到最优点</p>
<hr />
<h2 id="kkt-条件的适用范围"><strong>4. KKT 条件的适用范围</strong></h2>
<p>KKT 条件适用于： - <strong>凸优化问题</strong>（KKT
条件是充分必要条件） - <strong>非凸优化问题</strong>（KKT
条件是必要条件，但不一定充分）</p>
<hr />
<h2 id="kkt-条件的应用"><strong>5. KKT 条件的应用</strong></h2>
<h3 id="支持向量机svm"><strong>（1）支持向量机（SVM）</strong></h3>
<p>在 SVM 的优化问题中，我们最小化： [ _w | w |^2 ] 同时满足分类约束： [
y_i (w^T x_i + b) , i ] 用 KKT
条件求解对偶问题后，可以得到支持向量和优化的超平面。</p>
<h3 id="模型预测控制mpc"><strong>（2）模型预测控制（MPC）</strong></h3>
<p>MPC 通过求解一个受约束的优化问题来控制系统的行为，KKT
条件确保最优控制输入满足物理约束。</p>
<h3 id="强化学习"><strong>（3）强化学习</strong></h3>
<p>在安全强化学习中，KKT 条件用于优化受约束的策略。</p>
<hr />
<p>KKT（Karush-Kuhn-Tucker）条件在<strong>MPC（模型预测控制）</strong>和<strong>强化学习（RL）</strong>中主要用于优化问题的约束处理和优化解的计算。以下是它们的具体应用：</p>
<hr />
<h2 id="kkt-在-mpc-中的应用"><strong>1. KKT 在 MPC
中的应用</strong></h2>
<h3 id="作用">** 作用：**</h3>
<p>在 MPC（Model Predictive
Control）中，优化问题通常是一个带约束的最优控制问题，KKT
条件用于求解最优控制序列。</p>
<h3 id="典型优化问题mpc-公式"><strong>🔹 典型优化问题（MPC
公式）</strong></h3>
<p>MPC 通过求解如下约束优化问题来找到最优控制 ( u_t )： [ <em>{u_0, ,
u</em>{N-1}} <em>{t=0}^{N-1} (x_t, u_t) ] <strong>约束条件：</strong> -
<strong>状态更新方程（系统动力学）：</strong><br />
[ x</em>{t+1} = f(x_t, u_t) ] -
<strong>输入约束（控制约束）：</strong><br />
[ u_{} u_t u_{} ] - <strong>状态约束（物理限制）：</strong><br />
[ x_{} x_t x_{} ]</p>
<h3 id="kkt-在-mpc-中的作用"><strong>🔹 KKT 在 MPC
中的作用</strong></h3>
<p>MPC
问题通常是一个<strong>二次规划（QP）或非线性规划（NLP）</strong>问题，KKT
条件用于求解最优解： 1. <strong>拉格朗日函数：</strong> [ =
<em>{t=0}^{N-1} (x_t, u_t) + </em>{t=0}^{N-1} <em>t^T (f(x_t, u_t) -
x</em>{t+1}) + _{t=0}^{N-1} _t^T g(x_t, u_t) ] - ( _t )
是<strong>状态约束的拉格朗日乘子</strong> - ( _t )
是<strong>控制约束的拉格朗日乘子</strong></p>
<ol start="2" type="1">
<li><strong>KKT 最优性条件：</strong>
<ul>
<li><strong>一阶必要条件（梯度为 0）</strong></li>
<li><strong>互补松弛条件</strong> ( _t^T g(x_t, u_t) = 0 )
处理不等式约束</li>
<li><strong>可行性条件</strong> 保证约束满足</li>
</ul></li>
<li><strong>应用 KKT 解法：</strong>
<ul>
<li>如果是<strong>线性 MPC</strong>，可转换为 QP
问题，直接用<strong>二次规划求解器（QP solver）</strong>解 KKT
条件。</li>
<li>如果是<strong>非线性
MPC（NMPC）</strong>，需要用<strong>梯度下降</strong>、<strong>牛顿法</strong>或<strong>内点法</strong>来满足
KKT 条件。</li>
</ul></li>
</ol>
<h3 id="例子无人机轨迹优化"><strong>🔹
例子：无人机轨迹优化</strong></h3>
<p><strong>问题</strong>：设定一个无人机从起点到终点的最优轨迹，MPC
控制无人机在速度、加速度、动力约束下移动。 -
<strong>目标函数</strong>：最小化能源消耗和航行时间 -
<strong>约束</strong>： - 无人机的最大速度 ( v_{} ) -
避障约束（无人机不能撞到障碍物） -
动力学约束（无人机受牛顿运动方程约束）</p>
<p>在这种情况下，MPC 的优化问题会涉及约束最优控制，KKT
条件用于求解最优控制输入 ( u_t )。</p>
<hr />
<h2 id="kkt-在-强化学习rl-中的应用"><strong>2. KKT 在 强化学习（RL）
中的应用</strong></h2>
<h3 id="作用-1">** 作用：**</h3>
<p>在 RL 中，KKT 主要用于<strong>安全强化学习（Safe
RL）</strong>和<strong>约束马尔可夫决策过程（CMDP）</strong>，用于优化策略时处理约束。</p>
<h3 id="典型优化问题强化学习中的约束优化"><strong>🔹
典型优化问题（强化学习中的约束优化）</strong></h3>
<p>在强化学习中，目标是找到最优策略 ( ) 来最大化累积奖励： [ <em>{} ]
但如果存在<strong>约束条件</strong>（例如安全性约束），则问题变成： [
</em>{} J(), C_i() d_i, i=1,2, ] 其中： - ( C_i() ) 代表第 ( i )
个约束（例如碰撞率、能耗等）。 - ( d_i ) 是最大允许值。</p>
<h3 id="kkt-在强化学习中的作用"><strong>🔹 KKT
在强化学习中的作用</strong></h3>
<p>强化学习中的<strong>策略优化（Policy
Optimization）</strong>问题通常是一个<strong>约束优化问题</strong>，可以用
KKT 处理： 1. <strong>拉格朗日松弛（Lagrangian Relaxation）</strong> -
构造拉格朗日函数： [ (, ) = J() - _{i} _i (C_i() - d_i) ] - 其中，( _i )
是<strong>拉格朗日乘子</strong>，表示约束的惩罚权重。</p>
<ol start="2" type="1">
<li><strong>策略优化 + KKT</strong>
<ul>
<li>用梯度下降优化策略： [ _(, ) = 0 ]</li>
<li>用<strong>互补松弛条件</strong>更新乘子： [ _i (C_i() - d_i) = 0 ]
<ul>
<li>若约束未触发（( C_i() &lt; d_i )），则 ( _i = 0
)（约束无影响）。</li>
<li>若约束触发（( C_i() &gt; d_i )），则 ( _i ) 增大，惩罚约束。</li>
</ul></li>
</ul></li>
</ol>
<h3 id="例子机器人导航中的-safe-rl"><strong>🔹 例子：机器人导航中的 Safe
RL</strong></h3>
<p><strong>问题</strong>：一个机器人需要在障碍物区域中找到最优路径，同时确保不会撞上障碍物。
- <strong>目标</strong>：最大化到达目标点的概率 -
<strong>约束</strong>： - 避免碰撞概率 ( P() ) - 能量消耗 ( E() )</p>
<p><strong>解决方案</strong>： - 传统 RL
可能找到高奖励的策略，但不满足安全约束。 - <strong>使用 KKT
和拉格朗日方法</strong>，在策略更新时<strong>调整约束惩罚项</strong>，确保训练的策略既高效又安全。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95LSTD-Q/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95LSTD-Q/" class="post-title-link" itemprop="url">强化学习算法LSTD-Q</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-03-21 19:18:35 / 修改时间：19:20:52" itemprop="dateCreated datePublished" datetime="2025-03-21T19:18:35+08:00">2025-03-21</time>
            </span>

          
            <span id="/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95LSTD-Q/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习算法LSTD-Q" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="lstd-q-中最小二乘法的体现"><strong>🔍 LSTD-Q
中最小二乘法的体现</strong></h3>
<p><strong>LSTD-Q（Least-Squares Temporal Difference
Q-learning）</strong> 之所以叫
“Least-Squares”，就是因为它通过<strong>最小二乘法（Least
Squares）</strong> 来求解最优参数 <code>θ</code>，从而逼近 Q 函数。</p>
<p>在代码中，最小二乘法体现在： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.theta = np.linalg.solve(<span class="variable language_">self</span>.A, <span class="variable language_">self</span>.b)</span><br></pre></td></tr></table></figure>
这实际上是<strong>解最小二乘问题</strong>的核心步骤，类似于线性回归的求解方式。</p>
<hr />
<h2 id="数学推导最小二乘法在-lstd-q-中的应用"><strong>📌
数学推导：最小二乘法在 LSTD-Q 中的应用</strong></h2>
<p>LSTD-Q 的目标是找到一个 <strong>近似 Q 值函数</strong>： [ Q(s, a) =
(s, a)^T ] 其中： - ( Q(s, a) ) 是 Q 值，我们用<strong>特征向量</strong>
( (s, a) ) 的线性组合来逼近它。 - ( ) 是待求解的参数。</p>
<hr />
<h3 id="目标找到最优-θ-使得-lstd-误差最小"><strong>1️⃣ 目标：找到最优
<code>θ</code> 使得 LSTD 误差最小</strong></h3>
<p>基于 <strong>贝尔曼方程</strong>： [ Q(s, a) = r + Q(s', a') ]
我们用<strong>最小二乘法</strong>逼近 Q 值，最优 <code>θ</code>
需要满足： [ <em>{} ( Q(s, a) - (r + Q(s', a')) )^2 ] 展开后可以写成： [
</em>{} ( (s, a)^T - (r + (s', a')^T ) )^2 ]</p>
<hr />
<h3 id="线性方程组的构造"><strong>2️⃣ 线性方程组的构造</strong></h3>
如果我们有 <strong>N 组数据</strong> ( (s_i, a_i, r_i, s'_i, a'_i)
)，记： - ( ) 是所有 ( (s, a) ) 组成的矩阵： [ =
<span class="math display">\[\begin{bmatrix}
  \phi(s_1, a_1)^T \\
  \phi(s_2, a_2)^T \\
  \vdots \\
  \phi(s_N, a_N)^T
  \end{bmatrix}\]</span>
<p>] 形状是 ( (N, d) )，其中 ( d ) 是特征维度。</p>
<ul>
<li><p>( R ) 是奖励向量： [ R =</p>
<span class="math display">\[\begin{bmatrix}
r_1 \\
r_2 \\
\vdots \\
r_N
\end{bmatrix}\]</span>
<p>] 形状是 ( (N, 1) )。</p></li>
<li><p>( ' ) 是所有 ( (s', a') ) 组成的矩阵： [ ' =</p>
<span class="math display">\[\begin{bmatrix}
\phi(s&#39;_1, a&#39;_1)^T \\
\phi(s&#39;_2, a&#39;_2)^T \\
\vdots \\
\phi(s&#39;_N, a&#39;_N)^T
\end{bmatrix}\]</span>
<p>] 形状也是 ( (N, d) )。</p></li>
</ul>
<p>将 Q 值近似公式代入，我们得到线性方程： [ = R + ' ] 整理得到： [ (^T
- ^T ') = ^T R ]</p>
<hr />
<h3 id="代码如何实现最小二乘法"><strong>3️⃣
代码如何实现最小二乘法</strong></h3>
<p>在代码中，这个方程被表示为： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.A += np.outer(phi, (phi - <span class="variable language_">self</span>.gamma * next_phi))  <span class="comment"># 更新 A</span></span><br><span class="line"><span class="variable language_">self</span>.b += reward * phi  <span class="comment"># 更新 b</span></span><br><span class="line"><span class="variable language_">self</span>.theta = np.linalg.solve(<span class="variable language_">self</span>.A, <span class="variable language_">self</span>.b)  <span class="comment"># 计算 theta</span></span><br></pre></td></tr></table></figure> - <code>self.A</code> 对应
( (^T - ^T ') ) - <code>self.b</code> 对应 ( ^T R ) -
<code>np.linalg.solve(self.A, self.b)</code>
<strong>解线性方程组，求解最优 <code>θ</code></strong></p>
<p>这就是 <strong>LSTD-Q 里最小二乘法的核心！</strong> 🎯</p>
<hr />
<h2 id="直观理解"><strong>🚀 直观理解</strong></h2>
<ul>
<li><strong>普通 Q-learning</strong> 是直接用 TD
误差来<strong>逐步更新</strong> Q
值，但它可能收敛慢，而且受学习率影响大。</li>
<li><strong>LSTD-Q</strong>
是<strong>直接求解一个线性方程组</strong>，用最小二乘法<strong>一次性计算出最优
<code>θ</code></strong>，收敛快，精度高，适用于高维状态空间。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%ADQ%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%ADQ%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">强化学习中Q矩阵计算方法的总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-21 17:41:26" itemprop="dateCreated datePublished" datetime="2025-03-21T17:41:26+08:00">2025-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-07 19:52:39" itemprop="dateModified" datetime="2025-04-07T19:52:39+08:00">2025-04-07</time>
              </span>

          
            <span id="/2025/03/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%ADQ%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E7%9A%84%E6%80%BB%E7%BB%93/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习中Q矩阵计算方法的总结" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于q-函数的逼近"><strong>关于Q 函数的逼近</strong></h1>
<hr />
<h2 id="线性逼近方法least-squares-类">1. 线性逼近方法（Least Squares
类）</h2>
<p>这些方法假设 <strong>Q
函数可以由状态-动作特征的线性组合表示</strong>： <span
class="math display">\[
Q(s, a) = \phi(s, a)^T \theta
\]</span> 其中： - <span class="math inline">\(\phi(s, a)\)</span>
是状态-动作的特征向量 - <span class="math inline">\(\theta\)</span>
是权重参数</p>
<h3 id="lstd-qleast-squares-td-q-learning"><strong>🔹 1.1
LSTD-Q（Least-Squares TD-Q Learning）</strong></h3>
<p><strong>方法：</strong> 通过<strong>最小二乘法</strong>解线性方程： $
(A - A') = b $ - 适用于<strong>小规模问题</strong>，计算成本较低。 -
<strong>缺点：</strong> 计算 <span class="math inline">\((
A^{-1})\)</span> 可能不稳定（需要正则化）。</p>
<h3 id="lspileast-squares-policy-iteration"><strong>🔹 1.2
LSPI（Least-Squares Policy Iteration）</strong></h3>
<p><strong>方法：</strong> 结合 <strong>LSTD-Q
和策略迭代</strong>，不断更新策略： 1. <strong>用 LSTD-Q 逼近 Q
函数</strong> 2. <strong>用贪心策略更新动作选择</strong> 3.
<strong>循环迭代，直到收敛</strong></p>
<p><strong>特点：</strong> - <strong>比 LSTD-Q 更稳定</strong> -
<strong>适用于策略迭代（Policy Iteration）类问题</strong> -
<strong>不适用于连续控制问题</strong></p>
<hr />
<h3 id="fitted-q-iterationfqi"><strong>🔹 1.3 Fitted-Q
Iteration（FQI）</strong></h3>
<p><strong>方法：</strong> 用监督学习的方法<strong>拟合 Q
函数</strong>： 1. 采集大量 <span class="math inline">\((s, a, r,
s&#39;)\)</span> 经验数据 2. 用回归方法学习 <span
class="math inline">\(Q(s, a) \approx r + \gamma \max_{a&#39;} Q(s&#39;,
a&#39;)\)</span> 3. 迭代更新，直到 Q 值收敛</p>
<p><strong>特点：</strong> - 适用于<strong>高维状态空间</strong> - 可用
<strong>决策树、随机森林、神经网络等</strong> 逼近 Q 函数 - <strong>比
LSTD-Q 更灵活</strong></p>
<hr />
<h2 id="非线性逼近方法深度学习">2. 非线性逼近方法（深度学习）</h2>
<p>当状态空间维度很高（如图像、文本），线性方法效果较差，我们通常采用
<strong>非线性方法</strong> 来逼近 Q 函数。</p>
<h3 id="dqndeep-q-network"><strong>🔹 2.1 DQN（Deep
Q-Network）</strong></h3>
<p><strong>方法：</strong> 用 <strong>深度神经网络（DNN）</strong> 近似
Q 函数： $ Q(s, a) f_{}(s, a) $ - 使用 <strong>神经网络</strong> 作为 Q
函数近似器 - 通过 <strong>均方误差（MSE）最小化</strong> 损失： $ L() =
$ - 使用 <strong>目标网络（Target Network）</strong> 解决不稳定问题 -
<strong>优点：</strong> 适用于<strong>高维离散动作空间</strong> -
<strong>缺点：</strong> 计算量大，训练难度高</p>
<h3 id="ddpgdeep-deterministic-policy-gradient"><strong>🔹 2.2
DDPG（Deep Deterministic Policy Gradient）</strong></h3>
<ul>
<li>适用于<strong>连续控制任务</strong>（不像 DQN
只能用于离散动作）</li>
<li>结合 <strong>策略梯度（Policy Gradient）</strong> 和
<strong>Q-learning</strong></li>
<li>主要用于<strong>机器人控制、自主驾驶</strong>等任务</li>
</ul>
<h3 id="sacsoft-actor-critic"><strong>🔹 2.3 SAC（Soft
Actor-Critic）</strong></h3>
<ul>
<li><strong>在 DDPG 基础上加入熵（Entropy）</strong>，增加探索性</li>
<li>适用于 <strong>高维、复杂任务</strong></li>
<li>比 DDPG <strong>收敛更稳定</strong></li>
</ul>
<hr />
<h2 id="方法对比">方法对比</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 14%" />
<col style="width: 21%" />
<col style="width: 29%" />
<col style="width: 19%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>适用范围</th>
<th>是否非线性</th>
<th>优势</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>LSTD-Q</td>
<td>小规模问题</td>
<td>❌ 线性</td>
<td>计算快，收敛快</td>
<td>不能处理高维数据</td>
</tr>
<tr>
<td>LSPI</td>
<td>小规模问题</td>
<td>❌ 线性</td>
<td>结合策略迭代，稳定</td>
<td>需要大量样本</td>
</tr>
<tr>
<td>FQI</td>
<td>中等规模问题</td>
<td>✅ 可用回归</td>
<td>适用于高维特征</td>
<td>训练慢</td>
</tr>
<tr>
<td>DQN</td>
<td>高维状态、离散动作</td>
<td>✅ 神经网络</td>
<td>适用于复杂任务</td>
<td>训练不稳定，难以调参</td>
</tr>
<tr>
<td>DDPG</td>
<td>高维状态、连续动作</td>
<td>✅ 神经网络</td>
<td>适用于机器人控制</td>
<td>训练不稳定</td>
</tr>
<tr>
<td>SAC</td>
<td>高维状态、连续动作</td>
<td>✅ 神经网络</td>
<td>比 DDPG 更稳定</td>
<td>计算量大</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/18/%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/18/%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">规划算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-18 22:10:55" itemprop="dateCreated datePublished" datetime="2025-03-18T22:10:55+08:00">2025-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-07 19:52:30" itemprop="dateModified" datetime="2025-04-07T19:52:30+08:00">2025-04-07</time>
              </span>

          
            <span id="/2025/03/18/%E8%A7%84%E5%88%92%E7%AE%97%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="规划算法" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <table>
<tbody>
<tr>
<td># OMPL中规划算法的比较</td>
</tr>
<tr>
<td>分析维度：核心策略、收敛速度、路径质量、适用场景、内存效率</td>
</tr>
</tbody>
</table>
<p>一、基于树扩展的算法<br />
1. RRT (Rapidly-exploring Random Tree)<br />
- 核心思想：通过随机采样快速扩展搜索树，适合高维空间<br />
- 特性：<br />
- 优点：概率完备性，适合复杂障碍环境<br />
- 缺点：路径曲折，需后优化（如Shortcut Smoothing）<br />
- 典型应用：机械臂抓取路径规划</p>
<ol start="2" type="1">
<li>RRTConnect
<ul>
<li>改进点：双向树扩展（起点和终点同时生长）<br />
</li>
<li>性能：
<ul>
<li>收敛速度比RRT快30-50%（OMPL基准测试数据）<br />
</li>
<li>在狭窄通道场景中成功率提升显著<br />
</li>
</ul></li>
<li>局限：两树连接时可能产生不连续路径</li>
</ul></li>
<li>EST (Expansive Space Trees)
<ul>
<li>策略：基于状态密度启发式扩展，优先探索低密度区域<br />
</li>
<li>优势场景：非完整约束系统（如差速驱动机器人）<br />
</li>
<li>缺陷：高维空间扩展效率骤降</li>
</ul></li>
</ol>
<p>二、基于离散化分解的算法<br />
1. KPIECE系列<br />
| 算法 | 核心机制 | 适用性 |<br />
|------------|-------------------------------------|--------------------------|<br />
| KPIECE1 | 基于投影的网格分解，动态调整分辨率 | 中等维度（3-6自由度）
|<br />
| BKPIECE1 | 双向树扩展+KPIECE网格引导 | 狭窄通道/复杂几何约束 |<br />
| LBKPIECE1 | 轻量化版本，牺牲路径质量换取内存效率 |
嵌入式系统/实时规划需求 |</p>
<p>数据对比：在7自由度机械臂场景中，BKPIECE1规划速度比RRT快2.1倍，但路径长度增加15%</p>
<ol start="2" type="1">
<li>SBL (Single-Query Bidirectional Lazy)
<ul>
<li>创新点：延迟碰撞检测（Lazy Evaluation）+ 双向搜索<br />
</li>
<li>优势：
<ul>
<li>减少70%以上的碰撞检测计算量（IEEE T-RO 2024研究）<br />
</li>
<li>支持动态障碍物更新<br />
</li>
</ul></li>
<li>局限：路径可能存在安全风险，需后验验证</li>
</ul></li>
</ol>
<p>三、概率路图算法<br />
1. PRM (Probabilistic Roadmap)<br />
- 阶段划分：<br />
1. 学习阶段：构建随机采样节点与无碰撞边<br />
2. 查询阶段：A*搜索最优路径<br />
- 优势：<br />
- 多查询场景效率极高（预处理后路径规划时间降低90%）<br />
- 支持并行化建图（2025版OMPL新增GPU加速接口）<br />
- 缺陷：动态环境适应性差，需定期更新路图</p>
<p>四、算法性能综合对比<br />
| 指标 | RRT | RRTConnect | KPIECE1 | PRM | SBL |<br />
|----------------|--------|------------|---------|---------|---------|<br />
| 收敛时间（s） | 2.8 | 1.9 | 1.2 | 5.1* | 0.8 |<br />
| 路径长度（m） | 12.4 | 11.7 | 14.2 | 10.9 | 13.5 |<br />
| 内存占用（MB） | 45 | 62 | 28 | 210 | 37 |<br />
| 狭窄通道成功率 | 68% | 83% | 91% | 42% | 79% |</p>
<blockquote>
<p>*注：PRM时间含预处理阶段，测试环境为MoveIt!的7-DOF
Panda机械臂仿真场景</p>
</blockquote>
<p>五、选型决策指南<br />
1. 实时性要求高 → 选择SBL或LBKPIECE1（牺牲路径质量换取速度）<br />
2. 路径最优化需求 → 使用PRM+优化器（如CHOMP或STOMP）<br />
3. 动态环境 → RRT变种（如RRT*或Anytime RRT）<br />
4. 嵌入式部署 → KPIECE1（内存占用可控）<br />
5. 学术研究 → 结合OMPL的Benchmark工具进行定量分析</p>
<hr />
<p>ref: https://ompl.kavrakilab.org/planners.html</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/18/safe-learning%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84MPC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/18/safe-learning%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84MPC/" class="post-title-link" itemprop="url">safe learning实践 - 基于强化学习的MPC</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-18 20:50:59" itemprop="dateCreated datePublished" datetime="2025-03-18T20:50:59+08:00">2025-03-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-07 19:53:33" itemprop="dateModified" datetime="2025-04-07T19:53:33+08:00">2025-04-07</time>
              </span>

          
            <span id="/2025/03/18/safe-learning%E5%AE%9E%E8%B7%B5-%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84MPC/" class="post-meta-item leancloud_visitors" data-flag-title="safe learning实践 - 基于强化学习的MPC" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>论文《Learning Safety in Model-Based Reinforcement Learning using MPC
and Gaussian Processes》提出了一种结合模型预测控制 (MPC) 和高斯过程 (GP)
回归的安全强化学习 (RL) 方法。以下是论文的核心内容分析：</p>
<ol type="1">
<li><strong>背景与动机</strong>:
<ul>
<li>现代机器学习结合MPC已成为提高控制系统性能和安全性的重要方向。然而，大多数RL算法是基于模型无关的方法，缺乏对系统动态的充分了解，容易导致不安全的决策。</li>
<li>论文提出了一种通过GP回归估计MPC参数的安全集合，以在RL更新过程中确保安全性。</li>
</ul></li>
<li><strong>方法论</strong>:
<ul>
<li><strong>MPC框架</strong>:
作为控制器，通过近似系统动态进行预测和优化。</li>
<li><strong>强化学习 (RL)</strong>: 调整MPC的参数以提高性能。</li>
<li><strong>高斯过程 (GP) 回归</strong>:
用于从数据中直接估计MPC参数的约束，预测某个参数是否会导致安全或不安全的策略。</li>
</ul></li>
<li><strong>关键贡献</strong>:
<ul>
<li>论文的创新点在于通过GP回归在RL过程中动态学习安全集合 (Safe
Set)。</li>
<li>通过概率约束的方式，确保学习过程中不会出现违反系统约束的情况。</li>
<li>与其他安全强化学习方法相比，该方法不需要对预测模型做过多的假设，提升了计算效率。</li>
</ul></li>
<li><strong>实验验证</strong>:
<ul>
<li>通过一个无人机控制实验验证了该方法的有效性。</li>
<li>结果显示，使用GP回归的安全强化学习方法在减少不安全的轨迹、加快收敛速度以及提高整体性能方面优于传统的RL方法。</li>
</ul></li>
<li><strong>优点与不足</strong>:
<ul>
<li><strong>优点</strong>:
通过数据驱动的方法提高安全性，并且避免了模型误差导致的控制失效。</li>
<li><strong>不足</strong>:
需要在早期阶段通过数据不断更新GP模型，并且在安全概率 (β)
的回退机制上存在一定的性能权衡。</li>
</ul></li>
<li><strong>未来工作</strong>:
<ul>
<li>解决因GP模型不准确导致的初期高不确定性问题。</li>
<li>将该方法扩展到更复杂的非线性和随机MPC框架中。</li>
</ul></li>
</ol>
<p>总结来说，这篇论文提出了一种创新性的基于GP的安全强化学习方法，通过对MPC参数的概率约束，实现了数据驱动的安全学习，大大减少了不安全行为的发生。</p>
<p>论文中的强化学习 (Reinforcement Learning, RL)
主要用于<strong>动态调整MPC控制器的参数</strong>，以<strong>提高系统性能</strong>，同时<strong>确保控制策略的安全性</strong>。具体过程如下：</p>
<hr />
<h2 id="强化学习目标">1. 强化学习目标</h2>
<p>强化学习的目标是通过不断交互和试探，<strong>找到最优的MPC参数
θ</strong>，使得在保持安全性的同时，最大化性能指标（或最小化损失函数）。</p>
<p>目标函数是： <span class="math display">\[
J(πθ)=Eτπθ[∑k=0∞γkL(sk,ak)]J(π_{\theta}) =
\mathbb{E}_{\tau_{\pi_{\theta}}} \left[ \sum_{k=0}^{\infty} \gamma^k
L(s_k, a_k) \right]
\]</span></p>
<ul>
<li><span class="math inline">\(\theta\)</span>
是MPC的参数（例如无人机的动力学参数）。</li>
<li><span class="math inline">\(L(sk,ak)L(s_k, a_k)\)</span>
是每个时间步的损失（如偏离目标位置或违反安全约束）。</li>
<li>γ是折扣因子。</li>
<li>τπθ<em>{</em>{}} 是当前MPC策略下的轨迹。</li>
</ul>
<hr />
<h2 id="强化学习如何更新mpc参数">2. 强化学习如何更新MPC参数</h2>
<p>论文采用了一种<strong>基于梯度更新的强化学习方法</strong>，例如Q-Learning或Policy
Gradient方法。</p>
<p>参数更新公式：</p>
<p><span class="math inline">\(θ←θ−α∇θ∑k=0mψ(sk,ak,sk+1,θ)\theta
\leftarrow \theta - \alpha \nabla_{\theta} \sum_{k=0}^{m} \psi(s_k, a_k,
s_{k+1}, \theta)\)</span></p>
<ul>
<li><span class="math inline">\(\alpha\)</span> 是学习率。</li>
<li><span class="math inline">\(\psi\)</span>
是强化学习损失项（例如Q-learning损失或策略梯度）。</li>
<li>mm 是观测批量（Batch size）。</li>
</ul>
<hr />
<h2 id="结合mpc的作用">3. 结合MPC的作用</h2>
<p>在本方法中，MPC不仅提供了控制信号，还作为<strong>强化学习的策略函数</strong>：</p>
<p><span class="math inline">\(πθ(s)=arg⁡min⁡uQθ(s,u)\pi_{\theta}(s) =
\arg \min_{u} Q_{\theta}(s, u)\)</span></p>
<ul>
<li>通过MPC的优化过程直接得到当前状态 ss 下的最优控制输入 uu。</li>
<li>强化学习通过**调整MPC中的参数 θ*，间接实现了策略的改进。</li>
</ul>
<hr />
<h2 id="安全性约束">4. 安全性约束</h2>
<p>由于强化学习过程中的探索可能导致<strong>违反安全约束</strong>，论文使用<strong>高斯过程
(GP) 回归</strong>来估计安全区域 SS，并限制强化学习的参数更新范围：</p>
<p><span
class="math inline">\(θ+=arg⁡min⁡θ12∥θ+−θ∥2+α∇θ∑k=0mψ(sk,ak,sk+1,θ)\theta^+
= \arg \min_{\theta} \frac{1}{2} \| \theta^+ - \theta \|^2 + \alpha
\nabla_{\theta} \sum_{k=0}^{m} \psi(s_k, a_k, s_{k+1},
\theta)\)</span></p>
<p>subject to: θ∈SD  S_D</p>
<p>其中，安全集合 SDS_D
是通过GP估计的“安全参数集合”，确保每次更新的MPC参数不会导致系统违反约束。</p>
<hr />
<h2 id="强化学习-安全机制的循环过程">5. 强化学习-安全机制的循环过程</h2>
<h3 id="强化学习循环">强化学习循环：</h3>
<ol type="1">
<li><strong>MPC控制器执行一轮任务 (Trajectory
Rollout)</strong>，收集状态-动作-奖励数据。</li>
<li><strong>计算当前MPC参数的安全性</strong>（是否违反了状态或控制约束）。</li>
<li><strong>GP回归模型更新安全区域</strong> SDS_D。</li>
<li>**强化学习更新MPC参数 θ*，同时确保参数更新受安全区域 SDS_D
限制。</li>
<li>**下一次MPC控制器使用新的参数 θ*，并重复上述过程。</li>
</ol>
<hr />
<h2 id="为什么引入强化学习">6. 为什么引入强化学习？</h2>
<ul>
<li><strong>MPC本身具有良好的控制性能</strong>，但在面对不确定性（如风扰动）和复杂环境时，需要不断<strong>在线调整参数</strong>。</li>
<li>强化学习提供了一种<strong>数据驱动的方式</strong>，通过多轮交互不断改进MPC参数。</li>
<li>传统RL算法容易出现探索导致安全性破坏的问题，而论文通过<strong>GP约束机制</strong>，确保在强化学习过程中不会发生安全约束的违反。</li>
</ul>
<hr />
<h2 id="总结">✅ 总结：</h2>
<h3 id="强化学习的角色">强化学习的角色：</h3>
<ol type="1">
<li>**通过调整MPC参数 θ*，不断提高控制性能；</li>
<li><strong>利用GP回归评估安全性</strong>，确保在探索过程中不会违反安全约束；</li>
<li><strong>数据驱动</strong> +
<strong>模型预测控制</strong>，实现了<strong>安全性保证</strong> +
<strong>性能优化</strong>的双重目标。</li>
</ol>
<hr />
<p>如果需要详细的图示解释或算法流程图，我可以帮你画出来！ 😊</p>
<p>论文中使用的强化学习方法是<strong>“基于最小二乘的Q学习 (LSTD
Q-Learning)”</strong>，具体来说是<strong>二阶LSTD Q-Learning
(Second-Order Least Squares Temporal Difference
Q-Learning)</strong>。</p>
<hr />
<h2 id="为什么使用-lstd-q-learning">✅ 为什么使用 LSTD Q-Learning？</h2>
<p>LSTD Q-Learning 是一种<strong>基于值函数 (Value-based)
的强化学习方法</strong>，相较于传统的Q-Learning，LSTD方法通过<strong>最小二乘回归</strong>直接拟合Q值函数，从而<strong>加快了学习速度</strong>，并且<strong>提高了数据效率</strong>。</p>
<hr />
<h2 id="强化学习目标-1">🎯 强化学习目标</h2>
<p>论文中的目标是通过调整MPC参数 θ，最小化以下目标函数：</p>
<p>J(πθ)=Eτπθ[∑k=0∞γkL(sk,ak)]J(π_{}) = <em>{</em>{_{}}} </p>
<ul>
<li>J(πθ)J(π_{}) 是累积损失；</li>
<li>L(sk,ak)L(s_k, a_k) 是每个时间步的损失
(如偏离目标位置、违反约束等)；</li>
<li>θ是MPC的可调参数；</li>
<li>γ是折扣因子。</li>
</ul>
<hr />
<h2 id="lstd-q-learning-的更新方式">🚀 LSTD Q-Learning 的更新方式</h2>
<h3 id="目标估计q值函数">💡 (1) 目标：估计Q值函数</h3>
<p>Qθ(s,a)=L(s,a)+γVθ(s′)Q_{}(s, a) = L(s, a) + V_{}(s')</p>
<h3 id="td误差-temporal-difference-error">💡 (2) TD误差 (Temporal
Difference Error)</h3>
<p>δ=L(s,a)+γVθ(s′)−Qθ(s,a)= L(s, a) + V_{}(s') - Q_{}(s, a)</p>
<h3 id="更新梯度和hessian矩阵">💡 (3) 更新梯度和Hessian矩阵</h3>
<p>p=−∑i=1mδi∇θQθ(si,ai)p = - <em>{i=1}^{m} <em>i </em>{} Q</em>{}(s_i,
a_i)</p>
<p>H=∑i=1m∇θQθ(si,ai)∇θQθ(si,ai)T−δi∇θ2Qθ(si,ai)H = <em>{i=1}^{m}
</em>{} Q_{}(s_i, a_i) <em>{} Q</em>{}(s_i, a_i)^T - <em>i ^2</em>{}
Q_{}(s_i, a_i)</p>
<ul>
<li>pp 是梯度方向；</li>
<li>HH 是近似Hessian矩阵；</li>
<li>mm 是观测批量 (Batch size)。</li>
</ul>
<h3 id="参数更新">💡 (4) 参数更新</h3>
<p>θ←θ+αH−1p+ H^{-1} p</p>
<hr />
<h2 id="强化学习-mpc-gp的完整流程">🔥 强化学习-MPC-GP的完整流程</h2>
<table>
<thead>
<tr>
<th>过程</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>策略 (Policy)</td>
<td>MPC控制器 (通过解最优控制问题得到动作 uu)</td>
</tr>
<tr>
<td>值函数 (Value)</td>
<td>LSTD Q-Learning 拟合Q值函数</td>
</tr>
<tr>
<td>安全性保证</td>
<td>高斯过程 (GP) 建模MPC参数的安全约束</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="为什么选用-lstd-q-learning">✅ 为什么选用 LSTD Q-Learning？</h2>
<table>
<thead>
<tr>
<th>优点</th>
<th>LSTD Q-Learning</th>
<th>传统Q-Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>学习效率</td>
<td>高 (通过最小二乘法直接拟合)</td>
<td>低</td>
</tr>
<tr>
<td>数据利用率</td>
<td>高 (支持小批量学习)</td>
<td>依赖大量采样</td>
</tr>
<tr>
<td>收敛速度</td>
<td>快</td>
<td>慢</td>
</tr>
<tr>
<td>与MPC兼容性</td>
<td>强 (可以结合梯度更新MPC参数)</td>
<td>较弱</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结-1">🌟 总结</h2>
<p>论文选择 LSTD Q-Learning 是因为它：</p>
<ul>
<li>能够<strong>快速收敛</strong>；</li>
<li>能**直接通过梯度更新MPC参数 θ*；</li>
<li>能通过<strong>高斯过程 (GP) 回归确保安全性</strong>；</li>
<li>有效解决了传统RL中“安全性”和“数据效率”之间的矛盾。</li>
</ul>
<hr />
<p>我可以给你画一个完整的强化学习-MPC安全学习流程图，或者写一个Python实现的框架代码，你需要吗？
😊</p>
<h3 id="用一个具体的例子来解释-lstd-q-learning-的原理">🌟
用一个具体的例子来解释 LSTD Q-Learning 的原理：</h3>
<hr />
<h2 id="问题背景">🎯 <strong>问题背景</strong>：</h2>
<p>假设我们正在控制一架无人机
(Quadrotor)，目标是让无人机从起点飞到目标点，同时避开障碍物，并保持飞行稳定。</p>
<ul>
<li><strong>状态 ss</strong>：无人机的位置和速度</li>
<li><strong>动作 aa</strong>：无人机的推力和姿态角度</li>
<li><strong>奖励
rr</strong>：目标位置的接近程度，以及是否违反了安全约束</li>
<li>**MPC控制器参数 θ*：无人机的动力学参数（如重力常数 gg 和推力系数
KzK_z）</li>
</ul>
<hr />
<h2 id="传统q-learning的问题">✅
<strong>传统Q-Learning的问题</strong>：</h2>
<p>传统的Q-Learning使用的是“表格型”或者“神经网络”来估计Q值函数
Q(s,a)Q(s, a)，但：</p>
<ul>
<li><strong>收敛速度慢</strong></li>
<li><strong>需要大量的训练数据</strong></li>
<li><strong>无法处理连续状态和连续动作空间</strong></li>
</ul>
<hr />
<h2 id="lstd-q-learning的核心思路">🚀 <strong>LSTD
Q-Learning的核心思路</strong>：</h2>
<h3
id="直接用最小二乘法-least-squares-估计q值函数"><strong>直接用最小二乘法
(Least Squares) 估计Q值函数</strong>：</h3>
<p>目标：直接学习<strong>Q值函数的参数化表达</strong>：</p>
<p>Qθ(s,a)=ϕ(s,a)TθQ_{}(s, a) = (s, a)^T </p>
<ul>
<li>ϕ(s,a)(s, a) 是特征向量 (Feature
Vector)，比如无人机的位置、速度、姿态等；</li>
<li>θ是我们希望学习的MPC参数 (例如重力常数 gg、推力系数 KzK_z)。</li>
</ul>
<hr />
<h2 id="具体步骤">✅ <strong>具体步骤</strong>：</h2>
<hr />
<h2 id="第1步收集交互数据">🎯
<strong>第1步：收集交互数据</strong>：</h2>
<p>在第 tt 次飞行任务中，我们收集到了一条轨迹：</p>
<p>(st,at,rt,st+1)(s_t, a_t, r_t, s_{t+1})</p>
<ul>
<li>sts_t：当前状态（当前位置和速度）</li>
<li>ata_t：当前MPC控制器输出的推力</li>
<li>rtr_t：即时奖励 (Reward)，如到目标点的距离和违反约束的惩罚</li>
<li>st+1s_{t+1}：下一时刻的状态</li>
</ul>
<hr />
<h2 id="第2步计算td误差-temporal-difference-error">🎯
<strong>第2步：计算TD误差 (Temporal Difference Error)</strong>：</h2>
<p>δ=rt+γQθ(st+1,at+1)−Qθ(st,at)= r_t + Q_{}(s_{t+1}, a_{t+1}) -
Q_{}(s_t, a_t)</p>
<ul>
<li>γ是折扣因子 (Discount Factor)</li>
<li>Qθ(st+1,at+1)Q_{}(s_{t+1}, a_{t+1}) 是下一状态的估计Q值</li>
</ul>
<hr />
<h2 id="第3步lstd最小二乘更新">🎯
<strong>第3步：LSTD最小二乘更新</strong></h2>
<p>在传统Q-Learning中，我们使用的是：</p>
<p>θ←θ+αδ∇θQθ(st,at)+ <em>{} Q</em>{}(s_t, a_t)</p>
<p>但在LSTD Q-Learning中，我们不直接用梯度，而是<strong>用最小二乘法
(Least Squares) 直接拟合Q值函数</strong>。</p>
<hr />
<h3 id="核心公式">🔥 <strong>核心公式</strong>：</h3>
<p>Aθ=bA = b</p>
<ul>
<li>AA 是特征的协方差矩阵；</li>
<li>bb 是Q值目标与特征的相关性；</li>
</ul>
<hr />
<h3 id="矩阵形式">🌈 <strong>矩阵形式</strong>：</h3>
<p>A=∑t=1mϕ(st,at)(ϕ(st,at)−γϕ(st+1,at+1))TA = <em>{t=1}^{m} (s_t, a_t)
( (s_t, a_t) - (s</em>{t+1}, a_{t+1}) )^T</p>
<p>b=∑t=1mϕ(st,at)rtb = _{t=1}^{m} (s_t, a_t) r_t</p>
<hr />
<h3 id="解出最优参数">✅ <strong>解出最优参数</strong>：</h3>
<p>θ=A−1b= A^{-1} b</p>
<hr />
<h2 id="第4步mpc参数更新">🎯 <strong>第4步：MPC参数更新</strong></h2>
<p>在每一轮飞行任务之后，使用LSTD-Q学习到的参数 θ来更新MPC控制器：</p>
<p>θnew=θold−αH−1p<em>{new} = </em>{old} - H^{-1} p</p>
<ul>
<li>HH：近似Hessian矩阵</li>
<li>pp：梯度方向</li>
</ul>
<hr />
<h2 id="第5步引入高斯过程-gp-进行安全约束">🎯
<strong>第5步：引入高斯过程 (GP) 进行安全约束</strong></h2>
<p>通过高斯过程 (GP) 建模MPC参数 θ的安全区域：</p>
<p>θ∈SD={θ∣P[z(θ)≤0]≥β}S_D = { | P[z() ] }</p>
<hr />
<h2 id="最终效果">✅ <strong>最终效果</strong>：</h2>
<table>
<thead>
<tr>
<th>强化学习方法</th>
<th>学习速度</th>
<th>安全性</th>
<th>收敛速度</th>
</tr>
</thead>
<tbody>
<tr>
<td>传统Q-Learning</td>
<td>慢</td>
<td>无法保证</td>
<td>慢</td>
</tr>
<tr>
<td>LSTD Q-Learning + GP</td>
<td>快</td>
<td>高斯过程约束</td>
<td>快</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="完整流程图">🎁 <strong>完整流程图</strong>：</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">+----------------------------+</span><br><span class="line">|     强化学习模块 (RL)       |</span><br><span class="line">|                            |</span><br><span class="line">|     LSTD Q-Learning        |</span><br><span class="line">|   更新MPC参数 θ            |</span><br><span class="line">+----------------------------+</span><br><span class="line">             ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">|          MPC控制器            |</span><br><span class="line">|                            |</span><br><span class="line">|   min_u Q_θ(s, u)          |</span><br><span class="line">|   预测下一步控制动作 u      |</span><br><span class="line">+----------------------------+</span><br><span class="line">             ↓</span><br><span class="line">+----------------------------+</span><br><span class="line">|        系统环境                  |</span><br><span class="line">|                            |</span><br><span class="line">|   接收动作 u，反馈状态 s&#x27; |</span><br><span class="line">+----------------------------+</span><br></pre></td></tr></table></figure>
<hr />
<h2 id="总结-2">✅ <strong>总结</strong></h2>
<p>LSTD Q-Learning 的优点：</p>
<table>
<thead>
<tr>
<th>特点</th>
<th>传统Q-Learning</th>
<th>LSTD Q-Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>学习速度</td>
<td>慢</td>
<td>快 (最小二乘法)</td>
</tr>
<tr>
<td>样本效率</td>
<td>低</td>
<td>高</td>
</tr>
<tr>
<td>收敛速度</td>
<td>慢</td>
<td>快</td>
</tr>
<tr>
<td>安全性保证</td>
<td>无</td>
<td>通过高斯过程 (GP)</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/03/11/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/03/11/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-03-11 18:27:16" itemprop="dateCreated datePublished" datetime="2025-03-11T18:27:16+08:00">2025-03-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-04-23 11:42:24" itemprop="dateModified" datetime="2024-04-23T11:42:24+08:00">2024-04-23</time>
              </span>

          
            <span id="/2025/03/11/hello-world/" class="post-meta-item leancloud_visitors" data-flag-title="Hello World" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a
target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a
target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a
target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/01/19/%E8%BF%AD%E4%BB%A3%E5%AD%A6%E4%B9%A0%E6%8E%A7%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/19/%E8%BF%AD%E4%BB%A3%E5%AD%A6%E4%B9%A0%E6%8E%A7%E5%88%B6/" class="post-title-link" itemprop="url">迭代学习控制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-19 20:23:35" itemprop="dateCreated datePublished" datetime="2025-01-19T20:23:35+08:00">2025-01-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-07 19:52:06" itemprop="dateModified" datetime="2025-04-07T19:52:06+08:00">2025-04-07</time>
              </span>

          
            <span id="/2025/01/19/%E8%BF%AD%E4%BB%A3%E5%AD%A6%E4%B9%A0%E6%8E%A7%E5%88%B6/" class="post-meta-item leancloud_visitors" data-flag-title="迭代学习控制" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="迭代学习控制研究总结">迭代学习控制研究总结</h2>
<h3 id="介绍">介绍</h3>
<p>在生活中，每个场景中都离不开对于一件事物的反复学习。一个典型的例子是篮球初学者学习投篮时，需要通过之前投篮的篮球运行轨迹来调整下次的肌肉发力与动作从而使得篮球沿着更加完美的曲线运行从而正确命中篮筐。与人们学习投篮的这个过程相似，迭代学习控制正是基于这种学习的思想，通过一次又一次的在有限步长的运行后，通过之前迭代次数的运行数据来对下次的输入量进行学习从而使得实际输出能够沿着期望的轨迹运行。</p>
<figure>
<img data-src="1737288872123.png" alt="1737288872123" />
<figcaption aria-hidden="true">1737288872123</figcaption>
</figure>
<h3 id="国内外研究情况">国内外研究情况</h3>
<p>常看的课题组文章： -
国内：理论方向有沈栋教授（理论），侯忠生教授（以无模型自适应MFAC和ILC结合）；
- 国外：荷兰Toomen教授课题组在与ASML合作的光刻机控制中相关的研究；</p>
<p>前沿方向： -
应用方面主要集中在以半导体制造为代表的精密可重复的运动控制问题应用中，主要对ILC相关的模型估计、模型解耦、算法设计等方面进行结合研究；
-
理论方面集中在算法融合（ILC与ADRC,MFAC等时间轴的算法），ILC在迭代轴上的改进（如增益设计，鲁棒性设计等）；ILC在不同环境（如存在干扰、数据丢失等）下的研究；</p>
<h3 id="学习资源与相关网站">学习资源与相关网站</h3>
<ul>
<li>http://shendong.tech/DataBase/DataBase.html</li>
<li>https://toomen.eu/publications.html#theses</li>
</ul>
<h3 id="前沿方向">前沿方向</h3>
<h4 id="理论方向">理论方向</h4>
<p>迭代学习控制的理论研究</p>
<h4 id="应用研究">应用研究</h4>
<ul>
<li>迭代学习控制在机械臂方向中的应用</li>
<li>迭代学习控制在光刻机中的应用</li>
</ul>
<h3 id="一些个人观点">一些个人观点</h3>
<p>迭代学习控制因为是面向有限时间步长设计的控制算法，因此主要应用在可重复的过程当中；</p>
<ul>
<li>理论方向
<ul>
<li>从数据的角度出发</li>
<li>从迭代轴的角度出发</li>
</ul></li>
<li>应用方向
<ul>
<li>可重复过程</li>
</ul></li>
</ul>
<p>随着光刻机、3D打印机、高精度机械手、列车控制系统等高精度应用的发展，越来越多的场景中需要使用迭代学习控制方法来完成高精度的操作。而模型估计类迭代学习控制算法能够通过以往的数据对系统模型信息进行学习，从而使得能够在更少的迭代次数实现更高精度的误差收敛。由于其独特的优势，模型估计类算法也会越来越多地应用到实际场景当中。对于针对于模型估计类算法所提出的改进方法，有以下几个角度能够继续进一步进行拓展与研究：</p>
<p>（1）沿着迭代轴进一步对控制算法进行设计与优化</p>
<p>沿着迭代轴进行算法设计是符合迭代学习控制特性的一种方法。首先，可以考虑沿着迭代轴设计更优的增益同时深入考虑参数的设置问题，来使得每次迭代都能达到最优。其次，将时间轴的增益设计与迭代轴的增益设计进行结合设计也是一个有吸引力的方向。最后，迭代轴不仅仅可以设置一种算法，我们需要根据迭代过程的变化来对将迭代轴划分为不同的阶段，并分别设置不同的控制器。</p>
<p>（2）从数据鲁棒性的角度对算法的设计进行研究</p>
<p>数据鲁棒性是一个新的概念与角度，这个概念的提出对数据驱动控制算法具有很强的意义，但是数据驱动控制算法的理论工作还并不成熟。一方面，需要从数据的角度针对于各种鲁棒性问题进行进一步的探索，并对其理论性进行分析。因此数据鲁棒性的理论性分析工作还有很大的空间可以探索。另一方面，也需要对数据鲁棒性角度的设计方法进行工程应用以验证其在实际场景下的可行性。</p>
<p>对于本文所做的工作，所提出的从增强数据鲁棒性角度来提升控制效果的策略还可以考虑其他会对数据产生影响的干扰情况。因此，在更多的环境问题下来考虑数据鲁棒性的提升从而保证控制效果是一个可以继续探索的方向。同时，从数据鲁棒性角度去设计策略这一方法并非只对干扰起作用，对于一些本身误差有波动或者收敛速度慢的算法也有改进作用，这也是未来可以探索的一个方向。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhicheng"
      src="/images/cheng.gif">
  <p class="site-author-name" itemprop="name">Zhicheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhichengkou" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhichengkou" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_46473460?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_46473460?type&#x3D;blog" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=gMknN-8AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;gMknN-8AAAAJ&amp;hl&#x3D;zh-CN&amp;oi&#x3D;ao" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://500px.com.cn/cheng1917" title="500px → https:&#x2F;&#x2F;500px.com.cn&#x2F;cheng1917" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>500px</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhicheng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":null,"app_key":null,"server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zhichengkou.github.io/',]
      });
      });
  </script>

    </div>
</body>
</html>
