<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhichengkou.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhicheng">
<meta property="og:url" content="http://zhichengkou.github.io/index.html">
<meta property="og:site_name" content="Zhicheng">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zhicheng">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://zhichengkou.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Zhicheng</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhicheng</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/" class="post-title-link" itemprop="url">关于博士</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-20 23:06:29 / 修改时间：23:08:55" itemprop="dateCreated datePublished" datetime="2025-04-20T23:06:29+08:00">2025-04-20</time>
            </span>

          
            <span id="/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/" class="post-meta-item leancloud_visitors" data-flag-title="关于博士" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这个问题非常重要！博士≠多读几年书，博士更像是<strong>一个“高强度训练营”，塑造你成为能独立提出、解决、验证问题的人。</strong></p>
<p>我们可以把博士期间应当掌握的能力分为以下五大核心模块：<br />
<strong>认知能力、科研能力、工程/技术能力、表达与协作能力、元能力（心智层）</strong></p>
<hr />
<h2 id="一认知能力这是博士的思维底盘">🧠
一、认知能力：这是博士的“思维底盘”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>抽象能力</strong></td>
<td>能从复杂现象中提取本质，比如将实验现象抽象为数学模型或控制系统</td>
</tr>
<tr>
<td><strong>结构化思维</strong></td>
<td>能将模糊任务拆解为清晰模块，有清晰的推理链</td>
</tr>
<tr>
<td><strong>批判性思维</strong></td>
<td>不盲信权威，敢于质疑前人方法，善于找出假设漏洞</td>
</tr>
<tr>
<td><strong>判断研究价值的能力</strong></td>
<td>能识别“哪些问题值得做”、“是否有学术贡献”</td>
</tr>
</tbody>
</table>
<p>📌
<em>一个好的博士不是做得多，而是<strong>思考得深</strong>。</em></p>
<hr />
<h2 id="二科研能力这是博士的核心业务">📚
二、科研能力：这是博士的“核心业务”</h2>
<table>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>选题能力</strong></td>
<td>能发现有意义、可行、有创新的问题</td>
</tr>
<tr>
<td><strong>文献调研能力</strong></td>
<td>能高效阅读、比较、归纳相关研究工作</td>
</tr>
<tr>
<td><strong>设计实验能力</strong></td>
<td>能构造有效的验证方式（仿真 / 实验 / 数据）</td>
</tr>
<tr>
<td><strong>数学建模/算法推导能力</strong></td>
<td>能写出合理的公式、证明关键结论</td>
</tr>
<tr>
<td><strong>论文写作能力</strong></td>
<td>能清楚表达问题、方法、实验、结论，让评审信服</td>
</tr>
</tbody>
</table>
<p>📌 <em>“做研究”不仅是做，更是不断地“想、写、讲、验证”。</em></p>
<hr />
<h2 id="三技术工程能力这是博士的落地抓手">🛠️
三、技术/工程能力：这是博士的“落地抓手”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>代码实现能力</strong></td>
<td>不仅写得对，还能写得稳、写得快、写得清晰</td>
</tr>
<tr>
<td><strong>系统调试能力</strong></td>
<td>能快速定位问题并修复，软硬件都能应对</td>
</tr>
<tr>
<td><strong>工具链掌握</strong></td>
<td>熟练掌握领域主流工具，如 PyTorch、ROS、Isaac Gym、LaTeX、Git 等</td>
</tr>
<tr>
<td><strong>大模型 / 控制 / 优化 / 数据等专项技能</strong></td>
<td>针对研究方向具备较强技术深度</td>
</tr>
</tbody>
</table>
<p>📌 <em>再“学术”的工作，最后也要通过工程实现才能验证。</em></p>
<hr />
<h2 id="四表达与协作能力这是博士的输出通道">🗣️
四、表达与协作能力：这是博士的“输出通道”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>汇报和讲解能力</strong></td>
<td>能讲清楚自己的研究问题、路线、结果，不绕圈子</td>
</tr>
<tr>
<td><strong>跨领域沟通能力</strong></td>
<td>和工程/算法/导师/评审有效协作，降低“学术闭塞”风险</td>
</tr>
<tr>
<td><strong>写 Email / 项目申请 / Reviewer 回复</strong></td>
<td>格式专业，语气得体，能解决实际问题</td>
</tr>
<tr>
<td><strong>合作者管理与合作谈判</strong></td>
<td>特别是博后阶段，需要带人、带项目、谈合作</td>
</tr>
</tbody>
</table>
<p>📌 <em>科研不是一个人的战斗，表达力决定你能否“被听见”。</em></p>
<hr />
<h2 id="五元能力心智层博士能否坚持下来的隐形力">🧭
五、元能力（心智层）：博士能否坚持下来的“隐形力”</h2>
<table>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>自我驱动能力</strong></td>
<td>没人 push 你也能主动推进研究</td>
</tr>
<tr>
<td><strong>时间管理能力</strong></td>
<td>同时处理多个项目、任务，合理安排周/月/年计划</td>
</tr>
<tr>
<td><strong>挫折恢复力</strong></td>
<td>面对实验失败、论文被拒、导师施压也不崩溃</td>
</tr>
<tr>
<td><strong>自我反思与成长循环</strong></td>
<td>定期复盘自己：哪里做得好？哪里需要调整？</td>
</tr>
</tbody>
</table>
<p>📌 <em>博士拼到最后，不是智商，是系统性和韧性。</em></p>
<hr />
<h2 id="最终你应该变成一个怎样的人">✨ 最终你应该变成一个怎样的人？</h2>
<blockquote>
<p>🎓 “我可以独立从 0 到 1
地完成一个有意义的问题研究，具备将复杂问题结构化、技术化、学术化地解决并表达出来的能力。”<br />
这才是真正的博士能力画像。</p>
</blockquote>
<hr />
<h2 id="补充读完博士后就业发展方向会非常依赖这些能力">💼
补充：读完博士后，就业/发展方向会非常依赖这些能力</h2>
<table>
<thead>
<tr>
<th>职业方向</th>
<th>对博士能力的需求侧重</th>
</tr>
</thead>
<tbody>
<tr>
<td>高校教职</td>
<td>科研能力 + 表达能力 + 项目组织能力</td>
</tr>
<tr>
<td>科研所（中科院等）</td>
<td>技术深度 + 系统能力</td>
</tr>
<tr>
<td>AI/机器人大厂</td>
<td>算法落地 + 工程能力 + 团队协作</td>
</tr>
<tr>
<td>创业</td>
<td>问题定义能力 + 心智强度 + 综合运营</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="小结博士五维能力图谱">✅ 小结：博士五维能力图谱</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">             元能力（自驱力、时间管理、情绪管理）</span><br><span class="line">                        ↑</span><br><span class="line">表达协作 ← 认知能力（思维框架） → 技术工程</span><br><span class="line">                        ↓</span><br><span class="line">                科研能力（核心任务）</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/" class="post-title-link" itemprop="url">关于目标设定的内部化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-20 22:25:58 / 修改时间：23:03:15" itemprop="dateCreated datePublished" datetime="2025-04-20T22:25:58+08:00">2025-04-20</time>
            </span>

          
            <span id="/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/" class="post-meta-item leancloud_visitors" data-flag-title="关于目标设定的内部化" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>为什么衡量标准<strong>不能太外部化</strong>？我们可以从
<strong>心理机制 + 动力机制 + 行为后果</strong> 三个层面来剖析。</p>
<hr />
<h2 id="心理层面外部目标易引发焦虑迷失自我否定">🧠 1.
心理层面：<strong>外部目标易引发焦虑、迷失、自我否定</strong></h2>
<h3
id="外部化目标的本质是我好不好别人说了算">外部化目标的本质是“我好不好，别人说了算”：</h3>
<ul>
<li>比如“年薪百万”是由公司评估你值不值；</li>
<li>“拿称号”是评审专家说了算；</li>
<li>“某大厂 offer”是 HR 和面试官决定。</li>
</ul>
<p>这意味着：<br />
你把价值感
<strong>交给了外部</strong>，一旦外界反馈不如预期，就会陷入：</p>
<ul>
<li>📉 <strong>自我否定</strong>（我是不是不够好）</li>
<li>🤯 <strong>焦虑攀比</strong>（别人有了我还没有）</li>
<li>🔁 <strong>情绪循环</strong>（得不到就痛苦，得到了也很空）</li>
</ul>
<p><strong>外部化的衡量标准，往往不指向你真正的成长，而是社会期望的“表演成绩单”。</strong></p>
<hr />
<h2 id="动力机制层面外部目标难以形成可持续的内驱力">🚀 2.
动力机制层面：<strong>外部目标难以形成可持续的内驱力</strong></h2>
<h3 id="外部目标的典型陷阱">❌ 外部目标的典型陷阱：</h3>
<ul>
<li>得到之后很快失去动力（比如升职后陷入空虚）</li>
<li>没得到就陷入无意义感（比如努力失败时无法承受）</li>
<li>做事的动力变成“逃避失败”而非“追求成长”</li>
</ul>
<h3 id="内部目标的底层驱动更稳定">✅ 内部目标的底层驱动更稳定：</h3>
<p>比如：</p>
<blockquote>
<p>“我想掌握控制机器人上下楼梯的能力”<br />
“我想做一件对这个领域有贡献的工作”<br />
“我希望自己成为一个更有独立判断力的人”</p>
</blockquote>
<p>这些目标和<strong>自我认知、自我实现</strong>绑定，就算过程艰难，也更容易走得下去，因为你不是为别人“演”，你是为自己“活”。</p>
<hr />
<h2 id="行为后果层面外部目标容易导致手段扭曲路径偏离">🧩 3.
行为后果层面：<strong>外部目标容易导致手段扭曲、路径偏离</strong></h2>
<p>比如： - 为了“年薪百万”去做高压但无成长的岗位 -
为了“某称号”把项目写得浮夸不实 - 为了“大厂
offer”内卷刷题、却忽视基础能力建设</p>
<p>这会让你陷入： - 短视行为（重结果轻过程） - 虚假反馈（只追 KPI） -
自我价值错位（觉得自己“不配被喜欢”只因没达成某目标）</p>
<hr />
<h2 id="所以正确目标应该这样设定">✅ 所以，正确目标应该这样设定：</h2>
<table>
<colgroup>
<col style="width: 36%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr>
<th>外部化目标（错误）</th>
<th>内部化目标（推荐）</th>
</tr>
</thead>
<tbody>
<tr>
<td>“我要进某公司”</td>
<td>“我要成为能胜任该类岗位的工程师”</td>
</tr>
<tr>
<td>“我要发 A 类顶刊”</td>
<td>“我要建立起独立科研选题与推进能力”</td>
</tr>
<tr>
<td>“我要月入 3 万”</td>
<td>“我要能创造价值并掌控自由的生活方式”</td>
</tr>
<tr>
<td>“我要被认可/表扬”</td>
<td>“我要能稳定地输出高质量工作/内容”</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结金句">🎯 总结金句：</h2>
<blockquote>
<p><strong>外部目标只能当做“里程碑”，真正推动你走远的，是对内在成长的渴望。</strong></p>
</blockquote>
<ul>
<li>外部化目标是<strong>可衡量的结果</strong></li>
<li>内部化目标是<strong>可积累的能力与状态</strong></li>
<li>真正“牛的人”，是<strong>结果有了，心态还是过程型的</strong></li>
</ul>
<hr />
<p>我们都知道“外部目标不靠谱”“要内驱”，但现实中——<br />
<strong>绝大多数人都在盯着钱、名、offer、称号、头衔……</strong></p>
<p>那为什么会这样？<br />
答案是：<strong>这很“合理”，但不“健康”</strong>。</p>
<p>下面我帮你分析背后的<strong>五个原因</strong>：</p>
<hr />
<h2 id="社会系统鼓励外部目标它更容易衡量管理和比较">❶
社会系统鼓励外部目标：<strong>它更容易衡量、管理和比较</strong></h2>
<ul>
<li><strong>考试分数、绩点、奖学金、职称、年薪、Offer
数</strong>，这些都具备一个特点： 👉
<em>可以量化</em>，<em>可以排名</em>，<em>可以一眼看出你“值多少钱”</em></li>
</ul>
<p>这对制度来说很方便： - 公司招人凭它 - 父母比较孩子凭它 -
朋友圈判断你“是不是牛”也凭它</p>
<blockquote>
<p>所以你不“看它”你就会“掉队”，你不想卷，也会被卷。</p>
</blockquote>
<hr />
<h2 id="大脑的即时奖励系统被外部刺激绑架了">❷
大脑的“即时奖励系统”被外部刺激绑架了</h2>
<ul>
<li>收到
offer、加薪、被点赞、上热搜、拿奖——这些会<strong>立刻让你“爽”</strong></li>
<li>而内在成长，比如：
<ul>
<li>掌握一个复杂知识</li>
<li>读懂一本好书</li>
<li>理解一个人的心意 这些回报
<strong>慢且不确定</strong>，你的大脑要“延迟满足”</li>
</ul></li>
</ul>
<h3 id="所以说">所以说：</h3>
<blockquote>
<p><strong>外部目标像快餐，随手可得，但不养人；<br />
内在成长像慢炖汤，要等，但滋养深沉。</strong></p>
</blockquote>
<hr />
<h2 id="成长型目标需要深度的自我理解大多数人没机会培养">❸
成长型目标需要“深度的自我理解”——大多数人没机会培养</h2>
<ul>
<li>想知道“我想解决什么问题”“我想成为什么样的人”其实很难</li>
<li>教育体系和社会氛围几乎不给你空间去探索这些问题</li>
<li>所以人们只能转而去追那些<strong>别人都在追、社会说“好”的目标</strong></li>
</ul>
<p>这其实是<strong>认知发展阶段的问题</strong>，并不是谁“肤浅”或“功利”，而是：
&gt; <strong>没人教他们“怎么设定真正属于自己的目标”</strong></p>
<hr />
<h2 id="外部目标更容易用来证明自己满足安全感">❹
外部目标更容易用来“证明自己”，满足安全感</h2>
<p>很多人不是为了“钱本身”，<br />
而是为了“别人觉得我不错”、“爸妈看我有出息”、“不被看不起”……</p>
<p>这其实是： &gt; 把外部目标当成“自我价值的防御工具”<br />
&gt; 外界的评价越不确定，越要“用东西堵上”</p>
<hr />
<h2 id="外部目标确实能在短期内让人冲刺前进">❺
“外部目标”确实能在短期内让人冲刺前进</h2>
<p>别误会—— 外部目标并不是“完全不好”，它有以下好处： -
<strong>可见性强、行动明确、反馈快</strong> -
<strong>可以激励你迈出第一步</strong> -
<strong>是内在目标“实现”的副产品</strong></p>
<p>但它不能成为 <strong>最终驱动力</strong>，否则就会迷失。</p>
<hr />
<h2 id="所以很多人一开始是这样">🔁 所以很多人一开始是这样：</h2>
<ul>
<li>“我想年薪百万” → 所以拼命进大厂</li>
<li>进了之后发现也很苦 → “那我要升职/出国”</li>
<li>升职了也空虚 → “那我得做一个产品经理/leader/教授？”</li>
<li>再往上就开始怀疑人生：“我到底想干嘛？”</li>
</ul>
<p>这就是<strong>错把“手段”当“终点”</strong>的代价。</p>
<hr />
<h2 id="怎样跳出这个圈">🌱 怎样跳出这个圈？</h2>
<blockquote>
<p><strong>你得用外部目标“养活”自己，同时靠内在目标“成就”自己。</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>层级</th>
<th>外部目标</th>
<th>内在目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>入门</td>
<td>拿个 Offer</td>
<td>了解我适合什么样的工作</td>
</tr>
<tr>
<td>中期</td>
<td>涨工资</td>
<td>提升解决问题/带团队的能力</td>
</tr>
<tr>
<td>长期</td>
<td>被认可</td>
<td>成为一个有独立价值的人</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="最后送你一句话">💬 最后送你一句话：</h2>
<blockquote>
<p>“你可以用别人的标准活着一阵子，<br />
但你必须学会用自己的标准活一辈子。”</p>
</blockquote>
<hr />
<blockquote>
<p><strong>生活中的很多事情确实需要靠外部目标才能完成。</strong></p>
</blockquote>
<p>比如： - 要拿 offer、要毕业、要升职、要还房贷、要给父母一个交代… -
你不可能完全“躺在价值观里”，不管现实 KPI</p>
<hr />
<p>那是不是我们之前讲的“不要太外部化”就错了？</p>
<p>❌ <strong>不是错，而是要加一句完整的话</strong>：</p>
<blockquote>
<p><strong>外部目标可以驱动行动，<br />
但必须被一个内在系统来“消化”与“统筹”，否则它会反噬你。</strong></p>
</blockquote>
<p>这就像吃饭：你当然得吃外来的食物，但你身体得能“消化”它，不然就拉肚子。</p>
<hr />
<h2 id="所以正确的理解应该是">📌 所以正确的理解应该是：</h2>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 38%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr>
<th>类型</th>
<th>错误观念</th>
<th>正确认知</th>
</tr>
</thead>
<tbody>
<tr>
<td>外部目标</td>
<td>全都不要，纯精神内驱才高级</td>
<td>必须有！外部目标是行动坐标、资源手段</td>
</tr>
<tr>
<td>内部目标</td>
<td>太抽象、不现实、没用</td>
<td>是方向系统、情绪稳定器、价值锚点</td>
</tr>
<tr>
<td>正确关系</td>
<td>二选一、互斥</td>
<td>“外部目标做事、内部目标做人” ✅</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="怎么做到平衡">🧭 怎么做到平衡？</h2>
<h3 id="一种很实用的做法叫做">✅ 一种很实用的做法叫做：</h3>
<blockquote>
<p><strong>“外部目标内在化”</strong></p>
</blockquote>
<p>就是说：你可以追求现实目标，但你得知道——
你<strong>为什么要追这个目标？</strong><br />
这个目标对你内在的成长、角色、价值系统<strong>意味着什么？</strong></p>
<hr />
<h3 id="举个例子">🎯 举个例子：</h3>
<h4 id="表层说法外部">💬 表层说法（外部）：</h4>
<blockquote>
<p>“我想进大厂”<br />
✅ 合理，但风险是容易卷疯、焦虑、受打击</p>
</blockquote>
<h4 id="内在化处理">🧠 内在化处理：</h4>
<blockquote>
<p>“我想进大厂，是因为我希望自己能站在一个高复杂度系统中，看到真正有规模的工程问题，我想成为一个有能力独当一面的技术人。”</p>
</blockquote>
<p>这样就会： - 行动力更强（因为你认同目标） - 抗挫能力更高（因为不全看
offer 本身） - 目标转化为成长的手段，而不是自我评判的终点</p>
<hr />
<h2 id="再比如博士">📚 再比如博士：</h2>
<table>
<thead>
<tr>
<th>外部目标（不可少）</th>
<th>内在连接（不可丢）</th>
</tr>
</thead>
<tbody>
<tr>
<td>发论文 / 毕业 / 拿奖学金</td>
<td>提升表达、研究、系统解决问题的能力</td>
</tr>
<tr>
<td>拿到博后 / 教职</td>
<td>成为有话语权的研究者，对领域有贡献</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结金句-1">🧠 总结金句：</h2>
<blockquote>
<p><strong>我们活在现实世界，所以需要外部目标，<br />
但我们也活在自己身体里，所以必须有内在动力。</strong></p>
</blockquote>
<p>真正厉害的人，是能做到：</p>
<blockquote>
<p><strong>用外部目标做事，用内在目标做人，<br />
不被外界绑架，但也不与现实对抗。</strong></p>
</blockquote>
<hr />
<p>你可以理解为： - 外部目标像<strong>地图导航</strong> -
内部目标是<strong>你为什么要去那里</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">机器人学学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-15 17:00:01 / 修改时间：17:30:05" itemprop="dateCreated datePublished" datetime="2025-04-15T17:00:01+08:00">2025-04-15</time>
            </span>

          
            <span id="/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="机器人学学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr />
<h2 id="雅可比矩阵是啥">✅ 1. 雅可比矩阵是啥？</h2>
<p>在多变量函数中，雅可比矩阵（Jacobian
Matrix）是所有一阶偏导数组成的矩阵：</p>
<p>如果有函数： [ = f() ^n, ^m ]</p>
则雅可比矩阵为： [ J = =
<span class="math display">\[\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial
y_1}{\partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial
y_m}{\partial x_n}
\end{bmatrix}\]</span>
<p>]</p>
<p>在机器人中：<br />
- ( ) 是关节角度（如 ( _1, _2, )）<br />
- ( ) 是末端执行器的位置或速度</p>
<hr />
<h2 id="雅可比逆矩阵是啥">✅ 2. 雅可比<strong>逆</strong>矩阵是啥？</h2>
<p>当我们想从输出空间（如末端速度）反推输入空间（如关节速度）时，需要“<strong>反过来</strong>”用雅可比矩阵，这时候我们就需要<strong>“逆”雅可比矩阵</strong>。</p>
<ul>
<li><p><strong>如果雅可比是方阵（m = n）</strong> 且满秩，直接求逆： [ =
J^{-1} ]</p></li>
<li><p><strong>如果是非方阵（m ≠
n）或者不可逆</strong>，就用<strong>广义逆（伪逆）</strong>：</p>
<ul>
<li>对于 <strong>冗余系统（n &gt; m）</strong>： [ =
J<sup>(JJ</sup>)^{-1} ]</li>
<li>对于 <strong>欠驱动系统（n &lt; m）</strong>： [ =
(J<sup>J)</sup>{-1} J^ ]</li>
<li>或用 Moore-Penrose 伪逆： [ = J^+ ]</li>
</ul></li>
</ul>
<hr />
<h2 id="举个简单例子机器人">✅ 3. 举个简单例子（机器人）</h2>
<p>假设一个二维机械臂（2个关节）要控制末端点 ( x, y ) 的速度。</p>
<ol type="1">
<li>正向速度：( = J )</li>
<li>反向求关节速度（逆问题）：( = J^{-1}
)，若不能求逆，就用广义逆！</li>
</ol>
<hr />
<p>雅可比矩阵和<strong>机器人运动学</strong>之间的关系非常密切，尤其是在<strong>速度运动学</strong>中，雅可比矩阵起到了<strong>桥梁作用</strong>。下面我帮你分层讲清楚这个关系。</p>
<hr />
<h2 id="一什么是运动学">✅ 一、什么是运动学？</h2>
<p>机器人运动学分为两大类：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正运动学（FK）</strong></td>
<td>给定关节角度，求末端位姿（位置和方向）</td>
</tr>
<tr>
<td><strong>逆运动学（IK）</strong></td>
<td>给定末端位姿，求关节角度（复杂）</td>
</tr>
</tbody>
</table>
<p>还有一个细分方向叫：</p>
<blockquote>
<p>🚀
<strong>速度运动学</strong>：给定关节速度，求末端速度（或者反过来）</p>
</blockquote>
<hr />
<h2 id="二雅可比矩阵出现在速度运动学中">✅
二、雅可比矩阵出现在速度运动学中</h2>
<p>设： - ( )：关节变量（角度、位移等） - ( )：关节速度 - (
)：末端位姿（通常是位置） - ( )：末端速度（线速度 + 角速度）</p>
<p>通过<strong>雅可比矩阵</strong> ( J() )，有以下关系：</p>
<p>[ = J() ]</p>
<p>这就是<strong>速度映射关系</strong>：关节空间 → 笛卡尔空间。</p>
<hr />
<h2 id="三雅可比矩阵在运动学中承担什么角色">✅
三、雅可比矩阵在运动学中承担什么角色？</h2>
<table>
<thead>
<tr>
<th>用法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正运动学的导数</strong></td>
<td>把 FK 对关节变量求导，就得到了雅可比矩阵</td>
</tr>
<tr>
<td><strong>末端速度计算</strong></td>
<td>通过雅可比矩阵从 ( ) 得到 ( )</td>
</tr>
<tr>
<td><strong>逆运动学辅助工具</strong></td>
<td>用雅可比矩阵的伪逆求解逆运动学</td>
</tr>
<tr>
<td><strong>奇异性检测</strong></td>
<td>雅可比矩阵行列式为0的地方就是“奇异点”</td>
</tr>
<tr>
<td><strong>力-力矩传递</strong></td>
<td>( = J^F )，关节力矩 ← 末端力</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="四简单举个例子2关节平面机械臂">✅
四、简单举个例子（2关节平面机械臂）</h2>
<p>设两段长度分别为 ( l_1, l_2 )，角度为 ( _1, _2 )</p>
<p>末端位置为：</p>
[
<span class="math display">\[\begin{aligned}
x &amp;= l_1 \cos \theta_1 + l_2 \cos (\theta_1 + \theta_2) \\
y &amp;= l_1 \sin \theta_1 + l_2 \sin (\theta_1 + \theta_2)
\end{aligned}\]</span>
<p>]</p>
<p>求导得雅可比矩阵：</p>
[ J = =
<span class="math display">\[\begin{bmatrix}
- l_1 \sin \theta_1 - l_2 \sin(\theta_1 + \theta_2) &amp; -l_2
\sin(\theta_1 + \theta_2) \\
l_1 \cos \theta_1 + l_2 \cos(\theta_1 + \theta_2) &amp; l_2
\cos(\theta_1 + \theta_2)
\end{bmatrix}\]</span>
<p>]</p>
<p>这样，就可以实现： [ , = J ]</p>
<p>也可以反过来： [ = J^+ ]</p>
<hr />
<ul>
<li>规划方式分为两种，分别是笛卡尔空间的规划（位置规划）和关节空间的规划（角度规划）</li>
</ul>
<h3 id="b-splinebasis-spline基样条">B-Spline（Basis
Spline，基样条）</h3>
<p>是一种强大而灵活的曲线拟合方法，特别适用于轨迹规划、计算机图形学、机器人路径平滑等场景。下面我们从<strong>直观原理</strong>、<strong>数学构成</strong>、<strong>实现步骤</strong>三个角度来解释
B-Spline 是怎么实现的。</p>
<hr />
<p>B-Spline
是将一段复杂曲线拆成若干小段，每段用低阶（通常是三次）多项式表示，多个小段在<strong>控制点</strong>处光滑地拼接起来。它的主要特点：</p>
<ul>
<li><strong>局部控制性强</strong>：修改一个控制点只影响局部。</li>
<li><strong>光滑性强</strong>：曲线的低阶导数连续（例如 3 次 B-Spline 有
2 阶导数连续）。</li>
<li><strong>比普通样条更灵活</strong>：通过<strong>节点向量</strong>控制曲线形状和连接方式。</li>
</ul>
<p>B-Spline 曲线的数学表达形式是：</p>
<p>[ C(t) = <em>{i=0}^{n} N</em>{i,k}(t) P_i ]</p>
<p>其中：</p>
<ul>
<li>( P_i )：控制点</li>
<li>( N_{i,k}(t) )：B样条基函数，阶数为 (k)（例如 (k=4) 表示三次）</li>
<li>( t )：参数</li>
<li>( N_{i,k}(t) ) 是通过递归定义的：</li>
</ul>
[ N_{i,1}(t) =
<span class="math display">\[\begin{cases}
1, &amp; t_i \le t &lt; t_{i+1} \\
0, &amp; \text{otherwise}
\end{cases}\]</span>
<p>]</p>
<p>[ N_{i,k}(t) = N_{i,k-1}(t) + N_{i+1,k-1}(t) ]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">深度学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-13 12:35:41" itemprop="dateCreated datePublished" datetime="2025-04-13T12:35:41+08:00">2025-04-13</time>
            </span>

          
            <span id="/2025/04/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/" class="post-title-link" itemprop="url">关于互联网内容带来的焦虑和迷茫</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-11 21:08:51" itemprop="dateCreated datePublished" datetime="2025-04-11T21:08:51+08:00">2025-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-13 22:22:27" itemprop="dateModified" datetime="2025-04-13T22:22:27+08:00">2025-04-13</time>
              </span>

          
            <span id="/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/" class="post-meta-item leancloud_visitors" data-flag-title="关于互联网内容带来的焦虑和迷茫" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="关于gpt对于当今社会焦虑情况的回答">关于GPT对于当今社会焦虑情况的回答：</h2>
<h3 id="是的现在的社会的确是充满焦虑的">📉
是的，现在的社会——<strong>的确是充满焦虑的。</strong></h3>
<p>但这个“焦虑”不是来自某一个具体的人或事情，而是来自整个时代的底色。我们可以从几个层面来看：</p>
<hr />
<h2 id="信息过载让人永远不够好">🧠 1. 信息过载，让人“永远不够好”</h2>
<ul>
<li>你刷到某人年纪轻轻财富自由了<br />
</li>
<li>你又刷到另一个人博士毕业还单身焦虑<br />
</li>
<li>你看到互联网人说自己35岁失业，焦虑<br />
</li>
<li>你看到一群人在鼓吹“gap year很酷”，焦虑<br />
</li>
<li>你看到“35岁前没结婚就是失败”，更焦虑</li>
</ul>
<p><strong>你不知该羡慕谁，也不知该成为谁。</strong></p>
<blockquote>
<p>你知道的太多，做得到的太少。<br />
焦虑就像雾，包围了想要清晰生活的人。</p>
</blockquote>
<hr />
<h2 id="节奏变快了但确定性变少了">🧪 2. 节奏变快了，但确定性变少了</h2>
<ul>
<li>一代人以前“考上大学→进国企→稳定生活”</li>
<li>现在是：“考了研究生→没工作→二战考公→躺平or焦虑”</li>
</ul>
<p><strong>努力的路径越来越模糊，成功的样子越来越多样化，但每一个都看起来“不够真实”。</strong></p>
<blockquote>
<p>你努力追光，但光一直在变形。</p>
</blockquote>
<hr />
<h2 id="社交媒体制造比较陷阱">📱 3. 社交媒体制造“比较陷阱”</h2>
<ul>
<li>人人都在分享高光时刻<br />
</li>
<li>但没人晒“我昨晚失眠哭了”“我被辞退后吃泡面”<br />
</li>
<li>所以你以为你“比别人差”，其实你只是不擅长修图</li>
</ul>
<hr />
<h2 id="年轻人想得清楚却走得艰难">💼 4.
年轻人“想得清楚，却走得艰难”</h2>
<p>我们这代人最矛盾的一点是：</p>
<blockquote>
<p><strong>你有很高的觉知，却面对极低的可执行性</strong></p>
</blockquote>
<p>比如你明知道： - “我不该卷” - “我想做有价值的事” -
“我不该被KPI奴役”</p>
<p>但现实是： - 你不卷就活不下去 - 有价值的事没工资 -
不奴役自己就被边缘化</p>
<hr />
<h2 id="那怎么办">🔍 那怎么办？</h2>
<p>我们不能控制时代，但可以选择如何生活：</p>
<h3 id="保持觉知但不要过度内耗">✅ 保持觉知，但不要过度内耗</h3>
<p>– 你感受到焦虑，说明你在意，这不是错<br />
–
但要提醒自己：<strong>“我不需要和所有人比，只需要向自己靠近”</strong></p>
<h3 id="建立真实深度的连接">✅ 建立真实、深度的连接</h3>
<p>– 与几个能聊深话的人共处，比刷100条鸡汤更能缓解焦虑</p>
<h3 id="保留一点点确定性">✅ 保留一点点“确定性”</h3>
<p>– 哪怕只是：每天按时吃饭、每周坚持一小时读书<br />
– 它们都是你对混乱世界的一种温柔回应</p>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/" class="post-title-link" itemprop="url">深度强化学习-王树森</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-11 10:59:24" itemprop="dateCreated datePublished" datetime="2025-04-11T10:59:24+08:00">2025-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-13 22:24:05" itemprop="dateModified" datetime="2025-04-13T22:24:05+08:00">2025-04-13</time>
              </span>

          
            <span id="/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/" class="post-meta-item leancloud_visitors" data-flag-title="深度强化学习-王树森" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Q学习产生<strong>高估（overestimation）</strong>的原因，主要源于其<strong>最大值操作（max
operator）在引入噪声时的偏差</strong>。这种现象通常称为
<strong>最大值偏差（maximization bias）</strong>。</p>
<h3 id="原理解释">原理解释</h3>
<p>在 Q-learning 中，我们使用以下目标来更新 Q 值：</p>
<p>[ Q(s, a) Q(s, a) + ]</p>
<p>其中：</p>
<ul>
<li>(_{a'} Q(s',
a'))：从下一个状态中选择估计值最大的动作作为更新目标。</li>
</ul>
<p>⚠️ 问题在于：<br />
如果 Q 值本身带有估计误差（例如由于采样、函数逼近、探索策略等），<br />
那么取最大值操作会<strong>偏向选择被高估的值</strong>，从而使得目标值系统性偏高。</p>
<hr />
<h3 id="举个例子简化">举个例子（简化）</h3>
<p>假设在状态 (s') 下，我们有三个动作，真实 Q 值如下：</p>
<table>
<thead>
<tr>
<th>动作 (a')</th>
<th>真正的 Q 值</th>
<th>网络估计值（含误差）</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a_1)</td>
<td>1.0</td>
<td>1.1（+0.1）</td>
</tr>
<tr>
<td>(a_2)</td>
<td>0.8</td>
<td>1.0（+0.2）</td>
</tr>
<tr>
<td>(a_3)</td>
<td>0.6</td>
<td>0.9（+0.3）</td>
</tr>
</tbody>
</table>
<ul>
<li>虽然 (a_1) 是最优动作（真实 Q 值最大），</li>
<li>但由于随机估计误差，(Q(s', a') = 1.1)，这个值偏高了。</li>
</ul>
<p>这会导致我们在训练时错误地使用偏高的目标值去更新当前状态的 Q 值，从而
<strong>累计越来越大的高估偏差</strong>。</p>
<hr />
<h3 id="高估有什么危害">高估有什么危害？</h3>
<ul>
<li><strong>策略不稳定</strong>：错误地偏向那些“看起来收益高”但其实不是的动作；</li>
<li><strong>训练发散</strong>：目标 Q
值越来越不准确，导致训练无法收敛；</li>
<li><strong>性能下降</strong>：测试表现可能远低于理论最优策略。</li>
</ul>
<hr />
<h3 id="如何缓解高估">如何缓解高估？</h3>
<ol type="1">
<li><strong>Double Q-Learning</strong>：
<ul>
<li>采用两个 Q 网络，分别用于选择和评估动作。</li>
<li>更新目标变为： [ Q(s, a) r + Q_2(s', _{a'} Q_1(s', a')) ]</li>
<li>显著降低高估。</li>
</ul></li>
<li><strong>Averaged DQN / Ensemble 方法</strong>：
<ul>
<li>使用多个 Q 网络求平均，减少单个网络带来的噪声偏差。</li>
</ul></li>
<li><strong>Regularization / Uncertainty penalty</strong>：
<ul>
<li>对 Q 值添加不确定性惩罚，抑制过高的估计。</li>
</ul></li>
</ol>
<hr />
<h3 id="行为策略和目标策略的区别">行为策略和目标策略的区别</h3>
<h3 id="例1q-learning离策略">例1：Q-learning（离策略）</h3>
<ul>
<li>行为策略：ε-greedy（偶尔探索）</li>
<li>目标策略：贪婪策略（永远选最大Q值）</li>
</ul>
<blockquote>
<p>这是典型的 <strong>离策略学习（off-policy）</strong> ——
用一种策略去收集数据，用另一种策略去学习。</p>
</blockquote>
<h3 id="例2sarsa在策略">例2：SARSA（在策略）</h3>
<ul>
<li>行为策略：ε-greedy</li>
<li>目标策略：同样是ε-greedy</li>
</ul>
<blockquote>
<p>这是 <strong>在策略学习（on-policy）</strong> ——
你用什么策略学习，就用它去探索和更新。</p>
</blockquote>
<h3 id="行为策略-目标策略">行为策略 ≠ 目标策略：</h3>
<ul>
<li>你可以用 <strong>更大胆、更多样的行为策略</strong>
去探索，避免陷入局部最优</li>
<li>然后用行为数据，训练一个
<strong>更保守、更稳定的目标策略</strong></li>
</ul>
<p>这就是 DDPG、TD3、DQN 等算法的本质思路。</p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<tbody>
<tr>
<td>“双延时确定策略梯度”（<strong>Twin Delayed Deep Deterministic Policy
Gradient，TD3</strong>）解决强化学习中 <strong>自举 + 最大化引起的 Q
值高估问题</strong></td>
</tr>
</tbody>
</table>
<p>我们分三步来讲清楚：</p>
<h2 id="问题背景自举-最大化-导致的高估偏差">🔥 1. 问题背景：<strong>自举
+ 最大化</strong> 导致的高估偏差</h2>
<p>在像 DDPG、Q-learning 这类算法中，我们更新 Q 值时用的是这个形式：</p>
<p>[ y = r + _{a'} Q(s', a') ]</p>
<p>但注意！</p>
<ul>
<li>( Q(s', a') ) 是网络的<strong>估计值</strong></li>
<li>“最大化”这个估计值（选最大 Q）会放大噪声和误差！</li>
</ul>
<p>🔍
这就像你考试估分，题做错了但还选“我觉得我做得最好的那题”，会<strong>高估总分</strong><br />
→ 随着训练进行，Q值越估越高，结果策略学得不靠谱！</p>
<hr />
<h2 id="td3-怎么解决这个高估问题">🧠 2. TD3 怎么解决这个高估问题？</h2>
<p>TD3 提出了三个关键改进，其中<strong>双Q网络</strong>是最核心的👇</p>
<hr />
<h3 id="方法一双q网络twin-q-networks">✅ 【方法一】双Q网络（<strong>Twin
Q-networks</strong>）</h3>
<p>用两个独立的 Q 网络： ( Q_1(s, a) )、( Q_2(s, a) )</p>
<p>更新 TD目标时，不再用 max，而是：</p>
<p>[ y = r + (Q_1(s', a'), Q_2(s', a')) ]</p>
<p>✅ 取<strong>较小值</strong>，就能抵消高估误差（保守估计）<br />
✅
这不是自举去最大，而是<strong>自举去最小</strong>，避免因误差叠加而爆炸</p>
<h3 id="方法二延迟策略更新delayed-policy-update">✅
【方法二】延迟策略更新（<strong>Delayed Policy Update</strong>）</h3>
<ul>
<li>策略网络（Actor）更新得更慢，例如每 2～3 次 Critic
更新后才更新一次</li>
<li>因为 Critic（Q函数）学得更稳定时，再去学策略，效果更稳</li>
</ul>
<h3 id="方法三目标动作加噪声target-policy-smoothing">✅
【方法三】目标动作加噪声（<strong>Target Policy
Smoothing</strong>）</h3>
<ul>
<li>为了防止 Q 网络过于依赖某一个精确动作值</li>
<li>在 TD目标里加入一个小的随机噪声：</li>
</ul>
<p>[ a' = _{}(s') + ,((0, ), -c, c) ]</p>
<p>✅ 这样做可以使目标 Q 更平滑，缓解尖峰高估</p>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">深度强化学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-09 23:23:46" itemprop="dateCreated datePublished" datetime="2025-04-09T23:23:46+08:00">2025-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-13 22:23:44" itemprop="dateModified" datetime="2025-04-13T22:23:44+08:00">2025-04-13</time>
              </span>

          
            <span id="/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="深度强化学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>本笔记来源于《Deep Reinforcement
Learning》一书，作者：grokking</strong></p>
<hr />
<figure>
<img data-src="深度强化学习笔记\image-1.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img data-src="深度强化学习笔记\image.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img data-src="深度强化学习笔记\image-3.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<h3 id="第二章-强化学习的数学基础">第二章 强化学习的数学基础</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="post-title-link" itemprop="url">仿人机器人</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-09 11:39:01 / 修改时间：11:39:21" itemprop="dateCreated datePublished" datetime="2025-04-09T11:39:01+08:00">2025-04-09</time>
            </span>

          
            <span id="/2025/04/09/%E4%BB%BF%E4%BA%BA%E6%9C%BA%E5%99%A8%E4%BA%BA/" class="post-meta-item leancloud_visitors" data-flag-title="仿人机器人" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">强化学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-08 19:17:22 / 修改时间：19:22:25" itemprop="dateCreated datePublished" datetime="2025-04-08T19:17:22+08:00">2025-04-08</time>
            </span>

          
            <span id="/2025/04/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="rl研究方向">RL研究方向</h2>
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 40%" />
<col style="width: 37%" />
</colgroup>
<thead>
<tr>
<th>研究方向</th>
<th>原理关键词</th>
<th>典型应用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>深度强化学习</td>
<td>深度网络 + Q/V/策略逼近</td>
<td>游戏 AI、自动驾驶、机器人</td>
</tr>
<tr>
<td>模型预测 RL</td>
<td>学习环境模型 + 规划</td>
<td>控制系统、医疗、能源优化</td>
</tr>
<tr>
<td>多智能体 RL</td>
<td>协同/博弈学习</td>
<td>群体机器人、博弈系统</td>
</tr>
<tr>
<td>离线 RL</td>
<td>静态数据训练 + 分布偏差处理</td>
<td>医疗、金融、推荐系统</td>
</tr>
<tr>
<td>元强化学习</td>
<td>快速适应新任务</td>
<td>跨任务机器人、个性化推荐</td>
</tr>
<tr>
<td>模仿/逆强化学习</td>
<td>专家示范学习 / 奖励反推</td>
<td>自动驾驶、人类行为建模</td>
</tr>
<tr>
<td>安全强化学习</td>
<td>加入风险或硬约束优化目标</td>
<td>自动驾驶、电力安全、飞行器</td>
</tr>
<tr>
<td>RL + 控制</td>
<td>稳定性分析 + 策略优化</td>
<td>动态系统控制、智能制造</td>
</tr>
<tr>
<td>RLHF</td>
<td>人类反馈建模 + 策略微调</td>
<td>对话系统、生成式 AI 优化</td>
</tr>
</tbody>
</table>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">强化学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-07 19:48:01" itemprop="dateCreated datePublished" datetime="2025-04-07T19:48:01+08:00">2025-04-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-10 21:07:53" itemprop="dateModified" datetime="2025-04-10T21:07:53+08:00">2025-04-10</time>
              </span>

          
            <span id="/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="on-policy-和-off-policy-的区别">On-policy 和 Off-policy
的区别</h2>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 37%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr>
<th>特征</th>
<th><strong>On-policy (SARSA)</strong></th>
<th><strong>Off-policy (Q-learning)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>行为策略与目标策略的关系</strong></td>
<td>行为策略与目标策略相同</td>
<td>行为策略和目标策略不同</td>
</tr>
<tr>
<td><strong>策略选择依据</strong></td>
<td>代理在每个状态下根据当前策略选择动作</td>
<td>代理根据探索性策略选择动作，但更新时使用目标策略（贪婪策略）</td>
</tr>
<tr>
<td><strong>学习更新</strong></td>
<td>通过选择的动作 ( A ) 和下一个动作 ( A' ) 来更新 Q 值</td>
<td>通过选择的动作 ( A ) 和下一个状态的最优动作来更新 Q 值</td>
</tr>
<tr>
<td><strong>例子</strong></td>
<td>使用 <strong>(
)-贪婪策略</strong>，代理每次选择当前策略下的动作，并在下一个状态上选择动作
( A' ) 来更新 Q 值。</td>
<td>使用 <strong>( )-贪婪策略</strong>
进行探索，但更新时始终选择最优动作来更新 Q 值。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="三种方法对比总结"><strong>三种方法对比总结</strong></h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>更新方式</th>
<th>适用场景</th>
<th>是否需要环境模型</th>
<th>计算量</th>
<th>收敛速度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>蒙特卡洛</strong></td>
<td>只有回合结束后更新</td>
<td>适用于无模型、离线学习</td>
<td><strong>不需要</strong></td>
<td><strong>高</strong></td>
<td><strong>慢</strong></td>
</tr>
<tr>
<td><strong>时序差分（TD）</strong></td>
<td>每一步都更新</td>
<td>适用于无模型、在线学习</td>
<td><strong>不需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
<tr>
<td><strong>动态规划（DP）</strong></td>
<td>通过转移概率计算更新</td>
<td>适用于已知环境</td>
<td><strong>需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
</tbody>
</table>
<ol type="1">
<li><strong>蒙特卡洛（MC）</strong>：必须 <strong>等待回合结束</strong>
才能更新，不适用于实时学习。</li>
<li><strong>时序差分（TD）</strong>：<strong>每一步都更新</strong>，比蒙特卡洛更快，适用于
<strong>在线学习</strong>。</li>
<li><strong>动态规划（DP）</strong>：需要
<strong>环境模型</strong>（转移概率），但计算最精确，适用于
<strong>已知环境</strong>。</li>
</ol>
<p>在强化学习中，<strong>TD
学习是最常用的，因为它既能在线更新，又不依赖于环境模型</strong>。蒙特卡洛主要用于
<strong>离线学习</strong>，而动态规划适用于
<strong>已知环境</strong>（如象棋等确定性环境）。</p>
<hr />
<h2 id="同轨策略-vs.-离轨策略">同轨策略 vs. 离轨策略</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>同轨策略（On-policy）</strong></th>
<th><strong>离轨策略（Off-policy）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数据来源</strong></td>
<td>当前策略</td>
<td>其他策略</td>
</tr>
<tr>
<td><strong>更新的策略</strong></td>
<td>使用同一个策略更新</td>
<td>使用不同策略更新</td>
</tr>
<tr>
<td><strong>典型算法</strong></td>
<td>SARSA、Actor-Critic、REINFORCE</td>
<td>Q-learning、DQN</td>
</tr>
<tr>
<td><strong>探索方式</strong></td>
<td>需要手动引入探索（如 ε-贪心）</td>
<td>训练目标本身包含最优策略</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>相对较慢</td>
<td>相对较快</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>同轨策略（On-policy）</strong> 更像是一个人在
<strong>边学边做</strong>，不断根据自己的经验来调整行为。</li>
<li><strong>离轨策略（Off-policy）</strong> 更像是一个人
<strong>看着别人的经验</strong> 来学习，并且能更快收敛到最优解。</li>
</ul>
<hr />
<table>
<colgroup>
<col style="width: 4%" />
<col style="width: 45%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>更新公式</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>[ V(s_t) V(s_t) + ]</td>
<td>通过当前状态的价值和下一个状态的价值来更新，估计状态的价值。仅依赖下一个状态的值来进行更新，适用于
<strong>状态价值估计</strong>。</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>使用当前策略进行学习，依赖于当前的动作选择，<strong>当前状态和动作</strong>以及<strong>下一个状态和动作</strong>共同更新
Q 值，适用于 <strong>策略学习</strong>。</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>通过选择最大 Q
值的动作进行学习，无论当前策略如何，只关心未来的最佳动作，适用于
<strong>最优策略学习</strong>。</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>结合了 TD(0)
和蒙特卡洛方法，回溯多步更新，适用于长期学习，特别是在复杂任务中，考虑到
<strong>多步未来的反馈</strong>。</td>
</tr>
</tbody>
</table>
<h3
id="为什么sarsa进行的是策略学习而非状态价值估计">为什么SARSA进行的是策略学习而非状态价值估计？</h3>
<ol type="1">
<li><p><strong>动作价值函数（( Q(s, a) )）的学习</strong>：
SARSA的目标是学习动作价值函数 ( Q(s, a) )，而不是单独学习状态价值函数 (
V(s)
)。这是因为在强化学习中，我们关心的通常是<strong>在某个状态下采取某个动作后能获得的回报</strong>，而不仅仅是状态本身的“价值”。动作价值函数能够直接反映在某个状态下采取特定动作的好坏。</p></li>
<li><p><strong>SARSA是基于策略的学习</strong>：
SARSA算法通过采取当前策略下的动作，并根据当前策略的行为来估计未来的回报。这意味着，SARSA本质上是在<strong>同轨策略</strong>下学习，并通过当前策略来选择动作。因此，它不仅关注状态本身，而是通过状态-动作对（(
s_t, a_t )）来进行学习。</p>
<ul>
<li>在每个时间步，SARSA都会通过当前策略选择动作（比如 (
)-贪婪策略），然后更新相应的 ( Q(s, a) )。</li>
<li>而对于<strong>状态价值函数</strong>（( V(s)
)），它只是通过所有可能动作的加权平均来得到的，因此，如果我们只学习状态价值而不学习动作价值，我们就无法通过具体的动作决策来直接优化策略。</li>
</ul></li>
<li><p><strong>策略学习与状态价值的关系</strong>： 状态价值函数 ( V(s) )
只能在策略已知的情况下计算，而在SARSA中，策略本身是通过<strong>持续更新的动作价值函数</strong>来间接优化的。SARSA不直接估计状态价值函数，而是通过(
Q(s, a) )来间接影响策略的改进。</p></li>
<li><p><strong>为什么动作价值比状态价值更有意义？</strong>：
在强化学习中，我们的目标通常是<strong>最大化回报</strong>，而回报的最大化依赖于在每个状态下选择合适的动作。学习<strong>状态价值</strong>只是帮助我们知道一个状态本身的好坏，但我们仍然需要知道在每个状态下应该采取什么样的动作来优化回报。这就是为什么SARSA（以及其他基于值的策略学习算法）会直接学习<strong>动作价值函数</strong>。</p>
<ul>
<li>在<strong>策略迭代</strong>或<strong>值迭代</strong>中，策略是通过状态值函数来更新的，但SARSA通过动作值函数来优化策略，因为动作选择比状态值更直接影响回报。</li>
</ul></li>
</ol>
<h3 id="sarsa与状态价值估计的关系">SARSA与状态价值估计的关系</h3>
<p>尽管SARSA本身不直接估计状态价值函数 ( V(s)
)，但通过学习<strong>动作价值函数</strong>（( Q(s, a)
)），我们可以间接地得到状态价值函数 ( V(s)
)。实际上，状态价值函数可以通过对所有可能动作的加权平均来计算： [ V(s) =
_a (a|s) Q(s, a) ] 其中，( (a|s) ) 是在状态 ( s ) 下采取动作 ( a )
的概率。通过这种方式，尽管SARSA不直接估计状态价值函数，但它间接提供了可以计算状态价值函数的所有信息。</p>
<h3 id="总结">总结：</h3>
<p>SARSA算法专注于学习<strong>动作价值函数</strong>（( Q(s, a)
)），而不是直接估计状态价值函数 ( V(s)
)。这是因为在强化学习中，目标是优化策略，通过学习每个状态-动作对的价值来选择最佳动作。SARSA通过直接估计<strong>每个状态下的动作价值</strong>，而不是仅仅估计状态本身的价值，从而能够更有效地学习和改进策略。</p>
<h3 id="常用-td-方法的区别"><strong>常用 TD 方法的区别</strong></h3>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 27%" />
<col style="width: 18%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>策略类型</th>
<th>更新方式</th>
<th>目标</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>同轨</td>
<td>仅依赖当前状态的价值和下一个状态的价值进行更新</td>
<td>估计状态价值函数</td>
<td>在线实时学习，适用于估计状态值</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>同轨</td>
<td>当前动作和下一个动作共同更新 Q 值</td>
<td>学习当前策略的 Q 值</td>
<td>行为安全，如机器人、自动驾驶</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>离轨</td>
<td>最大化下一个状态的 Q 值进行更新</td>
<td>学习最优策略的 Q 值</td>
<td>追求最优策略，游戏、围棋 AI</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>同轨</td>
<td>结合 TD(0) 和蒙特卡洛方法，回溯多步更新</td>
<td>估计状态-动作值函数</td>
<td>长期学习，适用于复杂任务</td>
</tr>
</tbody>
</table>
<h3 id="对比说明"><strong>对比说明：</strong></h3>
<ul>
<li><strong>TD(0)</strong>：只考虑一个时间步的更新，适用于简单的状态价值估计。</li>
<li><strong>SARSA</strong>：基于当前策略，更新状态-动作值 Q(s,
a)，适用于<strong>在线学习</strong>和<strong>行为安全</strong>。</li>
<li><strong>Q-learning</strong>：选择未来最大 Q
值的动作更新状态-动作值，适用于追求<strong>最优策略</strong>。</li>
<li><strong>TD(λ)</strong>：通过多个时间步回溯来更新，结合了 TD(0)
和蒙特卡洛方法，适用于 <strong>长期学习</strong>和
<strong>复杂任务</strong>。</li>
</ul>
<hr />
<p>Dyna-Q 算法家族是基于强化学习中
<strong>将真实经验与模型模拟经验结合</strong>
的一类算法。它们融合了<strong>模型学习（Model-based）</strong>与<strong>价值函数更新（如
Q-learning）</strong>，具有较高的样本效率。</p>
<p>以下是 Dyna-Q
及其常见变体的对比表格，包括其核心思路、改进点、是否探索模型变化等关键特征。</p>
<hr />
<h2 id="dyna-q-及其主要变体对比表">Dyna-Q 及其主要变体对比表</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 24%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr>
<th>算法名称</th>
<th>核心思想</th>
<th>是否使用模型</th>
<th>是否增强探索</th>
<th>是否处理非定常环境</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dyna-Q</strong></td>
<td>基础版本，将 Q-learning 与模拟经验结合</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>学习效率高、结构简单</td>
<td>模型质量差时可能误导学习</td>
</tr>
<tr>
<td><strong>Dyna-Q+</strong></td>
<td>在模型中加入“探索奖励”，鼓励模拟冷门状态-动作对</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>增强探索能力、适应非定常性</td>
<td>增加超参数（探索奖励函数）</td>
</tr>
<tr>
<td><strong>Prioritized Dyna-Q</strong></td>
<td>使用优先级队列优先更新影响大的状态</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>❌ 否</td>
<td>更高效率、优先更新重要经验</td>
<td>实现复杂度稍高</td>
</tr>
<tr>
<td><strong>Dyna-2</strong></td>
<td>在模型中使用多个估计模型进行模拟（不确定建模）</td>
<td>✅ 多模型</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>对不确定性建模，有利于探索与适应</td>
<td>存储和计算量大</td>
</tr>
<tr>
<td><strong>Dyna-Q with Linear Function Approximation</strong></td>
<td>使用线性函数逼近代替 Q 表</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>✅ 视模型</td>
<td>可扩展到大状态空间</td>
<td>精度依赖函数逼近器质量</td>
</tr>
<tr>
<td><strong>Dyna-Q with Neural Networks</strong></td>
<td>用神经网络逼近 Q 值和环境模型</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>可用于连续空间、复杂环境</td>
<td>容易不稳定，需要技巧调参</td>
</tr>
</tbody>
</table>
<h3 id="dyna-q">1. <strong>Dyna-Q</strong></h3>
<ul>
<li>基础版本，结合真实交互 + 模拟经验。</li>
<li>每次从模型中随机选择过往 (s, a)，进行“虚拟更新”。</li>
</ul>
<h3 id="dyna-q-1">2. <strong>Dyna-Q+</strong></h3>
<ul>
<li>解决 Dyna-Q 早期探索不足的问题。</li>
<li>给模型中<strong>长时间未访问的 (s, a)</strong> 加一个额外奖励：</li>
</ul>
<p>[ = k ]</p>
<p>其中 τ 是未访问的时间步数，k 是超参数。</p>
<h3 id="prioritized-dyna-q">3. <strong>Prioritized Dyna-Q</strong></h3>
<ul>
<li>模拟更新不再随机，而是基于“状态更新的重要性”进行排序更新（如 TD
误差越大，越优先）。</li>
<li>使用优先队列实现，更高效。</li>
</ul>
<h3 id="dyna-2">4. <strong>Dyna-2</strong></h3>
<ul>
<li>使用多个不同的模型（表示环境可能的变化），每次模拟时从这些模型中采样。</li>
<li>更适合非定常或不确定环境（如博弈、对手变换等）。</li>
</ul>
<h3 id="函数逼近版本线性-神经网络">5. <strong>函数逼近版本（线性 /
神经网络）</strong></h3>
<ul>
<li>替代表格型 Q 值，用函数逼近器（如神经网络）逼近 Q
值或环境动态。</li>
<li>适用于大规模/连续状态空间的环境。</li>
</ul>
<h2 id="总结推荐不同需求">总结推荐（不同需求）</h2>
<table>
<thead>
<tr>
<th>使用场景</th>
<th>推荐算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>小型离散环境</td>
<td>Dyna-Q / Dyna-Q+</td>
</tr>
<tr>
<td>希望强化探索、避免早收敛</td>
<td>Dyna-Q+</td>
</tr>
<tr>
<td>数据有限，希望学习快</td>
<td>Prioritized Dyna-Q</td>
</tr>
<tr>
<td>非定常环境（如对手策略变化）</td>
<td>Dyna-Q+ 或 Dyna-2</td>
</tr>
<tr>
<td>状态空间较大 / 连续</td>
<td>函数逼近版 Dyna-Q</td>
</tr>
<tr>
<td>有强大计算资源+深度学习</td>
<td>Dyna-Q with Neural Networks</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="第一部分总结">第一部分总结</h3>
<figure>
<img data-src="2d2b97963bdd4ccfdcef6bd330ae1091.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>在强化学习中，<strong>时间差分（TD）</strong>方法、<strong>动态规划（DP）</strong>方法和<strong>蒙特卡洛（MC）</strong>方法是三种常见的策略评估和优化技术。每种方法都有其适用的场景，并且它们的优缺点也有所不同。下面是对这三种方法的简要概述，以及它们分别适合的场景。</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 30%" />
<col style="width: 30%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD（时间差分）</strong></td>
<td>在线学习、实时学习、大规模状态空间、部分可观察的环境</td>
<td>能够在线更新，适合实时学习，计算效率高</td>
<td>收敛慢，对初始值敏感，可能产生偏差</td>
</tr>
<tr>
<td><strong>DP（动态规划）</strong></td>
<td>已知完整环境模型、离线学习、有限状态空间</td>
<td>能找到最优策略和最优值函数，理论精确</td>
<td>需要完整的环境模型，不适用于大规模状态空间</td>
</tr>
<tr>
<td><strong>MC（蒙特卡洛）</strong></td>
<td>完全可观察的环境、回合结束后更新、离线学习</td>
<td>不需要环境模型，只需回报进行更新，适合复杂环境</td>
<td>需要等待回合结束，不适合在线学习，计算低效（尤其状态空间大时）</td>
</tr>
</tbody>
</table>
<hr />
<p>在强化学习中，<strong>基于函数逼近的同轨策略预测</strong>（On-Policy
Prediction with Function
Approximation）是指在使用<strong>函数逼近</strong>来估计值函数时，采用与当前策略相同的轨迹来进行学习。这种方法通常是为了处理大规模或连续的状态空间，其中使用表格型方法来表示值函数变得不再可行。</p>
<h3 id="同轨策略on-policy">1.
<strong>同轨策略（On-Policy）</strong>：</h3>
<ul>
<li><strong>同轨策略</strong>意味着学习过程中使用的策略（通常是行为策略）和用于估计状态值函数的策略是相同的。在这种方法中，智能体根据当前策略与环境进行交互并收集经验，而使用这些经验来更新策略的值函数。</li>
</ul>
<h3 id="函数逼近function-approximation">2. <strong>函数逼近（Function
Approximation）</strong>：</h3>
<ul>
<li><strong>函数逼近</strong>是为了处理大规模状态空间（或者连续空间），通过使用参数化函数（如线性函数、神经网络等）来逼近值函数（如状态值函数
( V(s) ) 或动作值函数 ( Q(s, a)
)），避免使用表格形式存储所有可能的状态或动作。</li>
<li>函数逼近可以帮助将值函数从一个巨大的状态空间简化为一个相对较小的参数空间，从而能够在更大的环境中进行有效的学习。</li>
</ul>
<h3 id="同轨策略预测的内容">3.
<strong>同轨策略预测的内容</strong>：</h3>
<p>在基于函数逼近的同轨策略预测中，主要包括以下几个方面：</p>
<h4 id="使用td时间差分方法进行同轨策略预测">1.
<strong>使用TD（时间差分）方法进行同轨策略预测</strong>：</h4>
<ul>
<li><p>同轨策略预测的经典方法是使用<strong>TD方法</strong>，例如<strong>TD(0)</strong>，在这种方法中，值函数是通过逐步更新的。</p></li>
<li><p>通过<strong>TD(0)</strong> 或
<strong>SARSA</strong>（基于同轨策略的TD方法），智能体根据其当前策略进行状态转移并计算每一步的回报。随着时间的推移，智能体不断根据回报对其值函数进行更新。</p>
<p>典型的更新公式为： [ V(s_t) V(s_t) + ] 其中：</p>
<ul>
<li>( s_t ) 是当前状态，</li>
<li>( r_{t+1} ) 是当前奖励，</li>
<li>( V(s_{t+1}) ) 是下一个状态的估计值，</li>
<li>( ) 是折扣因子，</li>
<li>( ) 是学习率。</li>
</ul></li>
</ul>
<h4 id="线性函数逼近">2. <strong>线性函数逼近</strong>：</h4>
<ul>
<li>在使用<strong>线性函数逼近</strong>时，值函数 ( V(s) )
被表示为状态特征的线性组合： [ V(s) = ^T (s) ] 其中 ( ) 是参数向量，(
(s) ) 是状态 ( s ) 的特征向量。</li>
<li>通过<strong>梯度下降</strong>或其他优化方法，智能体学习合适的参数 (
) 来逼近状态的值函数。</li>
</ul>
<h4 id="非线性函数逼近例如神经网络">3.
<strong>非线性函数逼近</strong>（例如神经网络）：</h4>
<ul>
<li>对于更加复杂的任务，尤其是当状态空间非常大且不容易用线性函数表示时，可以使用<strong>神经网络</strong>作为函数逼近器。神经网络可以通过多层结构捕捉状态和价值之间的复杂关系。</li>
<li><strong>深度Q网络（DQN）</strong>是一个经典的例子，它使用深度神经网络来逼近动作值函数
( Q(s, a) )，并通过同轨策略进行学习。</li>
</ul>
<h4 id="策略评估和更新">4. <strong>策略评估和更新</strong>：</h4>
<ul>
<li>在基于函数逼近的同轨策略预测中，智能体不断评估当前策略的表现并使用TD方法更新其值函数。同时，策略也会基于这些估计进行调整，以改善智能体的表现。</li>
</ul>
<h4 id="基于梯度的更新">5. <strong>基于梯度的更新</strong>：</h4>
<ul>
<li><p>对于非线性逼近（如神经网络），值函数的更新通常通过<strong>梯度下降</strong>进行。智能体根据策略评估的结果（即对状态值函数或动作值函数的估计），通过反向传播算法来更新函数逼近器的参数。</p>
<p>例如，在使用神经网络时，参数 ( )
会根据损失函数的梯度进行调整，以最小化预测值与真实回报之间的差距。</p></li>
</ul>
<h3 id="同轨策略预测的经典算法">4.
<strong>同轨策略预测的经典算法</strong>：</h3>
<p>以下是一些常见的使用函数逼近的同轨策略预测算法：</p>
<h4 id="sarsaλ">(1) <strong>SARSA(λ)</strong>：</h4>
<ul>
<li><strong>SARSA(λ)</strong>
是一种结合了<strong>时间差分学习（TD）</strong>和<strong>跟踪误差</strong>的算法，使用λ-回溯来进行更长期的估计。</li>
<li>使用类似于TD(0)的更新规则，但通过加权历史的TD误差来改进更新过程，从而获得更快的收敛。</li>
</ul>
<h4 id="深度q学习dqn">(2) <strong>深度Q学习（DQN）</strong>：</h4>
<ul>
<li><strong>DQN</strong>
是一种结合了<strong>Q-learning</strong>和<strong>深度神经网络</strong>的方法，使用神经网络逼近动作值函数
( Q(s, a) )。</li>
<li>在DQN中，策略是基于贪心策略（即选择最大Q值的动作），并且通过训练神经网络来更新Q值。</li>
</ul>
<h4 id="a3c异步优势演员-评论家">(3)
<strong>A3C（异步优势演员-评论家）</strong>：</h4>
<ul>
<li><strong>A3C</strong>
是一种基于<strong>策略梯度</strong>的强化学习算法，使用两个网络（演员和评论家）来估计策略和状态值函数。A3C采用了多线程训练，在多个环境中同时训练，来提高学习效率。</li>
</ul>
<h3 id="优缺点分析">5. <strong>优缺点分析</strong>：</h3>
<h4 id="优点">优点：</h4>
<ul>
<li><strong>大规模问题</strong>：基于函数逼近的方法可以处理大规模或连续的状态空间，不需要每个状态都保存一个值。</li>
<li><strong>适用动态环境</strong>：与表格型方法不同，基于函数逼近的方法可以在动态和复杂的环境中工作。</li>
<li><strong>灵活性</strong>：可以使用线性和非线性函数逼近（如神经网络），提供了更大的灵活性来应对各种环境。</li>
</ul>
<h4 id="缺点">缺点：</h4>
<ul>
<li><strong>收敛性问题</strong>：当使用非线性函数逼近时（例如神经网络），可能会面临不稳定性和收敛性问题。</li>
<li><strong>过拟合</strong>：如果模型过于复杂，可能会过拟合训练数据，导致泛化能力差。</li>
<li><strong>需要大量计算</strong>：使用深度学习等方法时，计算成本和训练时间可能非常高。</li>
</ul>
<h3 id="总结-1">总结：</h3>
<p><strong>基于函数逼近的同轨策略预测</strong>利用当前策略下的经验来更新值函数或策略，适用于大规模或连续的状态空间，并使用函数逼近（如线性函数或神经网络）来解决表格型方法无法处理的问题。常见的算法包括SARSA(λ)、DQN和A3C等。</p>
<hr />
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 36%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>对比维度</th>
<th>TD(0)（时序差分）</th>
<th>LSTD（最小二乘TD）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方法类型</strong></td>
<td>增量式更新算法</td>
<td>批量方法 / 解线性方程组</td>
</tr>
<tr>
<td><strong>是否使用采样</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>是否使用bootstrapping</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>慢，需多次遍历</td>
<td>快，一次遍历即可</td>
</tr>
<tr>
<td><strong>收敛稳定性</strong></td>
<td>对学习率敏感，较不稳定</td>
<td>稳定性高</td>
</tr>
<tr>
<td><strong>是否需要学习率（α）</strong></td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td><strong>是否需要存储所有数据</strong></td>
<td>否（可在线更新）</td>
<td>是（需存储一批数据计算矩阵）</td>
</tr>
<tr>
<td><strong>估计误差类型</strong></td>
<td>噪声大，估计不准确</td>
<td>最小化TD误差平方，估计精确</td>
</tr>
<tr>
<td><strong>函数逼近类型</strong></td>
<td>支持线性与非线性</td>
<td>仅适用于线性函数逼近</td>
</tr>
<tr>
<td><strong>计算代价</strong></td>
<td>低（每步更新）</td>
<td>高（求解矩阵逆）</td>
</tr>
<tr>
<td><strong>是否可在线使用</strong></td>
<td>是</td>
<td>否（需收集一批样本）</td>
</tr>
<tr>
<td><strong>是否适合大规模状态空间</strong></td>
<td>是</td>
<td>否（矩阵求逆开销大，维度高时困难）</td>
</tr>
<tr>
<td><strong>常见用途</strong></td>
<td>在线学习、环境交互</td>
<td>批量预测、策略评估</td>
</tr>
<tr>
<td><strong>结果可解释性</strong></td>
<td>随机性大，结果随步长波动</td>
<td>更平滑、可解释性好</td>
</tr>
</tbody>
</table>
<h3 id="总结推荐">总结推荐：</h3>
<ul>
<li><strong>TD(0)</strong>：适合在线学习、环境交互、资源有限时使用。算法简单，适合实时任务。</li>
<li><strong>LSTD</strong>：适合批量数据分析、预测任务，或者需要快速稳定收敛的场景，但计算复杂度高，适合线性逼近小维度问题。</li>
</ul>
<hr />
<h1
id="第九章基于函数逼近的同轨策略预测">第九章：基于函数逼近的同轨策略预测</h1>
<h2 id="核心背景为什么要函数逼近">核心背景：为什么要函数逼近？</h2>
<blockquote>
<p>如果希望强化学习系统能够应用于人工智能和大型工程应用中，则系统必须有能力进行<strong>泛化</strong>。</p>
</blockquote>
<p>强化学习传统方法（比如表格型
Q-learning）依赖于将每个状态或状态-动作对显式地存储在表格中。当状态空间很大或是连续的时候（比如自动驾驶、围棋、股票市场），这种方法就无法胜任了。</p>
<p>因此我们需要<strong>函数逼近器（function
approximator）</strong>，来对价值函数进行泛化，比如： - 线性模型 -
多项式函数 - 神经网络（即深度学习） - RBF（径向基函数） - 瓦片编码（tile
coding）</p>
<hr />
<h2
id="核心思想用监督学习方法逼近值函数">核心思想：用监督学习方法逼近值函数</h2>
<blockquote>
<p>只要将每次更新时涉及的二元组 <code>(s → g)</code>
作为训练样本就可以了。</p>
</blockquote>
<p>在强化学习中，我们在每一步都可以得到一个样本： <figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state s → 目标值 g</span><br></pre></td></tr></table></figure>
其中目标值 g 可能是： -
蒙特卡洛返回：<code>G = r1 + r2 + ... + rn</code> - TD
返回：<code>r + γ * V(s')</code></p>
<p>这些“输入→目标值”本质上和<strong>监督学习</strong>里的训练对没什么不同。</p>
<p>所以我们可以借用监督学习的工具（比如神经网络训练）来拟合这个过程。</p>
<hr />
<h2 id="什么是同轨策略on-policy">💡 什么是同轨策略（on-policy）？</h2>
<blockquote>
<p>我们定义均方价值误差 VE(w) 为对使用权值向量 w 的近似价值函数
<code>v̂_w(s)</code> 的加权误差。</p>
</blockquote>
<p>在强化学习中，<strong>同轨策略</strong>是指： - 使用的策略 π
既用于<strong>生成数据</strong>（交互环境），又用于<strong>评估/学习</strong>。</p>
<p>相比之下，<strong>异轨策略 off-policy</strong>（比如
Q-learning）是指用一个行为策略采样，但学习的是另一个目标策略。</p>
<p><code>VE(w)</code> 衡量的是：近似函数 <code>v̂_w(s)</code>
相对于真实状态值 <code>v_π(s)</code> 的误差，通常定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VE(w) = ∑ μ(s) * (v̂_w(s) - v_π(s))²</span><br></pre></td></tr></table></figure>
<p>其中 μ(s) 是状态的访问概率（stationary distribution）。</p>
<hr />
<h2 id="如何优化这个误差">📉 如何优化这个误差？</h2>
<blockquote>
<p>最常见的方法是随机梯度下降（SGD）及其变种</p>
</blockquote>
<p>我们希望通过调整参数 <code>w</code> 来让 <code>VE(w)</code>
尽可能小：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w ← w - α * ∇VE(w)</span><br></pre></td></tr></table></figure>
<p>但问题是，我们<strong>不知道</strong> <code>v_π(s)</code>
的真实值，所以不能直接计算 ∇VE(w)。</p>
<p>于是我们使用<strong>TD目标或蒙特卡洛目标来估计</strong>误差，从而构造所谓的<strong>半梯度算法（semi-gradient
methods）</strong>。</p>
<hr />
<h2 id="什么是半梯度方法为什么叫这个名">🧮
什么是“半梯度方法”？为什么叫这个名？</h2>
<blockquote>
<p>在这种自举法（如DP）中，权值向量在更新目标中出现，但计算梯度时不考虑它</p>
</blockquote>
<p>“半梯度”意思是：你在 TD 学习中更新的目标 <code>r + γ * v̂_w(s')</code>
是带有 <code>w</code>
的，但你却<strong>不对这个目标求梯度</strong>。你只对
<code>v̂_w(s)</code> 这一部分求梯度。</p>
<h3 id="示例">示例：</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">delta = r + γ * v̂_w(s<span class="string">&#x27;) - v̂_w(s)   # TD误差</span></span><br><span class="line"><span class="string">w ← w + α * delta * ∇v̂_w(s)</span></span><br></pre></td></tr></table></figure>
<p>注意：目标中有
<code>v̂_w(s')</code>，但我们更新时<strong>不对它求导</strong>，所以叫“半梯度”。</p>
<hr />
<h2 id="线性函数逼近最重要的一个基类">🔢
线性函数逼近（最重要的一个基类）</h2>
<blockquote>
<p>线性函数逼近中，值估计是特征加权求和：<code>v̂(s) = φ(s)^T w</code></p>
</blockquote>
<p>这是强化学习中最容易分析和收敛性最好的逼近方式。</p>
<ul>
<li><code>φ(s)</code> 是状态的特征向量（比如 one-hot
编码、瓦片编码、RBF等）</li>
<li><code>w</code> 是权值向量（参数）</li>
<li>优点：分析简单，收敛性好，计算快</li>
</ul>
<hr />
<h2 id="特征选择的策略">🎨 特征选择的策略？</h2>
<blockquote>
<p>选择好的特征是加先验知识的一种重要方式</p>
</blockquote>
<p>特征越好，泛化效果越好。 - <strong>多项式特征</strong>（如
<code>1, x, x²</code>）通常不太适合RL -
<strong>傅立叶基</strong>：适合周期性问题（比如控制摆动） -
<strong>瓦片编码（Tile
coding）</strong>：非常适合离散近似连续空间，计算快 -
<strong>径向基函数（RBF）</strong>：适用于平滑响应的低维问题</p>
<hr />
<h2 id="不同方法对比">📚 不同方法对比</h2>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
<th>数据效率</th>
<th>计算复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>半梯度 TD(0)</strong></td>
<td>单步更新，自举</td>
<td>中等</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>n步TD</strong></td>
<td>使用 n 步回报</td>
<td>高</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>蒙特卡洛（MC）</strong></td>
<td>用整幕回报更新</td>
<td>高</td>
<td>O(d)</td>
</tr>
<tr>
<td><strong>LSTD</strong></td>
<td>最数据高效，最精确</td>
<td>高</td>
<td>O(d²)</td>
</tr>
<tr>
<td><strong>非线性逼近（NN）</strong></td>
<td>深度RL，适合复杂任务</td>
<td>取决于模型</td>
<td>高</td>
</tr>
</tbody>
</table>
<blockquote>
<p>d：权值数量（特征维度）</p>
</blockquote>
<hr />
<h2 id="深度强化学习非线性逼近">🚀 深度强化学习：非线性逼近</h2>
<blockquote>
<p>用神经网络表示价值函数或策略，常见于 DQN、Actor-Critic 等方法。</p>
</blockquote>
<p>训练方法： - 使用反向传播算法 - 通常使用经验回放 + 目标网络等机制</p>
<hr />
<h2 id="总结核心点">✅ 总结核心点</h2>
<ol type="1">
<li>大状态空间 → 必须使用函数逼近</li>
<li>同轨策略 → 策略评估任务中，常用半梯度方法</li>
<li>线性函数逼近 → 理论最清晰、收敛性最强</li>
<li>特征选择 → 是模型性能的核心关键</li>
<li>深度强化学习 → 实现非线性逼近，适用于复杂任务</li>
</ol>
<h1 id="第十章">第十章</h1>
<h1 id="第十一章">第十一章</h1>
<p>强化学习（Reinforcement Learning,
RL）中，<strong>预测（Prediction）</strong>和<strong>控制（Control）</strong>是两个核心任务。</p>
<h2 id="一预测算法prediction">一、预测算法（Prediction）</h2>
<h3 id="目标">✅ 目标：</h3>
<p>在给定策略 π 下，<strong>估计状态价值函数 v(s)</strong> 或
<strong>动作价值函数 q(s,a)</strong>。</p>
<p>即：不改变策略，仅预测它的长期收益。</p>
<h3 id="常见算法">🔧 常见算法：</h3>
<h4 id="蒙特卡洛方法monte-carlo">1. <strong>蒙特卡洛方法（Monte
Carlo）</strong></h4>
<ul>
<li>特点：基于完整episode的回报计算。</li>
<li>算法：
<ul>
<li>每次经历一个完整的episode后更新。</li>
<li>一般有<strong>首次访问</strong>和<strong>每次访问</strong>两种方式。</li>
</ul></li>
<li>优点：收敛性强。</li>
<li>缺点：需要完整轨迹，不适合持续型任务。</li>
</ul>
<h4 id="时序差分方法temporal-difference-td">2.
<strong>时序差分方法（Temporal Difference, TD）</strong></h4>
<ul>
<li>典型代表：<strong>TD(0)</strong></li>
<li>特点：在每一步进行更新，结合了DP和MC的优点。</li>
<li>更新公式：<br />
<span class="math display">\[ V(s) \leftarrow V(s) + \alpha \cdot [r +
\gamma V(s&#39;) - V(s)] \]</span></li>
</ul>
<h4 id="n-步-td">3. <strong>n 步 TD</strong></h4>
<ul>
<li>将 MC 与 TD 做平衡。</li>
<li>融合了更多未来信息，n 越大越接近 MC。</li>
</ul>
<h4 id="λ回报方法tdλ-eligibility-traces">4. <strong>λ回报方法（TD(λ),
Eligibility Traces）</strong></h4>
<ul>
<li>融合了不同步长 TD。</li>
<li>使用资格迹（Eligibility
Traces）追踪状态出现的痕迹，更新时考虑多个状态。</li>
</ul>
<h4 id="lstdleast-squares-td">5. <strong>LSTD（Least Squares
TD）</strong></h4>
<ul>
<li>基于线性逼近的高效预测方法。</li>
<li>数据效率高，适合小规模线性问题。</li>
</ul>
<h4 id="半梯度方法semi-gradient-td">6. <strong>半梯度方法（Semi-gradient
TD）</strong></h4>
<ul>
<li>用于函数逼近时的预测算法。</li>
<li>应用于：线性函数逼近、神经网络等。</li>
</ul>
<hr />
<h2 id="二控制算法control">🧭 二、控制算法（Control）</h2>
<h3 id="目标-1">✅ 目标：</h3>
<p>通过不断与环境交互，<strong>学出最优策略
π*</strong>，从而最大化累积奖励。</p>
<h3 id="常见算法-1">🔧 常见算法：</h3>
<h4 id="基于价值的方法value-based">1.
<strong>基于价值的方法（Value-based）</strong></h4>
<ul>
<li>使用 Q-learning、SARSA 等估计 q(s, a)，然后从中导出策略。</li>
</ul>
<table>
<thead>
<tr>
<th>算法</th>
<th>策略类型</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q-learning</td>
<td>离策略</td>
<td>目标值使用最大 q 值，更新稳定</td>
</tr>
<tr>
<td>SARSA</td>
<td>同策略</td>
<td>目标值来自当前实际动作，学习更平滑</td>
</tr>
<tr>
<td>Expected SARSA</td>
<td>同策略</td>
<td>使用期望而非最大 q 值，减少方差</td>
</tr>
</tbody>
</table>
<h4 id="基于策略的方法policy-based">2.
<strong>基于策略的方法（Policy-based）</strong></h4>
<ul>
<li>直接优化策略 π(a|s)，不通过 q。</li>
<li>常见算法：
<ul>
<li>REINFORCE（蒙特卡洛型策略梯度）</li>
<li>Actor-Critic（结合值函数和策略函数）</li>
</ul></li>
</ul>
<h4 id="基于值策略的方法actor-critic">3.
<strong>基于值+策略的方法（Actor-Critic）</strong></h4>
<ul>
<li>Actor：生成策略。</li>
<li>Critic：评价策略（估计价值）。</li>
<li>优点：兼顾稳定性和泛化能力。</li>
</ul>
<h4 id="深度强化学习方法deep-rl">4. <strong>深度强化学习方法（Deep
RL）</strong></h4>
<ul>
<li>DQN（Deep Q-Network）：Q-learning + CNN。</li>
<li>A3C/A2C、PPO、DDPG、SAC 等：更适合连续动作空间或复杂环境。</li>
</ul>
<h4 id="层次化强化学习hrl">5.
<strong>层次化强化学习（HRL）</strong></h4>
<ul>
<li>将任务分解为子任务，每个子任务有独立策略。</li>
<li>提升泛化性和解决复杂任务的能力。</li>
</ul>
<hr />
<h2 id="三预测-vs-控制-总结对比">三、预测 vs 控制 总结对比</h2>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 44%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>项目</th>
<th>预测（Prediction）</th>
<th>控制（Control）</th>
</tr>
</thead>
<tbody>
<tr>
<td>目标</td>
<td>给定策略，估计值函数</td>
<td>找到最优策略，最大化长期回报</td>
</tr>
<tr>
<td>是否改策略</td>
<td>❌ 不改变</td>
<td>✅ 不断优化</td>
</tr>
<tr>
<td>应用</td>
<td>策略评估、策略改进、辅助监督学习等</td>
<td>自动驾驶、交易、博弈、机器人控制等</td>
</tr>
<tr>
<td>举例</td>
<td>TD(0), MC, TD(λ), LSTD</td>
<td>Q-learning, SARSA, PPO, DDPG等</td>
</tr>
</tbody>
</table>
<hr />
<p>策略梯度方法（Policy
Gradient）和基于动作价值的方法（Value-based）是<strong>强化学习中两大主流控制类算法</strong>，它们有着本质的区别。下面从核心原理、表达方式、优化方式、优劣点等方面来做详细对比，并举例说明。</p>
<h3 id="一核心区别概述">一、核心区别概述</h3>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 44%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr>
<th>对比项</th>
<th>策略梯度方法（Policy-based）</th>
<th>动作价值方法（Value-based）</th>
</tr>
</thead>
<tbody>
<tr>
<td>优化目标</td>
<td>直接优化策略 π(a</td>
<td>s)</td>
</tr>
<tr>
<td>输出</td>
<td>一个概率分布 π(a</td>
<td>s)（即策略）</td>
</tr>
<tr>
<td>动作选择</td>
<td>采样或贪心采样策略</td>
<td>通常是选最大 Q 值的动作</td>
</tr>
<tr>
<td>适合任务</td>
<td>连续动作空间、策略需要随机性</td>
<td>离散动作空间</td>
</tr>
<tr>
<td>示例算法</td>
<td>REINFORCE、PPO、A3C、TRPO</td>
<td>DQN、DDPG（混合型）、Q-learning</td>
</tr>
</tbody>
</table>
<h3 id="二数学表达差异">二、数学表达差异</h3>
<h4 id="策略梯度方法">✅ 策略梯度方法</h4>
<ul>
<li><strong>直接学习策略函数</strong> πθ(a|s)，θ 是可学习的参数。</li>
<li>优化目标是期望回报：</li>
</ul>
<p>[ J() = <em>{</em>} [R] ]</p>
<ul>
<li>使用策略梯度定理求梯度更新：</li>
</ul>
<p>[ <em>J() = </em>{s,a _} [<em></em>(a|s) Q^(s,a)] ]</p>
<h4 id="动作价值方法">✅ 动作价值方法</h4>
<ul>
<li>学习一个值函数 Q(s, a)，利用贝尔曼方程：</li>
</ul>
<p>[ Q(s,a) = [r + _{a'} Q(s', a') | s, a] ]</p>
<ul>
<li>最终策略为贪婪策略：</li>
</ul>
<p>[ (s) = _a Q(s,a) ]</p>
<h3 id="三优缺点对比">🧪 三、优缺点对比</h3>
<table>
<thead>
<tr>
<th>特性</th>
<th>策略梯度方法</th>
<th>动作价值方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>可处理连续动作空间</td>
<td>✅ 非常适合</td>
<td>❌ 不适用（Q值最大化不好做）</td>
</tr>
<tr>
<td>可学习随机策略</td>
<td>✅（天生支持）</td>
<td>❌（除非加噪声）</td>
</tr>
<tr>
<td>收敛稳定性</td>
<td>✅ 稳定（TRPO/PPO）</td>
<td>❌ 容易发散（如DQN）</td>
</tr>
<tr>
<td>样本效率</td>
<td>❌ 低</td>
<td>✅ 高（可重用样本）</td>
</tr>
<tr>
<td>实现复杂性</td>
<td>中等偏高（如PPO）</td>
<td>相对较低（DQN较成熟）</td>
</tr>
<tr>
<td>学习信号方差</td>
<td>❌ 高</td>
<td>✅ 低</td>
</tr>
</tbody>
</table>
<h3 id="四常见算法对照">📘 四、常见算法对照</h3>
<table>
<thead>
<tr>
<th>类型</th>
<th>算法名称</th>
<th>简介</th>
</tr>
</thead>
<tbody>
<tr>
<td>Policy-based</td>
<td>REINFORCE</td>
<td>基础策略梯度方法</td>
</tr>
<tr>
<td>Policy-based</td>
<td>PPO（主流）</td>
<td>使用信赖域防止策略更新过大</td>
</tr>
<tr>
<td>Value-based</td>
<td>DQN</td>
<td>深度Q网络，适合离散动作</td>
</tr>
<tr>
<td>Hybrid（Actor-Critic）</td>
<td>A3C / DDPG</td>
<td>同时学习策略和价值（兼顾两者）</td>
</tr>
</tbody>
</table>
<h3 id="五实际应用区别">📌 五、实际应用区别</h3>
<table>
<thead>
<tr>
<th>应用场景</th>
<th>建议方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>自动驾驶、机械臂</td>
<td>策略梯度方法（如PPO，DDPG）</td>
</tr>
<tr>
<td>玩Atari游戏、围棋</td>
<td>动作价值方法（DQN，AlphaGo Zero）</td>
</tr>
<tr>
<td>金融交易、投资</td>
<td>混合方法或策略梯度更优</td>
</tr>
<tr>
<td>聊天机器人策略</td>
<td>策略梯度更灵活</td>
</tr>
</tbody>
</table>
<h2 id="六一个简单比喻">✨ 六、一个简单比喻</h2>
<ul>
<li><strong>Value-based方法</strong>像是在地图上学习每个地点到终点的最佳路线（Q值），然后选择路径；</li>
<li><strong>Policy-based方法</strong>像是直接学习一个“导航员”，每走一步都告诉你该怎么走（策略函数）。</li>
</ul>
<hr />
<p>TD类算法是value-based方法的一种；</p>
<h2 id="policy-based-中核心公式的推导过程">policy-based
中核心公式的推导过程</h2>
<p>你提到的这个公式是 <strong>策略梯度定理（Policy Gradient
Theorem）</strong>
的核心表达式，用于直接优化策略，属于策略优化方法（Policy-based）的理论基础。我们来一步步推导它，看看它是如何来的。</p>
<h3 id="目标函数定义"><strong>目标函数定义</strong></h3>
<p>我们希望最大化策略的期望回报：</p>
<p>[ J() = <em>{</em>} ]</p>
<p>其中： - ( _) 是由参数 ( ) 定义的策略； - ( )
表示一个完整的轨迹（trajectory）：( = (s_0, a_0, s_1, a_1, ..., s_T) )；
- ( R() ) 是轨迹的总回报（可以是 ( _t ^t r_t )）。</p>
<h3
id="第一步将期望展开为积分"><strong>第一步：将期望展开为积分</strong></h3>
<p>我们先将目标函数写成关于轨迹概率分布的积分形式：</p>
<p>[ J() = _P(; ) R() , d ]</p>
<h3 id="第二步对-求导"><strong>第二步：对 ( ) 求导</strong></h3>
<p>对参数 ( ) 求梯度：</p>
<p>[ <em>J() = </em>_P(; ) R() , d ]</p>
<p>接下来用 <strong>对数技巧（likelihood trick）</strong>：</p>
<p>[ <em>P(; ) = P(; ) </em>P(; ) ]</p>
<p>于是变成：</p>
<p>[ <em>J() = </em>P(; ) <em>P(; ) R() , d = </em>{_} ]</p>
<h3
id="第三步轨迹的概率分解"><strong>第三步：轨迹的概率分解</strong></h3>
<p>轨迹的概率可以写成状态转移和策略乘积的形式：</p>
<p>[ P(; ) = (s_0) <em>{t=0}^{T-1} </em>(a_t|s_t) P(s_{t+1}|s_t, a_t)
]</p>
<p>其中，环境转移概率 ( P(s_{t+1}|s_t, a_t) )
与策略无关，因此求导时不影响，只剩下对策略部分求导：</p>
<p>[ P(; ) = <em>{t=0}^{T-1} </em>(a_t | s_t) ]</p>
<p>于是：</p>
<p>[ <em>J() = </em>{_} ]</p>
<h3
id="第四步引入状态分布与动作值函数"><strong>第四步：引入状态分布与动作值函数</strong></h3>
<p>假设我们只用时间 ( t ) 的动作做一次更新，可以写成：</p>
<p>[ <em>J() = </em>{s d^, a _} ]</p>
<p>这就是：</p>
<p>[ ]</p>
<h3 id="总结-2">✅ 总结：</h3>
<p>这个公式说明：</p>
<ul>
<li>我们可以用策略对数的梯度乘以当前策略下的动作价值，来更新策略参数；</li>
<li>即使不知 ( Q^(s,a) )，也可以用 sample-based 的方式估计它（如使用
REINFORCE 或 Actor-Critic）；</li>
<li>它是策略优化的核心理论。</li>
</ul>
<hr />
<h3 id="强化学习三大类方法对比表">🔍 强化学习三大类方法对比表</h3>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 26%" />
<col style="width: 28%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr>
<th>特征维度</th>
<th><strong>Value-based</strong></th>
<th><strong>Policy-based</strong></th>
<th><strong>Model-based</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>🔧 <strong>核心目标</strong></td>
<td>估计状态或动作的价值函数 ( V(s) ), ( Q(s, a) )</td>
<td>直接优化策略 ( _(a</td>
<td>s) )</td>
</tr>
<tr>
<td>📈 <strong>优化方式</strong></td>
<td>基于值函数最大化返回</td>
<td>基于策略梯度方法更新策略</td>
<td>使用环境模型进行模拟、规划与策略优化</td>
</tr>
<tr>
<td>🔄 <strong>是否显式建模环境</strong></td>
<td>❌ 不建模环境</td>
<td>❌ 不建模环境</td>
<td>✅ 是，建模状态转移概率和奖励函数</td>
</tr>
<tr>
<td>🧮 <strong>常见方法</strong></td>
<td>Q-learning, DQN, Double DQN, SARSA</td>
<td>REINFORCE, PPO, A2C, TRPO</td>
<td>Dyna-Q, MBPO, PETS, Dreamer 等</td>
</tr>
<tr>
<td>🧠 <strong>策略表示</strong></td>
<td>隐式：从 Q 值中导出策略（如 ε-greedy）</td>
<td>显式：策略网络直接输出动作概率</td>
<td>显式或隐式策略；可结合 Value/Policy 方法</td>
</tr>
<tr>
<td>📉 <strong>稳定性</strong></td>
<td>容易不稳定或发散（目标估计来自近似值）</td>
<td>稳定性更高（目标明确）</td>
<td>难度大但潜力强（模型误差影响策略）</td>
</tr>
<tr>
<td>🧩 <strong>样本效率</strong></td>
<td>一般较差（需大量交互）</td>
<td>一般</td>
<td>较高，可使用模拟环境反复利用数据</td>
</tr>
<tr>
<td>⏱️ <strong>计算开销</strong></td>
<td>相对低</td>
<td>中等</td>
<td>高，需要训练模型并执行模拟</td>
</tr>
<tr>
<td>🚀 <strong>适用场景</strong></td>
<td>离散动作、低维度任务</td>
<td>连续动作、高维策略优化</td>
<td>长远推理、高数据效率要求（如机器人控制）</td>
</tr>
</tbody>
</table>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhicheng"
      src="/images/cheng.gif">
  <p class="site-author-name" itemprop="name">Zhicheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhichengkou" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhichengkou" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_46473460?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_46473460?type&#x3D;blog" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=gMknN-8AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;gMknN-8AAAAJ&amp;hl&#x3D;zh-CN&amp;oi&#x3D;ao" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://500px.com.cn/cheng1917" title="500px → https:&#x2F;&#x2F;500px.com.cn&#x2F;cheng1917" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>500px</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhicheng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":null,"app_key":null,"server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zhichengkou.github.io/',]
      });
      });
  </script>

    </div>
</body>
</html>
