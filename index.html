<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhichengkou.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Zhicheng">
<meta property="og:url" content="http://zhichengkou.github.io/index.html">
<meta property="og:site_name" content="Zhicheng">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Zhicheng">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://zhichengkou.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Zhicheng</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhicheng</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">深度学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-23 15:35:03 / 修改时间：21:09:44" itemprop="dateCreated datePublished" datetime="2025-04-23T15:35:03+08:00">2025-04-23</time>
            </span>

          
            <span id="/2025/04/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="注意力机制与mlp">注意力机制与MLP</h1>
<h2 id="自注意力self-attention">自注意力（Self-Attention）</h2>
<blockquote>
<p>自注意力的核心思想是：<strong>每个输入向量（比如一个词、一个状态）都对其它所有输入进行“注意力加权”，从而自己更新自己。</strong></p>
</blockquote>
<p>这和传统神经网络不同，它不只是“看自己”，而是“看大家，然后决定自己该变成啥样”。</p>
<h2
id="举个机器人控制的例子强化学习场景">举个机器人控制的例子（强化学习场景）</h2>
<p>机器人过去 5 步状态： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ s₁, s₂, s₃, s₄, s₅ ]</span><br></pre></td></tr></table></figure></p>
<p>你想让当前状态 <code>s₅</code>
能<strong>综合考虑历史轨迹、预测未来动作</strong>。</p>
<ul>
<li>用自注意力，每个 <code>sᵢ</code> 都看一眼其它状态；</li>
<li>输出的是一个“上下文增强后的状态向量”，更适合拿来做策略决策。</li>
</ul>
<hr />
<h2
id="pytorch-示例代码使用-nn.multiheadattention-实现-self-attention">🔧
PyTorch 示例代码（使用 <code>nn.MultiheadAttention</code> 实现
Self-Attention）</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟一个状态序列输入（比如 5 步状态，每步是64维向量）</span></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line">seq_len = <span class="number">5</span></span><br><span class="line">embed_dim = <span class="number">64</span></span><br><span class="line">x = torch.rand(batch_size, seq_len, embed_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多头自注意力：embed_dim必须能被num_heads整除</span></span><br><span class="line">self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=<span class="number">4</span>, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自注意力：Q=K=V=x</span></span><br><span class="line">output, attn_weights = self_attn(x, x, x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.shape)       <span class="comment"># [1, 5, 64]</span></span><br><span class="line"><span class="built_in">print</span>(attn_weights.shape) <span class="comment"># [1, 4, 5, 5]  → 4个头的注意力图</span></span><br></pre></td></tr></table></figure>
<hr />
<h2 id="自注意力原理图简略">自注意力原理图（简略）</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">输入序列：  [ x₁, x₂, x₃, ..., xₙ ]</span><br><span class="line">                   ↓</span><br><span class="line">        每个位置都对其他位置做注意力加权</span><br><span class="line">                   ↓</span><br><span class="line">输出序列：  [ x₁&#x27;, x₂&#x27;, x₃&#x27;, ..., xₙ&#x27; ]</span><br></pre></td></tr></table></figure>
<p>每个输出向量 ( x_i’ )
是所有输入向量的加权和（权重由注意力计算得出）。</p>
<hr />
<h2 id="优点总结">优点总结</h2>
<table>
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr>
<th>优点</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>融合上下文</td>
<td>每个位置都能看全局信息，不像 RNN 只看前面</td>
</tr>
<tr>
<td>并行计算</td>
<td>相比 RNN 快得多，可以全部一起算</td>
</tr>
<tr>
<td>可解释性强</td>
<td>注意力权重告诉你“谁对谁重要”</td>
</tr>
<tr>
<td>万能积木</td>
<td>Transformer、ViT、Decision Transformer、GPT 都在用</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结一句话">总结一句话：</h2>
<blockquote>
<p>自注意力是一种让每个元素都“看别人来强化自己”的机制，能捕捉长距离依赖、适配时序数据，是现代
AI 模型的核心模块。</p>
</blockquote>
<hr />
<p>从<strong>物理意义</strong>上深入理解
Q（Query）、K（Key）、V（Value）</p>
<hr />
<h2 id="自注意力中-q-k-v-的物理含义">自注意力中 Q / K / V
的物理含义</h2>
<table>
<colgroup>
<col style="width: 5%" />
<col style="width: 13%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr>
<th>符号</th>
<th>全称</th>
<th>物理含义（直觉）</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q</td>
<td>Query（查询）</td>
<td>表示「我要找什么信息」 → 当前这个元素在寻找它最关心的内容</td>
</tr>
<tr>
<td>K</td>
<td>Key（键）</td>
<td>表示「我是谁，别人要不要关注我」 →
每个元素给自己贴了一个描述标签</td>
</tr>
<tr>
<td>V</td>
<td>Value（值）</td>
<td>表示「我真正携带的内容信息」 →
如果别人注意到我，要把这部分信息传递出去给对方</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="类比现实世界的例子">类比（现实世界的例子）</h3>
<p>假设你在读论文中一句话：</p>
<blockquote>
<p>“The robot must be able to generalize to new terrains.”</p>
</blockquote>
<p>你看到“generalize”这个词（Query）时，你大脑会：</p>
<ul>
<li>去看看别的词（Key），比如“robot”、“terrains”；</li>
<li>哪个词跟“generalize”最相关？（点积匹配度）；</li>
<li>最后从那个词的上下文（Value）中提取信息，帮助你理解“generalize”的含义。</li>
</ul>
<hr />
<h2 id="举个数值例子3个词向量简单化">🧪
举个数值例子：3个词向量（简单化）</h2>
<p>假设我们有 3 个词向量，每个是 2 维：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = [</span><br><span class="line">  [1, 0],   # 词A</span><br><span class="line">  [0, 1],   # 词B</span><br><span class="line">  [1, 1]    # 词C</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>我们通过 Linear
变换得到：（**这里线性映射（变换）的作用在于通过虬的视角进行学习。做了线性映射之后可以学习不同的关注角度）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W_Q = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]]   <span class="comment"># 单位矩阵，Q = X</span></span><br><span class="line">W_K = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]]   <span class="comment"># K = X @ W_K → 每个向量求和</span></span><br><span class="line">W_V = [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]]   <span class="comment"># 保持原始结构</span></span><br></pre></td></tr></table></figure>
<h3 id="step-1计算-qkv">➤ Step 1：计算 Q、K、V</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Q = X            = [[1, 0], [0, 1], [1, 1]]</span><br><span class="line">K = X @ W_K      = [[1, 1], [1, 1], [2, 2]]</span><br><span class="line">V = X            = [[1, 0], [0, 1], [1, 1]]</span><br></pre></td></tr></table></figure>
<hr />
<h3 id="step-2计算相似度点积-q-kᵗ">➤ Step 2：计算相似度（点积 Q ×
Kᵗ）</h3>
<p>[ _{ij} = d_k = 2, ) ]</p>
<p>以 ( q_1 = [1, 0] )，计算：</p>
<ul>
<li>与 ( k_1 = [1, 1] ) 的相似度：( 1×1 + 0×1 = 1 )</li>
<li>与 ( k_2 = [1, 1] )：也是 1</li>
<li>与 ( k_3 = [2, 2] )：( 1×2 + 0×2 = 2 )</li>
</ul>
<p>[ = ]</p>
<hr />
<h3 id="step-3softmax-得权重越相关权重越大">➤ Step 3：softmax
得权重（越相关权重越大）</h3>
<p>对 <code>[0.71, 0.71, 1.41]</code> 做 softmax：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">scores = torch.tensor([<span class="number">0.71</span>, <span class="number">0.71</span>, <span class="number">1.41</span>])</span><br><span class="line">weights = torch.softmax(scores, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(weights)  <span class="comment"># tensor([0.2584, 0.2584, 0.4832])</span></span><br></pre></td></tr></table></figure>
<p>→ 说明这个 Query（词A）最关注词C，其次是A、B。</p>
<hr />
<h3 id="step-4权重-v-得到输出">➤ Step 4：权重 × V → 得到输出</h3>
<p>最终输出为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output = 0.26 × V1 + 0.26 × V2 + 0.48 × V3</span><br><span class="line">       = 0.26 × [1, 0] + 0.26 × [0, 1] + 0.48 × [1, 1]</span><br><span class="line">       = [0.74, 0.74]</span><br></pre></td></tr></table></figure>
<p>这个就是“词A”的更新表示，它融合了“自己”和其它词的信息，重点借鉴了“词C”的内容。</p>
<hr />
<h2 id="mlp">MLP</h2>
<p>MLP（<strong>多层感知机</strong>，Multi-Layer
Perceptron）是一种最基础但又非常强大的<strong>前馈神经网络</strong>，是几乎所有深度学习模型的“基本构件”。</p>
<p>它本质上就是由多个<strong>线性变换（Linear）+
激活函数（非线性）</strong>层堆叠组成的网络。</p>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr>
<td>## MLP 原理</td>
</tr>
<tr>
<td>### 结构图： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入 x → [Linear → 激活] → ... → Linear → 输出 y</span><br></pre></td></tr></table></figure></td>
</tr>
<tr>
<td>每一层做的事是： [ h_i = (xW_i + b_i) ] 其中： - ( W_i, b_i
)：这一层的权重和偏置（可训练） - ( )：激活函数，如 ReLU, Tanh</td>
</tr>
</tbody>
</table>
<h2 id="举个手算的例子">举个手算的例子：</h2>
<h2 id="section"><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">输入 x = [2.0, 1.0, 0.5, -1.0]</span><br><span class="line">W1（4×8）随机初始化 → 得到隐藏层向量 h</span><br><span class="line">ReLU(h) → 截断负数</span><br><span class="line">W2（8×3）继续计算 → 得到 y（未归一化的分类分数）</span><br><span class="line"></span><br><span class="line">最终：用 softmax(y) 得到概率分布</span><br></pre></td></tr></table></figure></h2>
<h2 id="总结一句话-1">总结一句话：</h2>
<blockquote>
<p><strong>MLP 就是一串“线性 →
非线性”的堆叠结构，能拟合从输入到输出的复杂关系，适合各种
tabular、连续向量任务，是强化学习中最常见的网络结构。</strong></p>
</blockquote>
<h1
id="网络训练中的优化方法和正则化方法">网络训练中的优化方法和正则化方法</h1>
<p>网络训练想要做到
<strong>既收敛快、又泛化强、不过拟合</strong>，核心依赖两大类技术：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>目标</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>优化方法</strong></td>
<td>提高训练效率 &amp; 收敛速度</td>
</tr>
<tr>
<td><strong>正则化方法</strong></td>
<td>提高泛化能力 &amp; 防止过拟合</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="一优化方法让网络快准稳">一、优化方法（让网络“快准稳”）</h2>
<table>
<colgroup>
<col style="width: 24%" />
<col style="width: 76%" />
</colgroup>
<thead>
<tr>
<th>方法名</th>
<th>说明/原理</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SGD</strong></td>
<td>最基本优化器：小批量 + 梯度更新</td>
</tr>
<tr>
<td><strong>Momentum</strong></td>
<td>类似“物理动量” → 缓解震荡、加快方向收敛</td>
</tr>
<tr>
<td><strong>Adam</strong></td>
<td>学习率自适应：结合 RMSProp + Momentum，最常用优化器</td>
</tr>
<tr>
<td><strong>RMSProp</strong></td>
<td>适合非平稳目标，自动调整不同参数的学习率</td>
</tr>
<tr>
<td><strong>AdaGrad/AdaDelta</strong></td>
<td>调整学习率，适合稀疏特征</td>
</tr>
<tr>
<td><strong>Learning Rate Scheduler</strong></td>
<td>训练中动态调整学习率（如逐步降低、Warm-up 等）</td>
</tr>
<tr>
<td><strong>Gradient Clipping</strong></td>
<td>限制梯度最大值，防止爆炸（特别是 RNN）</td>
</tr>
<tr>
<td><strong>Weight Decay</strong></td>
<td>权重衰减 = L2 正则，兼顾正则化效果</td>
</tr>
<tr>
<td><strong>Mixed Precision</strong></td>
<td>使用 FP16 加速训练同时减少显存</td>
</tr>
</tbody>
</table>
<hr />
<h2
id="二正则化方法让网络不过拟合">二、正则化方法（让网络“不过拟合”）</h2>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr>
<th>方法名</th>
<th>原理 / 效果</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>L1/L2 正则化</strong></td>
<td>在损失函数中加入参数大小惩罚（L1 使权重稀疏，L2 平滑）</td>
</tr>
<tr>
<td><strong>Dropout</strong></td>
<td>训练时随机“丢弃”部分神经元（防止 co-adaptation）</td>
</tr>
<tr>
<td><strong>Early Stopping</strong></td>
<td>验证集性能不再提升就停止训练，防止训练过头</td>
</tr>
<tr>
<td><strong>数据增强（DA）</strong></td>
<td>图像/文本/状态等做扰动 → 提升数据多样性</td>
</tr>
<tr>
<td><strong>BatchNorm / LayerNorm</strong></td>
<td>归一化中间层 → 缓解梯度问题，提升稳定性</td>
</tr>
<tr>
<td><strong>Label Smoothing</strong></td>
<td>防止网络过于确信，常用于分类（输出目标不是one-hot，而是更平滑）</td>
</tr>
<tr>
<td><strong>随机噪声 / 缺失输入</strong></td>
<td>输入扰动 → 提升鲁棒性，如 Cutout、随机遮挡、状态 Dropout</td>
</tr>
<tr>
<td><strong>对抗训练</strong></td>
<td>添加小的扰动（FGSM等）对抗性训练，提高泛化</td>
</tr>
<tr>
<td><strong>正则化损失项（KL、MSE、对比损失）</strong></td>
<td>用额外 loss 限制网络行为（如蒸馏、模仿学习中）</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结一句话-2">总结一句话：</h2>
<blockquote>
<p><strong>优化方法</strong> 让模型训练得更快、更稳定；<br />
<strong>正则化方法</strong>
则是让模型不要只记住训练集，而要能泛化到新数据。</p>
</blockquote>
<hr />
<p>在<strong>深度强化学习（Deep Reinforcement Learning,
DRL）</strong>中，优化与正则化方法同样非常关键，它们被用于：</p>
<ul>
<li><strong>优化策略训练过程的效率和稳定性</strong>；</li>
<li><strong>提升策略的泛化能力、鲁棒性和安全性</strong>。</li>
</ul>
<hr />
<h2 id="一深度强化学习中的优化方法">一、深度强化学习中的优化方法</h2>
<table>
<colgroup>
<col style="width: 26%" />
<col style="width: 73%" />
</colgroup>
<thead>
<tr>
<th>方法名</th>
<th>应用 / 原因</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Adam / RMSProp</strong></td>
<td>最常用优化器，适应性强，适合非平稳、稀疏的 RL 梯度</td>
</tr>
<tr>
<td><strong>学习率调度器（Scheduler）</strong></td>
<td>动态调整学习率（如 warm-up, decay）提高收敛性能</td>
</tr>
<tr>
<td><strong>梯度裁剪（Gradient Clipping）</strong></td>
<td>防止梯度爆炸，尤其适用于 RNN 策略、Actor-Critic 结构</td>
</tr>
<tr>
<td><strong>奖励缩放（Reward Scaling）</strong></td>
<td>统一 reward 的分布，便于策略梯度更新（如 DDPG、SAC）</td>
</tr>
<tr>
<td><strong>归一化状态/输入</strong></td>
<td>增强训练稳定性，常用于连续控制任务</td>
</tr>
<tr>
<td><strong>并行采样（A3C / PPO）</strong></td>
<td>提升样本采样效率、增加训练多样性</td>
</tr>
<tr>
<td><strong>目标网络（Target Network）</strong></td>
<td>TD 学习中缓解不稳定性（用于 Q 学习：DQN, DDPG, TD3 等）</td>
</tr>
<tr>
<td><strong>熵奖励（Entropy Bonus）</strong></td>
<td>在策略更新中鼓励随机性，避免策略早期陷入局部最优（PPO、SAC 等）</td>
</tr>
<tr>
<td><strong>优势函数归一化（Advantage Norm）</strong></td>
<td>加速收敛，缓解梯度抖动（PPO）</td>
</tr>
</tbody>
</table>
<hr />
<h2
id="二深度强化学习中的正则化方法">二、深度强化学习中的正则化方法</h2>
<table>
<colgroup>
<col style="width: 28%" />
<col style="width: 72%" />
</colgroup>
<thead>
<tr>
<th>方法名</th>
<th>应用场景 / 原理</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Entropy Regularization</strong></td>
<td>在策略中鼓励随机性，防止过拟合、提高探索能力</td>
</tr>
<tr>
<td><strong>Dropout（在策略网络中）</strong></td>
<td>控制网络复杂度，提高策略泛化能力</td>
</tr>
<tr>
<td><strong>Early Stopping（验证环境）</strong></td>
<td>根据验证性能判断是否终止训练，尤其适用于 offline-RL</td>
</tr>
<tr>
<td><strong>L2 权重惩罚（Weight Decay）</strong></td>
<td>防止过拟合，约束策略网络参数</td>
</tr>
<tr>
<td><strong>对抗扰动训练（Adversarial RL）</strong></td>
<td>增加输入扰动，如状态扰动、环境扰动，提高策略鲁棒性</td>
</tr>
<tr>
<td><strong>输入扰动/遮蔽（State Dropout）</strong></td>
<td>模仿 Dropout，但针对状态维度，增强策略泛化</td>
</tr>
<tr>
<td><strong>模仿学习中的 KL 正则项</strong></td>
<td>例如 GAIL、RLPD，约束策略不偏离专家行为</td>
</tr>
<tr>
<td><strong>Value Clipping（PPO）</strong></td>
<td>避免 critic 网络估值不稳定或过大</td>
</tr>
<tr>
<td><strong>Policy Clipping（PPO）</strong></td>
<td>避免策略过快变化，保证收敛性和性能</td>
</tr>
<tr>
<td><strong>经验重用正则（Replay Filtering）</strong></td>
<td>选取更有代表性的经验进行训练，过滤无效 /
假轨迹（DDPG/TD3/Replay）</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="强化学习中优化-正则化的协同使用示例">强化学习中优化 &amp;
正则化的协同使用示例</h2>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 39%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr>
<th>算法</th>
<th>典型优化策略</th>
<th>典型正则化策略</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>DQN</strong></td>
<td>目标网络、经验回放、RMSProp</td>
<td>Reward Clip、Value Norm</td>
</tr>
<tr>
<td><strong>DDPG</strong></td>
<td>Target network、Adam</td>
<td>Noise injection、L2 正则</td>
</tr>
<tr>
<td><strong>PPO</strong></td>
<td>Advantage norm、Clip ratio</td>
<td>Policy clipping、Entropy 正则</td>
</tr>
<tr>
<td><strong>SAC</strong></td>
<td>自动调整熵目标、双 Q 网络</td>
<td>熵正则、reward scale、Q 均衡更新</td>
</tr>
<tr>
<td><strong>A3C</strong></td>
<td>多线程优化、shared 网络结构</td>
<td>Entropy、gradient clip</td>
</tr>
<tr>
<td><strong>GAIL</strong></td>
<td>分离优化器：策略 vs 判别器</td>
<td>KL divergence + imitation penalty</td>
</tr>
</tbody>
</table>
<table style="width:6%;">
<colgroup>
<col style="width: 5%" />
</colgroup>
<tbody>
<tr>
<td># 三种神经网络对比</td>
</tr>
<tr>
<td>特性 | 前馈神经网络 | CNN | RNN |——————|——|——|——| 输入结构 | 向量 |
网格/图像/多维矩阵 | 序列（时序数据） 参数共享 | ❌ | ✅（卷积核共享） |
✅（时间共享） 局部感知 | ❌ | ✅ | ✅（时间上） 捕捉顺序依赖 | ❌ | ❌
| ✅ 并行能力 | ✅ | ✅ | ❌（LSTM需逐步迭代） 应用场景 | 分类、预测 |
图像处理、目标识别 | NLP、语音、时间预测</td>
</tr>
</tbody>
</table>
<h2 id="非凸优化问题non-convex-optimization">1. 非凸优化问题（Non-Convex
Optimization）</h2>
<ul>
<li>优化目标不是一个“碗”形函数，而是“山谷+山峰+凹坑”组合；</li>
<li>损失函数存在多个局部最小值、鞍点，甚至“平坦区”；</li>
<li>最优化不再能用凸优化的“全局最优”性质。</li>
</ul>
<h3 id="在-mlp-中的原因">在 MLP 中的原因</h3>
<ul>
<li>网络结构是多层复合非线性函数（比如 ReLU、Sigmoid）；</li>
<li>每层的权重矩阵 ( W_i ) 和激活函数组合使得整个 loss function
非凸；</li>
<li>存在大量局部极小点 &amp; 鞍点（尤其在高维空间中更常见）。</li>
</ul>
<h3 id="应对方法">应对方法</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 66%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>原理说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>多次初始化（种子）</td>
<td>从多个点开始训练，选最好一个</td>
</tr>
<tr>
<td>使用优化器（如 Adam）</td>
<td>自适应步长跳出鞍点或平坦区</td>
</tr>
<tr>
<td>批归一化（BatchNorm）</td>
<td>缓解梯度分布不均，提高可训练性</td>
</tr>
<tr>
<td>残差结构（ResNet）</td>
<td>保持信息通路、减少极小点陷阱（用于更深网络）</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="梯度消失问题vanishing-gradient">2. 梯度消失问题（Vanishing
Gradient）</h2>
<h3 id="现象">现象</h3>
<ul>
<li>训练过程中，靠近输入层的权重梯度变得非常小；</li>
<li>导致前面层几乎不更新；</li>
<li>网络“记不住长距离依赖”，或者训练非常慢。</li>
</ul>
<h3 id="原因">原因</h3>
<ul>
<li>多层链式求导 ( = )</li>
<li>如果激活函数梯度 &lt; 1（如 Sigmoid 的最大梯度 0.25），多层乘起来 →
趋近 0</li>
<li>对于深层 MLP（&gt;5 层），非常严重！</li>
</ul>
<h3 id="解决方案">解决方案</h3>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 72%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>原理 / 效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>✅ 使用 ReLU 激活</td>
<td>导数恒为 1（或 0），不会收缩梯度</td>
</tr>
<tr>
<td>✅ 批归一化（BN）</td>
<td>标准化输入输出，稳定激活值分布</td>
</tr>
<tr>
<td>✅ 初始化技巧</td>
<td>Xavier、He 初始化 → 保持梯度和激活值方差一致</td>
</tr>
<tr>
<td>✅ 残差连接</td>
<td>在深网络中加入 skip-connection，保留梯度通路</td>
</tr>
<tr>
<td>✅ 使用浅层网络</td>
<td>对于简单任务，少用不必要的深层结构</td>
</tr>
</tbody>
</table>
<h3 id="梯度消失-vs-非凸优化的区别对比">梯度消失 vs
非凸优化的区别对比</h3>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 23%" />
<col style="width: 23%" />
<col style="width: 34%" />
</colgroup>
<thead>
<tr>
<th>问题类型</th>
<th>本质</th>
<th>出现位置</th>
<th>后果</th>
</tr>
</thead>
<tbody>
<tr>
<td>非凸优化</td>
<td>损失面是复杂地形</td>
<td>整体损失空间</td>
<td>易陷局部极小、鞍点</td>
</tr>
<tr>
<td>梯度消失</td>
<td>链式导数趋近于 0</td>
<td>深层靠前的层</td>
<td>权重更新很小，训练停滞</td>
</tr>
</tbody>
</table>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/23/%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/23/%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">四足机器人论文学习总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-23 09:18:51 / 修改时间：10:15:17" itemprop="dateCreated datePublished" datetime="2025-04-23T09:18:51+08:00">2025-04-23</time>
            </span>

          
            <span id="/2025/04/23/%E5%9B%9B%E8%B6%B3%E6%9C%BA%E5%99%A8%E4%BA%BA%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/" class="post-meta-item leancloud_visitors" data-flag-title="四足机器人论文学习总结" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="hilma-res-a-general-hierarchical-framework-via-residual-rl-for-combining-quadrupedal-locomotion-and-manipulation">HiLMa-Res:
A General Hierarchical Framework via Residual RL for Combining
Quadrupedal Locomotion and Manipulation</h1>
<h3 id="提出了一个可泛化的-loco-manipulation-分层强化学习框架">✅ 1.
<strong>提出了一个可泛化的 loco-manipulation
分层强化学习框架</strong></h3>
<h4 id="原理">📌 原理：</h4>
<ul>
<li>Loco-manipulation 是 locomotion（行走）+
manipulation（操作）的组合问题，控制空间极大，策略难以学习。</li>
<li>将整体任务<strong>分层</strong>：
<ul>
<li><strong>低层：Locomotion Controller</strong> →
学习行走并追踪任意足端轨迹；
<ul>
<li>用神经网络实现 PPO 策略
输入：包含机器人当前的运动状态、历史状态、外部扰动、目标轨迹（轨迹隐含在相位变量和周期中）；
输出：每只脚的期望接触力 / 足端位置（用于力控或位置控）； 训练方法：PPO
强化学习算法； 在 Isaac Gym 中高效并行训练； 加了 domain
randomization（随机地形、噪声）以增强泛化性；
特征：一旦训练完成，可以直接部署到真实机器人，无需再训练。</li>
</ul></li>
<li><strong>高层：Manipulation Planner</strong> →
规划残差轨迹，实现操纵任务。
<ul>
<li>用神经网络实现轨迹规划策略
输入：状态输入：机器人自身状态、目标信息（如目标位置、速度方向）；
或图像输入：深度图、摄像头画面（需加 CNN）； 输出：每条腿一条 3 阶
Bezier 曲线的控制点（一般是 3~4 个点）； CPG
的周期/相位调节参数（例如走路节奏）； 训练方法：
同样使用强化学习算法（PPO/DroQ）；
奖励函数任务相关，比如推球的方向、推箱位置、避障是否成功；
可接入现实数据再训练（RLPD），提升现实表现； 特点：通用框架 +
不同任务换不同 reward 即可复用。</li>
</ul></li>
<li>用低层网络学“怎么走”；用高层网络学“走向哪里、怎么推、怎么跨”。 ####
✅ 好处：</li>
</ul></li>
<li>解耦复杂任务 → 降低学习难度；</li>
<li>低层可重用多个任务，高层策略任务特定，<strong>实现模块化和迁移能力</strong>。</li>
</ul>
<h3 id="采用任务无关的运动控制器和任务相关的操作规划器">✅ 2.
<strong>采用任务无关的运动控制器和任务相关的操作规划器</strong></h3>
<h4 id="原理-1">📌 原理：</h4>
<ul>
<li>低层运动控制器（Locomotion
Controller）专注于<strong>足端轨迹跟踪</strong>，不关心上层任务内容。</li>
<li>高层操作规划器只需要设计<strong>足端轨迹（residual）</strong>，来完成具体任务目标。</li>
</ul>
<h4 id="实现方式">✅ 实现方式：</h4>
<ul>
<li>低层训练时，只用模拟中<strong>随机生成的轨迹</strong>训练它追踪；</li>
<li>高层策略负责设计这些轨迹的“形状”（比如球要往左推 →
左脚向外伸）；</li>
<li>二者通过轨迹接口（Bezier +
CPG）连接，高层给轨迹，低层负责执行。</li>
<li>实现方式：
<ul>
<li>低层训练时，只用模拟中随机生成的轨迹训练它追踪；</li>
<li>高层策略负责设计这些轨迹的“形状”（比如球要往左推 →
左脚向外伸）；</li>
<li>二者通过轨迹接口（Bezier +
CPG）连接，高层给轨迹，低层负责执行。</li>
</ul></li>
</ul>
<h3 id="利用了残差学习bezier-曲线-cpg中央模式发生器来灵活生成轨迹">✅ 3.
<strong>利用了残差学习（Bezier 曲线）+
CPG（中央模式发生器）来灵活生成轨迹</strong></h3>
<h4 id="原理-2">📌 原理：</h4>
<ul>
<li><strong>CPG</strong>（Central Pattern
Generator）：用于生成周期性足端轨迹，如走路、跳跃；</li>
<li><strong>Bezier
曲线</strong>：高层策略学习残差，用平滑的曲线微调轨迹，实现操作动作（如推、踢、跨越）。</li>
</ul>
<h4 id="实现方式-1">✅ 实现方式：</h4>
<ul>
<li>控制器最终执行的轨迹 = <strong>CPG生成的默认轨迹 +
Bezier残差轨迹</strong>；</li>
<li>Bezier 的控制点由高层网络输出，调节轨迹形状；</li>
<li>可用较少参数，<strong>生成复杂、多样的足端轨迹</strong>。</li>
</ul>
<h3 id="在多个现实任务上验证了方法的通用性与优越性">✅ 4.
<strong>在多个现实任务上验证了方法的通用性与优越性</strong></h3>
<h4 id="实验设计原理">📌 实验设计原理：</h4>
<ul>
<li>将高层策略迁移到多个任务上，如：
<ol type="1">
<li>带球（dribble）</li>
<li>踩点避障（stepostone）</li>
<li>推箱（navload）</li>
</ol></li>
<li>低层控制器不变，仅更换高层策略 →
验证了框架的<strong>模块化和通用性</strong>。</li>
</ul>
<h4 id="成果">✅ 成果：</h4>
<ul>
<li>所有任务均能<strong>从仿真零样本直接迁移到现实机器人</strong>，性能优于多个对比基线（Reward
shaping、Motion tracking、AMP 等）。</li>
</ul>
<h3 id="支持视觉状态多模态输入适配多种-rl-算法">✅ 5.
<strong>支持视觉、状态、多模态输入，适配多种 RL 算法</strong></h3>
<h4 id="原理-3">📌 原理：</h4>
<ul>
<li>高层策略输入是任务相关的信息，具有很强的扩展性；
<ul>
<li>可接受：
<ul>
<li>状态向量（如物体位置、机器人速度等）；</li>
<li>视觉输入（如深度图）；</li>
<li>多模态（状态 + 视觉）；</li>
</ul></li>
</ul></li>
<li>可适配不同类型的 RL 算法，如
PPO、DroQ、RLPD（现实数据驱动）等。</li>
</ul>
<h4 id="实现方式-2">✅ 实现方式：</h4>
<ul>
<li>高层策略结构灵活：可以是 MLP（状态输入）或
CNN+MLP（图像输入）；</li>
<li>训练框架可无缝切换不同策略优化器；</li>
<li>利用现实 fine-tuning（如
RLPD）<strong>提升现实成功率和样本效率</strong>。</li>
</ul>
<h2 id="小结它是如何做成的">🧩 小结：它是如何“做成”的？</h2>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 40%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr>
<th>功能点</th>
<th>背后原理</th>
<th>实现机制</th>
</tr>
</thead>
<tbody>
<tr>
<td>分层控制</td>
<td>解耦 loco &amp; manipulation</td>
<td>高层规划轨迹、低层执行轨迹</td>
</tr>
<tr>
<td>可复用性</td>
<td>低层不关心任务内容</td>
<td>通用轨迹跟踪器（足端控制）</td>
</tr>
<tr>
<td>灵活轨迹生成</td>
<td>曲线 + 振荡器组合</td>
<td>Bezier（残差） + CPG（基准周期轨迹）</td>
</tr>
<tr>
<td>多任务适应</td>
<td>高层策略切换</td>
<td>各任务独立设计 reward，高层重新训练</td>
</tr>
<tr>
<td>多模态输入 &amp; 多RL支持</td>
<td>高层接收任务信息，使用 RL 优化轨迹</td>
<td>支持 PPO、DroQ、RLPD，多模态策略输入结构灵活</td>
</tr>
</tbody>
</table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/22/%E5%AE%89%E5%85%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/22/%E5%AE%89%E5%85%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">安全强化学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-22 10:52:47 / 修改时间：18:07:17" itemprop="dateCreated datePublished" datetime="2025-04-22T10:52:47+08:00">2025-04-22</time>
            </span>

          
            <span id="/2025/04/22/%E5%AE%89%E5%85%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="安全强化学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1
id="王雪松-王荣荣-程玉虎.-安全强化学习综述.-自动化学报-2023-499-18131835">王雪松,
王荣荣, 程玉虎. 安全强化学习综述. 自动化学报, 2023, 49(9):
1813−1835</h1>
<figure>
<img src="image.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="image1.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<hr />
<h1
id="safe-reinforcement-learning-using-robust-control-barrier-functionsemam-et-al.-2022本质上是一种策略约束方法">《<strong>Safe
Reinforcement Learning Using Robust Control Barrier
Functions</strong>》（Emam et al.,
2022）（本质上是一种策略约束方法）</h1>
<p>提出了一种结合<strong>鲁棒控制屏障函数（RCBF）</strong>和强化学习的安全强化学习框架，并以<strong>Soft
Actor-Critic
(SAC)</strong>为基底，解决训练过程中可能进入不安全状态的问题。</p>
<h2 id="论文核心结构与思路">论文核心结构与思路</h2>
<h3 id="问题背景">1. 问题背景</h3>
<ul>
<li>强化学习需要大量探索，但这可能会引发<strong>安全问题</strong>（如机器人撞墙、无人机坠落）。</li>
<li>安全强化学习目标：在训练中就<strong>保持系统不进入不安全状态</strong>，而不是仅最终收敛到安全策略。</li>
</ul>
<h3 id="方法结构sac-rcbf">2. 方法结构（SAC-RCBF）</h3>
<h4 id="系统建模">📎 系统建模：</h4>
<p>考虑扰动控制仿射系统：</p>
<p>[ (t) = f(x(t)) + g(x(t))u(x(t)) + d(x(t)) ]</p>
<ul>
<li>(d(x))：未知扰动，用<strong>高斯过程（GP）</strong>建模。</li>
<li>控制目标是：用 RL 找最优策略 ( )，在执行前通过 <strong>RCBF
层</strong>进行修正，确保安全。</li>
</ul>
<h4 id="安全模块rcbf-qp-层">📎 安全模块：RCBF-QP 层</h4>
<ul>
<li>用<strong>鲁棒控制屏障函数（RCBF）</strong>构造约束： [ h(x)^T(f(x)
+ g(x)u(x)) -(h(x)) - h(x)^T d(x) ]</li>
<li>通过<strong>二次规划（QP）</strong>调整策略输出，最小化修改幅度，确保满足约束。</li>
</ul>
<h4 id="强化学习部分soft-actor-critic-sac">📎 强化学习部分：Soft
Actor-Critic (SAC)</h4>
<ul>
<li>引入<strong>可微分安全层</strong>，可以反向传播梯度，增强策略学习能力。</li>
<li>提出两个优化：
<ol type="1">
<li><strong>Differentiable RCBF
Layer</strong>：允许梯度传播：使得策略能够进行自动修正。可微分安全层让“安全修正”变成了策略可以感知和优化的目标，因此是实实在在的“加速
+ 提升性能”的方法。</li>
<li><strong>Model-based
rollout</strong>：用学习到的模型生成合成数据，加速学习</li>
</ol></li>
<li><strong>没有可微分安全层：</strong></li>
</ul>
<table>
<thead>
<tr>
<th>问题</th>
<th>影响</th>
</tr>
</thead>
<tbody>
<tr>
<td>策略网络不知道“自己动作被改了”</td>
<td>无法学习“避开”那些不安全动作</td>
</tr>
<tr>
<td>CBF 修正是黑箱、不可导</td>
<td>策略只能“撞墙后被拉回来”，而不是“提前不撞”</td>
</tr>
<tr>
<td>SAC 更新方向可能乱了</td>
<td>有效梯度方向被掩盖，学习效率低</td>
</tr>
<tr>
<td>高维控制系统尤其严重</td>
<td>RL 很难靠试错探索到“刚刚好不触碰屏障”的策略</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>有安全层：</strong></li>
</ul>
<table>
<thead>
<tr>
<th>优点</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>策略能“感知到”安全边界的位置</td>
<td>不再只是靠碰撞惩罚、随机试错</td>
</tr>
<tr>
<td>CBF 修正能“教会”网络如何避开风险</td>
<td>把 CBF 的结构性知识反传给策略</td>
</tr>
<tr>
<td>策略更快收敛、学到更优行为</td>
<td>不容易陷入安全/性能折中困境</td>
</tr>
</tbody>
</table>
<h4 id="模块化任务学习modular-learning">📎 模块化任务学习（Modular
Learning）</h4>
<ul>
<li><strong>任务 reward 与安全 constraint
解耦</strong>，使得训练出的策略具有更强的<strong>迁移能力（zero-shot
transfer）</strong>。</li>
</ul>
<h2 id="创新点总结">创新点总结</h2>
<table>
<thead>
<tr>
<th>创新点类别</th>
<th>内容说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>✅ 融合方法创新</td>
<td>将 RCBF 融入 SAC 中，确保训练过程安全</td>
</tr>
<tr>
<td>✅ 可微分安全层</td>
<td>提出 differentiable RCBF-QP，允许梯度传播</td>
</tr>
<tr>
<td>✅ 模块化训练思想</td>
<td>reward 与 safety 解耦，提升策略迁移性</td>
</tr>
<tr>
<td>✅ 数据效率优化</td>
<td>使用部分已学模型做合成 rollout，提升样本效率</td>
</tr>
</tbody>
</table>
<h2 id="可改进方向-研究机会">可改进方向 / 研究机会</h2>
<ol type="1">
<li><strong>更多维度扰动建模方法</strong>：GP 对高维 d(x)
效率有限，可考虑使用贝叶斯神经网络或 ensemble 模型。</li>
<li><strong>更通用的 CBF 设计工具</strong>：当前构造 CBF
需先验经验，可引入自动构造或学习 CBF 的方法。</li>
<li><strong>现实部署实验验证缺失</strong>：目前只在模拟环境中测试，若能在真实机器人上测试更具说服力。</li>
<li><strong>与 MPC 的融合探索</strong>：框架可拓展为 hybrid
RL-MPC，进一步提升实时安全性。</li>
</ol>
<hr />
<h2 id="sac算法">SAC算法：</h2>
<blockquote>
<p><strong>SAC 是一种最大化奖励 + 最大化动作多样性（熵）的 off-policy
深度强化学习算法，具有高效、稳定、探索性强的特点。</strong></p>
</blockquote>
<h3 id="一sac-的基本思想">一、SAC 的基本思想</h3>
<p>传统强化学习只最大化期望回报：</p>
<p>[ <em></em>{} ]</p>
<blockquote>
<p>不仅希望获得高奖励，还希望策略<strong>有足够的“随机性”</strong>，保持探索能力。</p>
</blockquote>
<p>引入<strong>最大熵强化学习目标</strong>：</p>
<p>[ <em></em>{} ]</p>
<ul>
<li>( () = -(a|s) ) 是策略的<strong>熵</strong></li>
<li>( )：探索与奖励之间的平衡因子</li>
</ul>
<h3 id="二sac-的结构组成">🔍 二、SAC 的结构组成</h3>
<p>SAC 有四个主要模块（四个网络）：</p>
<table>
<colgroup>
<col style="width: 72%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th>模块</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Actor</strong>（策略网络）</td>
<td>输出当前状态下的概率分布 ( (a</td>
</tr>
<tr>
<td><strong>Q1, Q2 网络</strong>（Critic）</td>
<td>近似动作值函数 ( Q(s, a) )，使用双 Q 避免过高估计</td>
</tr>
<tr>
<td><strong>Value 网络（可选）</strong></td>
<td>估计当前状态下的“soft value” ( V(s) )（新版可省略）</td>
</tr>
</tbody>
</table>
<blockquote>
<p>训练使用目标网络（target Q）、经验池（replay buffer）等稳定手段。</p>
</blockquote>
<h3 id="四sac-的优点">四、SAC 的优点</h3>
<table>
<thead>
<tr>
<th>特点</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>✅ off-policy</td>
<td>能重复使用旧数据，训练效率高</td>
</tr>
<tr>
<td>✅ 连续动作空间</td>
<td>可用于机器人控制等场景</td>
</tr>
<tr>
<td>✅ 探索性强</td>
<td>保留策略熵，鼓励更多尝试</td>
</tr>
<tr>
<td>✅ 收敛稳定</td>
<td>双 Q + target network 提升稳定性</td>
</tr>
<tr>
<td>✅ 可扩展</td>
<td>可结合可微分安全层（如前文 RCBF）</td>
</tr>
</tbody>
</table>
<h2 id="关于-在线-离线强化学习-与-on-policy-off-policy">关于 “在线 /
离线”强化学习 与 “on-policy / off-policy”</h2>
<table>
<colgroup>
<col style="width: 52%" />
<col style="width: 23%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr>
<th>比较点</th>
<th>在线 / 离线</th>
<th>On-policy / Off-policy</th>
</tr>
</thead>
<tbody>
<tr>
<td>关注的是？</td>
<td>数据是否来自当前环境？</td>
<td>样本是否由当前策略生成？</td>
</tr>
<tr>
<td>数据是否更新？</td>
<td>在线更新 / 离线固定</td>
<td>都可以实时或离线训练</td>
</tr>
<tr>
<td>能否与环境交互？</td>
<td>离线不能，在线可以</td>
<td>无限制，on/off-policy 都可在线训练</td>
</tr>
<tr>
<td>是否用 replay buffer？</td>
<td>不一定（视方法而定）</td>
<td>Off-policy 用，On-policy 通常不用</td>
</tr>
<tr>
<td>安全应用偏好？</td>
<td>离线更安全</td>
<td>与安全性无直接关系</td>
</tr>
</tbody>
</table>
<hr />
<h1
id="learning-safety-in-model-based-reinforcement-learning-using-mpc-and-gaussian-processes"><strong>《Learning
Safety in Model-Based Reinforcement Learning using MPC and Gaussian
Processes》</strong></h1>
<blockquote>
<p>提出了一种结合 <strong>高斯过程（GP）与模型预测控制（MPC）</strong>
的 <strong>安全强化学习（Safe
RL）方法</strong>。它的研究目标是<strong>在不牺牲性能的前提下，提高策略的安全性</strong>。</p>
</blockquote>
<p>我将从三个层面来解析：</p>
<h2 id="一文章的核心创新点分析">一、文章的核心创新点分析</h2>
<h3 id="使用-gp-回归学习-mpc-参数空间中的安全集"><strong>1. 使用 GP
回归学习 MPC 参数空间中的安全集</strong></h3>
<ul>
<li>提出了一个 <strong>用高斯过程回归建模安全约束</strong>
的方法，不直接建模系统动力学，而是建模“哪个 MPC 参数组合是安全的”。</li>
<li>安全性不再是手工设定的硬约束，而是通过历史数据学习的<strong>概率安全区域</strong>
SD_D。</li>
</ul>
<p>👉 <strong>创新点</strong>：这是将黑箱约束建模方法用于 MPC-RL
中安全约束建模的首个系统性应用。</p>
<h3 id="将-mpc-作为-rl-的函数近似器"><strong>2. 将 MPC 作为 RL
的函数近似器</strong></h3>
<ul>
<li>与传统 RL 使用神经网络不同，该文使用 <strong>参数化的 MPC
控制器作为策略表示器</strong>。</li>
<li>学习的不是控制策略本身，而是 MPC
的参数（如模型参数、约束backoff等）。</li>
</ul>
<p>👉 <strong>创新点</strong>：相比 DNN 更可解释，且 MPC
本身具有对约束的原生支持。</p>
<h3 id="提出基于-gp-的安全约束嵌入到策略更新优化问题中"><strong>3.
提出基于 GP 的安全约束嵌入到策略更新优化问题中</strong></h3>
<ul>
<li>在 RL 参数更新时，加入 GP 学习的安全约束 z(θ)≤0z()
，确保新参数落在安全区域。</li>
<li>若更新不可行，则逐步 <strong>backtrack 安全概率
β</strong>，提升可行性。</li>
</ul>
<p>👉 <strong>创新点</strong>：结合了 <strong>概率安全约束 + 二阶
Q-learning + backtracking β</strong>，实现数据驱动的安全探索。</p>
<h2 id="总结">总结</h2>
<table>
<thead>
<tr>
<th>类别</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>核心创新</td>
<td>基于 GP 的安全参数集建模，嵌入 MPC-RL 训练过程</td>
</tr>
<tr>
<td>关键技术点</td>
<td>安全约束函数建模、LSTD Q-learning、贝叶斯优化启发式</td>
</tr>
<tr>
<td>可改进之处</td>
<td>GP 初期性能差、高维扩展难、参数空间学习有限</td>
</tr>
<tr>
<td>潜在创新方向</td>
<td>多模型融合、迁移学习、安全 BO 比对、真实机器人部署</td>
</tr>
</tbody>
</table>
<hr />
<h1
id="constrained-variational-policy-optimization-for-safe-reinforcement-learning">《Constrained
Variational Policy Optimization for Safe Reinforcement Learning》</h1>
<h2 id="一研究背景与问题定义">一、研究背景与问题定义</h2>
<p>1.1 安全强化学习的挑战 -
核心问题：在安全关键场景（如机器人控制）中，策略需在最大化奖励的同时满足安全约束（如碰撞避免）。
- 传统方法的局限：<br />
-
原始-对偶方法（Primal-Dual）：交替优化策略参数和对偶变量（拉格朗日乘子），存在数值不稳定性和缺乏最优性保证。
(<strong>核心思想：把约束优化问题转换为拉格朗日形式，交替优化策略参数（θ）和对偶变量（λ），通过迭代更新使策略参数和对偶变量同时收敛。</strong>)
-
泰勒近似方法：通过低阶近似简化约束优化问题，但可能导致约束违反率较高，近似不够精确，可能“形式上满足约束”，但实际策略存在约束违反。
（<strong>核心思想：对约束函数进行一阶或二阶泰勒展开，简化约束优化的计算</strong>）
-
样本效率低：现有方法多为在线策略（on-policy），难以利用历史数据进行高效学习。</p>
<p>1.2 约束马尔可夫决策过程（CMDP） -
定义：在标准MDP基础上引入约束成本函数集合 (C)，目标为：<br />
[ ^* = _{} J_r() J_c() _1 ] 其中 (J_r()) 和 (J_c())
分别为奖励和成本的累积折扣期望（比如摔倒、电量消耗、与人碰撞等）。</p>
<h2 id="二方法创新cvpo算法">二、方法创新：CVPO算法</h2>
<p>2.1 概率推断视角的重新建模 -
关键思想：将安全RL问题转化为概率推断问题，通过变分推断和EM算法自然融入约束。
- 概率图模型：引入最优性变量 (O)，表示轨迹 ()
的“成功事件”，其似然函数与奖励相关：<br />
[ p(O=1|) (<em>t ^t r_t / ) ] - 证据下界（ELBO）：通过变分分布 (q())
最大化下界，同时约束 (q) 属于安全分布族 (</em>{Q}^{_1})。</p>
<p>2.2 算法框架：EM两步优化 E步：凸优化求解变分分布 -
优化目标：在约束下最大化奖励期望，同时限制与旧策略的KL散度：<br />
[ <em>q </em>{<em>q} ] </em>{<em>q} [_q [Q_c(s,a)]] <em>1, D</em>{KL}(q
| </em>{_i}) _2 ] - 闭式解与强对偶性：<br />
- 通过拉格朗日对偶方法解析求解，得到非参数变分分布的闭式表达式：<br />
[ q^*(a|s) _{_i}(a|s) ( ) ] -
证明优化问题的严格凸性，确保解的唯一性和最优性。</p>
<p>M步：监督学习更新策略 -
目标函数：在信任域内最小化与最优变分分布的KL散度：<br />
[ <em>{} </em>{_q} ] - 实现方式：通过监督学习（如神经网络）拟合
(q^*)，支持离线策略（off-policy）数据复用。</p>
<h2 id="三理论贡献与优势">三、理论贡献与优势</h2>
<p>3.1 理论保证 1.
最优性与可行性：E步的凸优化确保变分分布同时满足奖励最大化和约束条件。 2.
信任域机制：M步的KL约束提供策略更新的鲁棒性保证，避免过度偏离安全区域。
3.
样本效率：离线策略训练减少与环境交互的成本，实验显示样本效率提升高达1000倍。</p>
<p>3.2 对现有方法的改进 -
稳定性：分解为凸优化和监督学习，避免原始-对偶方法的梯度冲突。 -
约束满足性：直接优化安全约束，而非通过惩罚项近似。 -
灵活性：非参数变分分布避免参数化策略的近似误差。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/22/test-math/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/22/test-math/" class="post-title-link" itemprop="url">测试公式页面</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-22 00:00:00 / 修改时间：22:39:45" itemprop="dateCreated datePublished" datetime="2025-04-22T00:00:00+08:00">2025-04-22</time>
            </span>

          
            <span id="/2025/04/22/test-math/" class="post-meta-item leancloud_visitors" data-flag-title="测试公式页面" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>行内公式：$a^2 + b^2 = c^2$</p>
<p>块级公式：</p>
<p>[<br>\dot{x}(t) = f(x(t)) + g(x(t))u(x(t)) + d(x(t))<br>]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/" class="post-title-link" itemprop="url">关于博士</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-20 23:06:29 / 修改时间：23:08:55" itemprop="dateCreated datePublished" datetime="2025-04-20T23:06:29+08:00">2025-04-20</time>
            </span>

          
            <span id="/2025/04/20/%E5%85%B3%E4%BA%8E%E5%8D%9A%E5%A3%AB/" class="post-meta-item leancloud_visitors" data-flag-title="关于博士" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这个问题非常重要！博士≠多读几年书，博士更像是<strong>一个“高强度训练营”，塑造你成为能独立提出、解决、验证问题的人。</strong></p>
<p>我们可以把博士期间应当掌握的能力分为以下五大核心模块：<br />
<strong>认知能力、科研能力、工程/技术能力、表达与协作能力、元能力（心智层）</strong></p>
<hr />
<h2 id="一认知能力这是博士的思维底盘">🧠
一、认知能力：这是博士的“思维底盘”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>抽象能力</strong></td>
<td>能从复杂现象中提取本质，比如将实验现象抽象为数学模型或控制系统</td>
</tr>
<tr>
<td><strong>结构化思维</strong></td>
<td>能将模糊任务拆解为清晰模块，有清晰的推理链</td>
</tr>
<tr>
<td><strong>批判性思维</strong></td>
<td>不盲信权威，敢于质疑前人方法，善于找出假设漏洞</td>
</tr>
<tr>
<td><strong>判断研究价值的能力</strong></td>
<td>能识别“哪些问题值得做”、“是否有学术贡献”</td>
</tr>
</tbody>
</table>
<p>📌
<em>一个好的博士不是做得多，而是<strong>思考得深</strong>。</em></p>
<hr />
<h2 id="二科研能力这是博士的核心业务">📚
二、科研能力：这是博士的“核心业务”</h2>
<table>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>选题能力</strong></td>
<td>能发现有意义、可行、有创新的问题</td>
</tr>
<tr>
<td><strong>文献调研能力</strong></td>
<td>能高效阅读、比较、归纳相关研究工作</td>
</tr>
<tr>
<td><strong>设计实验能力</strong></td>
<td>能构造有效的验证方式（仿真 / 实验 / 数据）</td>
</tr>
<tr>
<td><strong>数学建模/算法推导能力</strong></td>
<td>能写出合理的公式、证明关键结论</td>
</tr>
<tr>
<td><strong>论文写作能力</strong></td>
<td>能清楚表达问题、方法、实验、结论，让评审信服</td>
</tr>
</tbody>
</table>
<p>📌 <em>“做研究”不仅是做，更是不断地“想、写、讲、验证”。</em></p>
<hr />
<h2 id="三技术工程能力这是博士的落地抓手">🛠️
三、技术/工程能力：这是博士的“落地抓手”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>代码实现能力</strong></td>
<td>不仅写得对，还能写得稳、写得快、写得清晰</td>
</tr>
<tr>
<td><strong>系统调试能力</strong></td>
<td>能快速定位问题并修复，软硬件都能应对</td>
</tr>
<tr>
<td><strong>工具链掌握</strong></td>
<td>熟练掌握领域主流工具，如 PyTorch、ROS、Isaac Gym、LaTeX、Git 等</td>
</tr>
<tr>
<td><strong>大模型 / 控制 / 优化 / 数据等专项技能</strong></td>
<td>针对研究方向具备较强技术深度</td>
</tr>
</tbody>
</table>
<p>📌 <em>再“学术”的工作，最后也要通过工程实现才能验证。</em></p>
<hr />
<h2 id="四表达与协作能力这是博士的输出通道">🗣️
四、表达与协作能力：这是博士的“输出通道”</h2>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>汇报和讲解能力</strong></td>
<td>能讲清楚自己的研究问题、路线、结果，不绕圈子</td>
</tr>
<tr>
<td><strong>跨领域沟通能力</strong></td>
<td>和工程/算法/导师/评审有效协作，降低“学术闭塞”风险</td>
</tr>
<tr>
<td><strong>写 Email / 项目申请 / Reviewer 回复</strong></td>
<td>格式专业，语气得体，能解决实际问题</td>
</tr>
<tr>
<td><strong>合作者管理与合作谈判</strong></td>
<td>特别是博后阶段，需要带人、带项目、谈合作</td>
</tr>
</tbody>
</table>
<p>📌 <em>科研不是一个人的战斗，表达力决定你能否“被听见”。</em></p>
<hr />
<h2 id="五元能力心智层博士能否坚持下来的隐形力">🧭
五、元能力（心智层）：博士能否坚持下来的“隐形力”</h2>
<table>
<thead>
<tr>
<th>能力</th>
<th>具体表现</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>自我驱动能力</strong></td>
<td>没人 push 你也能主动推进研究</td>
</tr>
<tr>
<td><strong>时间管理能力</strong></td>
<td>同时处理多个项目、任务，合理安排周/月/年计划</td>
</tr>
<tr>
<td><strong>挫折恢复力</strong></td>
<td>面对实验失败、论文被拒、导师施压也不崩溃</td>
</tr>
<tr>
<td><strong>自我反思与成长循环</strong></td>
<td>定期复盘自己：哪里做得好？哪里需要调整？</td>
</tr>
</tbody>
</table>
<p>📌 <em>博士拼到最后，不是智商，是系统性和韧性。</em></p>
<hr />
<h2 id="最终你应该变成一个怎样的人">✨ 最终你应该变成一个怎样的人？</h2>
<blockquote>
<p>🎓 “我可以独立从 0 到 1
地完成一个有意义的问题研究，具备将复杂问题结构化、技术化、学术化地解决并表达出来的能力。”<br />
这才是真正的博士能力画像。</p>
</blockquote>
<hr />
<h2 id="补充读完博士后就业发展方向会非常依赖这些能力">💼
补充：读完博士后，就业/发展方向会非常依赖这些能力</h2>
<table>
<thead>
<tr>
<th>职业方向</th>
<th>对博士能力的需求侧重</th>
</tr>
</thead>
<tbody>
<tr>
<td>高校教职</td>
<td>科研能力 + 表达能力 + 项目组织能力</td>
</tr>
<tr>
<td>科研所（中科院等）</td>
<td>技术深度 + 系统能力</td>
</tr>
<tr>
<td>AI/机器人大厂</td>
<td>算法落地 + 工程能力 + 团队协作</td>
</tr>
<tr>
<td>创业</td>
<td>问题定义能力 + 心智强度 + 综合运营</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="小结博士五维能力图谱">✅ 小结：博士五维能力图谱</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">             元能力（自驱力、时间管理、情绪管理）</span><br><span class="line">                        ↑</span><br><span class="line">表达协作 ← 认知能力（思维框架） → 技术工程</span><br><span class="line">                        ↓</span><br><span class="line">                科研能力（核心任务）</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/" class="post-title-link" itemprop="url">关于目标设定的内部化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-20 22:25:58 / 修改时间：23:03:15" itemprop="dateCreated datePublished" datetime="2025-04-20T22:25:58+08:00">2025-04-20</time>
            </span>

          
            <span id="/2025/04/20/%E5%85%B3%E4%BA%8E%E7%9B%AE%E6%A0%87%E8%AE%BE%E5%AE%9A%E7%9A%84%E5%86%85%E9%83%A8%E5%8C%96/" class="post-meta-item leancloud_visitors" data-flag-title="关于目标设定的内部化" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>为什么衡量标准<strong>不能太外部化</strong>？我们可以从
<strong>心理机制 + 动力机制 + 行为后果</strong> 三个层面来剖析。</p>
<hr />
<h2 id="心理层面外部目标易引发焦虑迷失自我否定">🧠 1.
心理层面：<strong>外部目标易引发焦虑、迷失、自我否定</strong></h2>
<h3
id="外部化目标的本质是我好不好别人说了算">外部化目标的本质是“我好不好，别人说了算”：</h3>
<ul>
<li>比如“年薪百万”是由公司评估你值不值；</li>
<li>“拿称号”是评审专家说了算；</li>
<li>“某大厂 offer”是 HR 和面试官决定。</li>
</ul>
<p>这意味着：<br />
你把价值感
<strong>交给了外部</strong>，一旦外界反馈不如预期，就会陷入：</p>
<ul>
<li>📉 <strong>自我否定</strong>（我是不是不够好）</li>
<li>🤯 <strong>焦虑攀比</strong>（别人有了我还没有）</li>
<li>🔁 <strong>情绪循环</strong>（得不到就痛苦，得到了也很空）</li>
</ul>
<p><strong>外部化的衡量标准，往往不指向你真正的成长，而是社会期望的“表演成绩单”。</strong></p>
<hr />
<h2 id="动力机制层面外部目标难以形成可持续的内驱力">🚀 2.
动力机制层面：<strong>外部目标难以形成可持续的内驱力</strong></h2>
<h3 id="外部目标的典型陷阱">❌ 外部目标的典型陷阱：</h3>
<ul>
<li>得到之后很快失去动力（比如升职后陷入空虚）</li>
<li>没得到就陷入无意义感（比如努力失败时无法承受）</li>
<li>做事的动力变成“逃避失败”而非“追求成长”</li>
</ul>
<h3 id="内部目标的底层驱动更稳定">✅ 内部目标的底层驱动更稳定：</h3>
<p>比如：</p>
<blockquote>
<p>“我想掌握控制机器人上下楼梯的能力”<br />
“我想做一件对这个领域有贡献的工作”<br />
“我希望自己成为一个更有独立判断力的人”</p>
</blockquote>
<p>这些目标和<strong>自我认知、自我实现</strong>绑定，就算过程艰难，也更容易走得下去，因为你不是为别人“演”，你是为自己“活”。</p>
<hr />
<h2 id="行为后果层面外部目标容易导致手段扭曲路径偏离">🧩 3.
行为后果层面：<strong>外部目标容易导致手段扭曲、路径偏离</strong></h2>
<p>比如： - 为了“年薪百万”去做高压但无成长的岗位 -
为了“某称号”把项目写得浮夸不实 - 为了“大厂
offer”内卷刷题、却忽视基础能力建设</p>
<p>这会让你陷入： - 短视行为（重结果轻过程） - 虚假反馈（只追 KPI） -
自我价值错位（觉得自己“不配被喜欢”只因没达成某目标）</p>
<hr />
<h2 id="所以正确目标应该这样设定">✅ 所以，正确目标应该这样设定：</h2>
<table>
<colgroup>
<col style="width: 36%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr>
<th>外部化目标（错误）</th>
<th>内部化目标（推荐）</th>
</tr>
</thead>
<tbody>
<tr>
<td>“我要进某公司”</td>
<td>“我要成为能胜任该类岗位的工程师”</td>
</tr>
<tr>
<td>“我要发 A 类顶刊”</td>
<td>“我要建立起独立科研选题与推进能力”</td>
</tr>
<tr>
<td>“我要月入 3 万”</td>
<td>“我要能创造价值并掌控自由的生活方式”</td>
</tr>
<tr>
<td>“我要被认可/表扬”</td>
<td>“我要能稳定地输出高质量工作/内容”</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结金句">🎯 总结金句：</h2>
<blockquote>
<p><strong>外部目标只能当做“里程碑”，真正推动你走远的，是对内在成长的渴望。</strong></p>
</blockquote>
<ul>
<li>外部化目标是<strong>可衡量的结果</strong></li>
<li>内部化目标是<strong>可积累的能力与状态</strong></li>
<li>真正“牛的人”，是<strong>结果有了，心态还是过程型的</strong></li>
</ul>
<hr />
<p>我们都知道“外部目标不靠谱”“要内驱”，但现实中——<br />
<strong>绝大多数人都在盯着钱、名、offer、称号、头衔……</strong></p>
<p>那为什么会这样？<br />
答案是：<strong>这很“合理”，但不“健康”</strong>。</p>
<p>下面我帮你分析背后的<strong>五个原因</strong>：</p>
<hr />
<h2 id="社会系统鼓励外部目标它更容易衡量管理和比较">❶
社会系统鼓励外部目标：<strong>它更容易衡量、管理和比较</strong></h2>
<ul>
<li><strong>考试分数、绩点、奖学金、职称、年薪、Offer
数</strong>，这些都具备一个特点： 👉
<em>可以量化</em>，<em>可以排名</em>，<em>可以一眼看出你“值多少钱”</em></li>
</ul>
<p>这对制度来说很方便： - 公司招人凭它 - 父母比较孩子凭它 -
朋友圈判断你“是不是牛”也凭它</p>
<blockquote>
<p>所以你不“看它”你就会“掉队”，你不想卷，也会被卷。</p>
</blockquote>
<hr />
<h2 id="大脑的即时奖励系统被外部刺激绑架了">❷
大脑的“即时奖励系统”被外部刺激绑架了</h2>
<ul>
<li>收到
offer、加薪、被点赞、上热搜、拿奖——这些会<strong>立刻让你“爽”</strong></li>
<li>而内在成长，比如：
<ul>
<li>掌握一个复杂知识</li>
<li>读懂一本好书</li>
<li>理解一个人的心意 这些回报
<strong>慢且不确定</strong>，你的大脑要“延迟满足”</li>
</ul></li>
</ul>
<h3 id="所以说">所以说：</h3>
<blockquote>
<p><strong>外部目标像快餐，随手可得，但不养人；<br />
内在成长像慢炖汤，要等，但滋养深沉。</strong></p>
</blockquote>
<hr />
<h2 id="成长型目标需要深度的自我理解大多数人没机会培养">❸
成长型目标需要“深度的自我理解”——大多数人没机会培养</h2>
<ul>
<li>想知道“我想解决什么问题”“我想成为什么样的人”其实很难</li>
<li>教育体系和社会氛围几乎不给你空间去探索这些问题</li>
<li>所以人们只能转而去追那些<strong>别人都在追、社会说“好”的目标</strong></li>
</ul>
<p>这其实是<strong>认知发展阶段的问题</strong>，并不是谁“肤浅”或“功利”，而是：
&gt; <strong>没人教他们“怎么设定真正属于自己的目标”</strong></p>
<hr />
<h2 id="外部目标更容易用来证明自己满足安全感">❹
外部目标更容易用来“证明自己”，满足安全感</h2>
<p>很多人不是为了“钱本身”，<br />
而是为了“别人觉得我不错”、“爸妈看我有出息”、“不被看不起”……</p>
<p>这其实是： &gt; 把外部目标当成“自我价值的防御工具”<br />
&gt; 外界的评价越不确定，越要“用东西堵上”</p>
<hr />
<h2 id="外部目标确实能在短期内让人冲刺前进">❺
“外部目标”确实能在短期内让人冲刺前进</h2>
<p>别误会—— 外部目标并不是“完全不好”，它有以下好处： -
<strong>可见性强、行动明确、反馈快</strong> -
<strong>可以激励你迈出第一步</strong> -
<strong>是内在目标“实现”的副产品</strong></p>
<p>但它不能成为 <strong>最终驱动力</strong>，否则就会迷失。</p>
<hr />
<h2 id="所以很多人一开始是这样">🔁 所以很多人一开始是这样：</h2>
<ul>
<li>“我想年薪百万” → 所以拼命进大厂</li>
<li>进了之后发现也很苦 → “那我要升职/出国”</li>
<li>升职了也空虚 → “那我得做一个产品经理/leader/教授？”</li>
<li>再往上就开始怀疑人生：“我到底想干嘛？”</li>
</ul>
<p>这就是<strong>错把“手段”当“终点”</strong>的代价。</p>
<hr />
<h2 id="怎样跳出这个圈">🌱 怎样跳出这个圈？</h2>
<blockquote>
<p><strong>你得用外部目标“养活”自己，同时靠内在目标“成就”自己。</strong></p>
</blockquote>
<table>
<thead>
<tr>
<th>层级</th>
<th>外部目标</th>
<th>内在目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>入门</td>
<td>拿个 Offer</td>
<td>了解我适合什么样的工作</td>
</tr>
<tr>
<td>中期</td>
<td>涨工资</td>
<td>提升解决问题/带团队的能力</td>
</tr>
<tr>
<td>长期</td>
<td>被认可</td>
<td>成为一个有独立价值的人</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="最后送你一句话">💬 最后送你一句话：</h2>
<blockquote>
<p>“你可以用别人的标准活着一阵子，<br />
但你必须学会用自己的标准活一辈子。”</p>
</blockquote>
<hr />
<blockquote>
<p><strong>生活中的很多事情确实需要靠外部目标才能完成。</strong></p>
</blockquote>
<p>比如： - 要拿 offer、要毕业、要升职、要还房贷、要给父母一个交代… -
你不可能完全“躺在价值观里”，不管现实 KPI</p>
<hr />
<p>那是不是我们之前讲的“不要太外部化”就错了？</p>
<p>❌ <strong>不是错，而是要加一句完整的话</strong>：</p>
<blockquote>
<p><strong>外部目标可以驱动行动，<br />
但必须被一个内在系统来“消化”与“统筹”，否则它会反噬你。</strong></p>
</blockquote>
<p>这就像吃饭：你当然得吃外来的食物，但你身体得能“消化”它，不然就拉肚子。</p>
<hr />
<h2 id="所以正确的理解应该是">📌 所以正确的理解应该是：</h2>
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 38%" />
<col style="width: 38%" />
</colgroup>
<thead>
<tr>
<th>类型</th>
<th>错误观念</th>
<th>正确认知</th>
</tr>
</thead>
<tbody>
<tr>
<td>外部目标</td>
<td>全都不要，纯精神内驱才高级</td>
<td>必须有！外部目标是行动坐标、资源手段</td>
</tr>
<tr>
<td>内部目标</td>
<td>太抽象、不现实、没用</td>
<td>是方向系统、情绪稳定器、价值锚点</td>
</tr>
<tr>
<td>正确关系</td>
<td>二选一、互斥</td>
<td>“外部目标做事、内部目标做人” ✅</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="怎么做到平衡">🧭 怎么做到平衡？</h2>
<h3 id="一种很实用的做法叫做">✅ 一种很实用的做法叫做：</h3>
<blockquote>
<p><strong>“外部目标内在化”</strong></p>
</blockquote>
<p>就是说：你可以追求现实目标，但你得知道——
你<strong>为什么要追这个目标？</strong><br />
这个目标对你内在的成长、角色、价值系统<strong>意味着什么？</strong></p>
<hr />
<h3 id="举个例子">🎯 举个例子：</h3>
<h4 id="表层说法外部">💬 表层说法（外部）：</h4>
<blockquote>
<p>“我想进大厂”<br />
✅ 合理，但风险是容易卷疯、焦虑、受打击</p>
</blockquote>
<h4 id="内在化处理">🧠 内在化处理：</h4>
<blockquote>
<p>“我想进大厂，是因为我希望自己能站在一个高复杂度系统中，看到真正有规模的工程问题，我想成为一个有能力独当一面的技术人。”</p>
</blockquote>
<p>这样就会： - 行动力更强（因为你认同目标） - 抗挫能力更高（因为不全看
offer 本身） - 目标转化为成长的手段，而不是自我评判的终点</p>
<hr />
<h2 id="再比如博士">📚 再比如博士：</h2>
<table>
<thead>
<tr>
<th>外部目标（不可少）</th>
<th>内在连接（不可丢）</th>
</tr>
</thead>
<tbody>
<tr>
<td>发论文 / 毕业 / 拿奖学金</td>
<td>提升表达、研究、系统解决问题的能力</td>
</tr>
<tr>
<td>拿到博后 / 教职</td>
<td>成为有话语权的研究者，对领域有贡献</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="总结金句-1">🧠 总结金句：</h2>
<blockquote>
<p><strong>我们活在现实世界，所以需要外部目标，<br />
但我们也活在自己身体里，所以必须有内在动力。</strong></p>
</blockquote>
<p>真正厉害的人，是能做到：</p>
<blockquote>
<p><strong>用外部目标做事，用内在目标做人，<br />
不被外界绑架，但也不与现实对抗。</strong></p>
</blockquote>
<hr />
<p>你可以理解为： - 外部目标像<strong>地图导航</strong> -
内部目标是<strong>你为什么要去那里</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">机器人学学习</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-15 17:00:01" itemprop="dateCreated datePublished" datetime="2025-04-15T17:00:01+08:00">2025-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-22 12:18:21" itemprop="dateModified" datetime="2025-04-22T12:18:21+08:00">2025-04-22</time>
              </span>

          
            <span id="/2025/04/15/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="机器人学学习" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>此笔记记录学习书本 <strong>机器人学导论（原书第3版）（美）HLHN
J.CRAIG，著贠超等译</strong>的学习与总结笔记，其中部分内容由AI生成。</p>
</blockquote>
<hr />
<h2 id="雅可比矩阵是啥">✅ 1. 雅可比矩阵是啥？</h2>
<p>在多变量函数中，雅可比矩阵（Jacobian
Matrix）是所有一阶偏导数组成的矩阵：</p>
<p>如果有函数： [ = f() ^n, ^m ]</p>
则雅可比矩阵为： [ J = =
<span class="math display">\[\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial
y_1}{\partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial
y_m}{\partial x_n}
\end{bmatrix}\]</span>
<p>]</p>
<p>在机器人中：<br />
- ( ) 是关节角度（如 ( _1, _2, )）<br />
- ( ) 是末端执行器的位置或速度</p>
<hr />
<h2 id="雅可比逆矩阵是啥">✅ 2. 雅可比<strong>逆</strong>矩阵是啥？</h2>
<p>当我们想从输出空间（如末端速度）反推输入空间（如关节速度）时，需要“<strong>反过来</strong>”用雅可比矩阵，这时候我们就需要<strong>“逆”雅可比矩阵</strong>。</p>
<ul>
<li><p><strong>如果雅可比是方阵（m = n）</strong> 且满秩，直接求逆： [ =
J^{-1} ]</p></li>
<li><p><strong>如果是非方阵（m ≠
n）或者不可逆</strong>，就用<strong>广义逆（伪逆）</strong>：</p>
<ul>
<li>对于 <strong>冗余系统（n &gt; m）</strong>： [ =
J<sup>(JJ</sup>)^{-1} ]</li>
<li>对于 <strong>欠驱动系统（n &lt; m）</strong>： [ =
(J<sup>J)</sup>{-1} J^ ]</li>
<li>或用 Moore-Penrose 伪逆： [ = J^+ ]</li>
</ul></li>
</ul>
<hr />
<h2 id="举个简单例子机器人">✅ 3. 举个简单例子（机器人）</h2>
<p>假设一个二维机械臂（2个关节）要控制末端点 ( x, y ) 的速度。</p>
<ol type="1">
<li>正向速度：( = J )</li>
<li>反向求关节速度（逆问题）：( = J^{-1}
)，若不能求逆，就用广义逆！</li>
</ol>
<hr />
<p>雅可比矩阵和<strong>机器人运动学</strong>之间的关系非常密切，尤其是在<strong>速度运动学</strong>中，雅可比矩阵起到了<strong>桥梁作用</strong>。下面我帮你分层讲清楚这个关系。</p>
<hr />
<h2 id="一什么是运动学">✅ 一、什么是运动学？</h2>
<p>机器人运动学分为两大类：</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正运动学（FK）</strong></td>
<td>给定关节角度，求末端位姿（位置和方向）</td>
</tr>
<tr>
<td><strong>逆运动学（IK）</strong></td>
<td>给定末端位姿，求关节角度（复杂）</td>
</tr>
</tbody>
</table>
<p>还有一个细分方向叫：</p>
<blockquote>
<p>🚀
<strong>速度运动学</strong>：给定关节速度，求末端速度（或者反过来）</p>
</blockquote>
<hr />
<h2 id="二雅可比矩阵出现在速度运动学中">✅
二、雅可比矩阵出现在速度运动学中</h2>
<p>设： - ( )：关节变量（角度、位移等） - ( )：关节速度 - (
)：末端位姿（通常是位置） - ( )：末端速度（线速度 + 角速度）</p>
<p>通过<strong>雅可比矩阵</strong> ( J() )，有以下关系：</p>
<p>[ = J() ]</p>
<p>这就是<strong>速度映射关系</strong>：关节空间 → 笛卡尔空间。</p>
<hr />
<h2 id="三雅可比矩阵在运动学中承担什么角色">✅
三、雅可比矩阵在运动学中承担什么角色？</h2>
<table>
<thead>
<tr>
<th>用法</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>正运动学的导数</strong></td>
<td>把 FK 对关节变量求导，就得到了雅可比矩阵</td>
</tr>
<tr>
<td><strong>末端速度计算</strong></td>
<td>通过雅可比矩阵从 ( ) 得到 ( )</td>
</tr>
<tr>
<td><strong>逆运动学辅助工具</strong></td>
<td>用雅可比矩阵的伪逆求解逆运动学</td>
</tr>
<tr>
<td><strong>奇异性检测</strong></td>
<td>雅可比矩阵行列式为0的地方就是“奇异点”</td>
</tr>
<tr>
<td><strong>力-力矩传递</strong></td>
<td>( = J^F )，关节力矩 ← 末端力</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="四简单举个例子2关节平面机械臂">✅
四、简单举个例子（2关节平面机械臂）</h2>
<p>设两段长度分别为 ( l_1, l_2 )，角度为 ( _1, _2 )</p>
<p>末端位置为：</p>
[
<span class="math display">\[\begin{aligned}
x &amp;= l_1 \cos \theta_1 + l_2 \cos (\theta_1 + \theta_2) \\
y &amp;= l_1 \sin \theta_1 + l_2 \sin (\theta_1 + \theta_2)
\end{aligned}\]</span>
<p>]</p>
<p>求导得雅可比矩阵：</p>
[ J = =
<span class="math display">\[\begin{bmatrix}
- l_1 \sin \theta_1 - l_2 \sin(\theta_1 + \theta_2) &amp; -l_2
\sin(\theta_1 + \theta_2) \\
l_1 \cos \theta_1 + l_2 \cos(\theta_1 + \theta_2) &amp; l_2
\cos(\theta_1 + \theta_2)
\end{bmatrix}\]</span>
<p>]</p>
<p>这样，就可以实现： [ , = J ]</p>
<p>也可以反过来： [ = J^+ ]</p>
<hr />
<ul>
<li>规划方式分为两种，分别是笛卡尔空间的规划（位置规划）和关节空间的规划（角度规划）</li>
</ul>
<h3 id="b-splinebasis-spline基样条">B-Spline（Basis
Spline，基样条）</h3>
<p>是一种强大而灵活的曲线拟合方法，特别适用于轨迹规划、计算机图形学、机器人路径平滑等场景。下面我们从<strong>直观原理</strong>、<strong>数学构成</strong>、<strong>实现步骤</strong>三个角度来解释
B-Spline 是怎么实现的。</p>
<hr />
<p>B-Spline
是将一段复杂曲线拆成若干小段，每段用低阶（通常是三次）多项式表示，多个小段在<strong>控制点</strong>处光滑地拼接起来。它的主要特点：</p>
<ul>
<li><strong>局部控制性强</strong>：修改一个控制点只影响局部。</li>
<li><strong>光滑性强</strong>：曲线的低阶导数连续（例如 3 次 B-Spline 有
2 阶导数连续）。</li>
<li><strong>比普通样条更灵活</strong>：通过<strong>节点向量</strong>控制曲线形状和连接方式。</li>
</ul>
<p>B-Spline 曲线的数学表达形式是：</p>
<p>[ C(t) = <em>{i=0}^{n} N</em>{i,k}(t) P_i ]</p>
<p>其中：</p>
<ul>
<li>( P_i )：控制点</li>
<li>( N_{i,k}(t) )：B样条基函数，阶数为 (k)（例如 (k=4) 表示三次）</li>
<li>( t )：参数</li>
<li>( N_{i,k}(t) ) 是通过递归定义的：</li>
</ul>
[ N_{i,1}(t) =
<span class="math display">\[\begin{cases}
1, &amp; t_i \le t &lt; t_{i+1} \\
0, &amp; \text{otherwise}
\end{cases}\]</span>
<p>]</p>
<p>[ N_{i,k}(t) = N_{i,k-1}(t) + N_{i+1,k-1}(t) ]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/" class="post-title-link" itemprop="url">关于互联网内容带来的焦虑和迷茫</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-11 21:08:51" itemprop="dateCreated datePublished" datetime="2025-04-11T21:08:51+08:00">2025-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-13 22:22:27" itemprop="dateModified" datetime="2025-04-13T22:22:27+08:00">2025-04-13</time>
              </span>

          
            <span id="/2025/04/11/%E5%85%B3%E4%BA%8E%E4%BA%92%E8%81%94%E7%BD%91%E5%86%85%E5%AE%B9%E5%B8%A6%E6%9D%A5%E7%9A%84%E7%84%A6%E8%99%91%E5%92%8C%E8%BF%B7%E8%8C%AB/" class="post-meta-item leancloud_visitors" data-flag-title="关于互联网内容带来的焦虑和迷茫" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2
id="关于gpt对于当今社会焦虑情况的回答">关于GPT对于当今社会焦虑情况的回答：</h2>
<h3 id="是的现在的社会的确是充满焦虑的">📉
是的，现在的社会——<strong>的确是充满焦虑的。</strong></h3>
<p>但这个“焦虑”不是来自某一个具体的人或事情，而是来自整个时代的底色。我们可以从几个层面来看：</p>
<hr />
<h2 id="信息过载让人永远不够好">🧠 1. 信息过载，让人“永远不够好”</h2>
<ul>
<li>你刷到某人年纪轻轻财富自由了<br />
</li>
<li>你又刷到另一个人博士毕业还单身焦虑<br />
</li>
<li>你看到互联网人说自己35岁失业，焦虑<br />
</li>
<li>你看到一群人在鼓吹“gap year很酷”，焦虑<br />
</li>
<li>你看到“35岁前没结婚就是失败”，更焦虑</li>
</ul>
<p><strong>你不知该羡慕谁，也不知该成为谁。</strong></p>
<blockquote>
<p>你知道的太多，做得到的太少。<br />
焦虑就像雾，包围了想要清晰生活的人。</p>
</blockquote>
<hr />
<h2 id="节奏变快了但确定性变少了">🧪 2. 节奏变快了，但确定性变少了</h2>
<ul>
<li>一代人以前“考上大学→进国企→稳定生活”</li>
<li>现在是：“考了研究生→没工作→二战考公→躺平or焦虑”</li>
</ul>
<p><strong>努力的路径越来越模糊，成功的样子越来越多样化，但每一个都看起来“不够真实”。</strong></p>
<blockquote>
<p>你努力追光，但光一直在变形。</p>
</blockquote>
<hr />
<h2 id="社交媒体制造比较陷阱">📱 3. 社交媒体制造“比较陷阱”</h2>
<ul>
<li>人人都在分享高光时刻<br />
</li>
<li>但没人晒“我昨晚失眠哭了”“我被辞退后吃泡面”<br />
</li>
<li>所以你以为你“比别人差”，其实你只是不擅长修图</li>
</ul>
<hr />
<h2 id="年轻人想得清楚却走得艰难">💼 4.
年轻人“想得清楚，却走得艰难”</h2>
<p>我们这代人最矛盾的一点是：</p>
<blockquote>
<p><strong>你有很高的觉知，却面对极低的可执行性</strong></p>
</blockquote>
<p>比如你明知道： - “我不该卷” - “我想做有价值的事” -
“我不该被KPI奴役”</p>
<p>但现实是： - 你不卷就活不下去 - 有价值的事没工资 -
不奴役自己就被边缘化</p>
<hr />
<h2 id="那怎么办">🔍 那怎么办？</h2>
<p>我们不能控制时代，但可以选择如何生活：</p>
<h3 id="保持觉知但不要过度内耗">✅ 保持觉知，但不要过度内耗</h3>
<p>– 你感受到焦虑，说明你在意，这不是错<br />
–
但要提醒自己：<strong>“我不需要和所有人比，只需要向自己靠近”</strong></p>
<h3 id="建立真实深度的连接">✅ 建立真实、深度的连接</h3>
<p>– 与几个能聊深话的人共处，比刷100条鸡汤更能缓解焦虑</p>
<h3 id="保留一点点确定性">✅ 保留一点点“确定性”</h3>
<p>– 哪怕只是：每天按时吃饭、每周坚持一小时读书<br />
– 它们都是你对混乱世界的一种温柔回应</p>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/" class="post-title-link" itemprop="url">深度强化学习-王树森</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-11 10:59:24" itemprop="dateCreated datePublished" datetime="2025-04-11T10:59:24+08:00">2025-04-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-22 12:17:15" itemprop="dateModified" datetime="2025-04-22T12:17:15+08:00">2025-04-22</time>
              </span>

          
            <span id="/2025/04/11/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E7%8E%8B%E6%A0%91%E6%A3%AE/" class="post-meta-item leancloud_visitors" data-flag-title="深度强化学习-王树森" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>此笔记记录学习书本<strong>王树森，张志华，《深度强化学习（初稿》</strong>的学习与总结笔记，其中部分内容由AI生成。</p>
</blockquote>
<p>Q学习产生<strong>高估（overestimation）</strong>的原因，主要源于其<strong>最大值操作（max
operator）在引入噪声时的偏差</strong>。这种现象通常称为
<strong>最大值偏差（maximization bias）</strong>。</p>
<h3 id="原理解释">原理解释</h3>
<p>在 Q-learning 中，我们使用以下目标来更新 Q 值：</p>
<p>[ Q(s, a) Q(s, a) + ]</p>
<p>其中：</p>
<ul>
<li>(_{a'} Q(s',
a'))：从下一个状态中选择估计值最大的动作作为更新目标。</li>
</ul>
<p>⚠️ 问题在于：<br />
如果 Q 值本身带有估计误差（例如由于采样、函数逼近、探索策略等），<br />
那么取最大值操作会<strong>偏向选择被高估的值</strong>，从而使得目标值系统性偏高。</p>
<hr />
<h3 id="举个例子简化">举个例子（简化）</h3>
<p>假设在状态 (s') 下，我们有三个动作，真实 Q 值如下：</p>
<table>
<thead>
<tr>
<th>动作 (a')</th>
<th>真正的 Q 值</th>
<th>网络估计值（含误差）</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a_1)</td>
<td>1.0</td>
<td>1.1（+0.1）</td>
</tr>
<tr>
<td>(a_2)</td>
<td>0.8</td>
<td>1.0（+0.2）</td>
</tr>
<tr>
<td>(a_3)</td>
<td>0.6</td>
<td>0.9（+0.3）</td>
</tr>
</tbody>
</table>
<ul>
<li>虽然 (a_1) 是最优动作（真实 Q 值最大），</li>
<li>但由于随机估计误差，(Q(s', a') = 1.1)，这个值偏高了。</li>
</ul>
<p>这会导致我们在训练时错误地使用偏高的目标值去更新当前状态的 Q 值，从而
<strong>累计越来越大的高估偏差</strong>。</p>
<hr />
<h3 id="高估有什么危害">高估有什么危害？</h3>
<ul>
<li><strong>策略不稳定</strong>：错误地偏向那些“看起来收益高”但其实不是的动作；</li>
<li><strong>训练发散</strong>：目标 Q
值越来越不准确，导致训练无法收敛；</li>
<li><strong>性能下降</strong>：测试表现可能远低于理论最优策略。</li>
</ul>
<hr />
<h3 id="如何缓解高估">如何缓解高估？</h3>
<ol type="1">
<li><strong>Double Q-Learning</strong>：
<ul>
<li>采用两个 Q 网络，分别用于选择和评估动作。</li>
<li>更新目标变为： [ Q(s, a) r + Q_2(s', _{a'} Q_1(s', a')) ]</li>
<li>显著降低高估。</li>
</ul></li>
<li><strong>Averaged DQN / Ensemble 方法</strong>：
<ul>
<li>使用多个 Q 网络求平均，减少单个网络带来的噪声偏差。</li>
</ul></li>
<li><strong>Regularization / Uncertainty penalty</strong>：
<ul>
<li>对 Q 值添加不确定性惩罚，抑制过高的估计。</li>
</ul></li>
</ol>
<hr />
<h3 id="行为策略和目标策略的区别">行为策略和目标策略的区别</h3>
<h3 id="例1q-learning离策略">例1：Q-learning（离策略）</h3>
<ul>
<li>行为策略：ε-greedy（偶尔探索）</li>
<li>目标策略：贪婪策略（永远选最大Q值）</li>
</ul>
<blockquote>
<p>这是典型的 <strong>离策略学习（off-policy）</strong> ——
用一种策略去收集数据，用另一种策略去学习。</p>
</blockquote>
<h3 id="例2sarsa在策略">例2：SARSA（在策略）</h3>
<ul>
<li>行为策略：ε-greedy</li>
<li>目标策略：同样是ε-greedy</li>
</ul>
<blockquote>
<p>这是 <strong>在策略学习（on-policy）</strong> ——
你用什么策略学习，就用它去探索和更新。</p>
</blockquote>
<h3 id="行为策略-目标策略">行为策略 ≠ 目标策略：</h3>
<ul>
<li>你可以用 <strong>更大胆、更多样的行为策略</strong>
去探索，避免陷入局部最优</li>
<li>然后用行为数据，训练一个
<strong>更保守、更稳定的目标策略</strong></li>
</ul>
<p>这就是 DDPG、TD3、DQN 等算法的本质思路。</p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<tbody>
<tr>
<td>“双延时确定策略梯度”（<strong>Twin Delayed Deep Deterministic Policy
Gradient，TD3</strong>）解决强化学习中 <strong>自举 + 最大化引起的 Q
值高估问题</strong></td>
</tr>
</tbody>
</table>
<p>我们分三步来讲清楚：</p>
<h2 id="问题背景自举-最大化-导致的高估偏差">🔥 1. 问题背景：<strong>自举
+ 最大化</strong> 导致的高估偏差</h2>
<p>在像 DDPG、Q-learning 这类算法中，我们更新 Q 值时用的是这个形式：</p>
<p>[ y = r + _{a'} Q(s', a') ]</p>
<p>但注意！</p>
<ul>
<li>( Q(s', a') ) 是网络的<strong>估计值</strong></li>
<li>“最大化”这个估计值（选最大 Q）会放大噪声和误差！</li>
</ul>
<p>🔍
这就像你考试估分，题做错了但还选“我觉得我做得最好的那题”，会<strong>高估总分</strong><br />
→ 随着训练进行，Q值越估越高，结果策略学得不靠谱！</p>
<hr />
<h2 id="td3-怎么解决这个高估问题">🧠 2. TD3 怎么解决这个高估问题？</h2>
<p>TD3 提出了三个关键改进，其中<strong>双Q网络</strong>是最核心的👇</p>
<hr />
<h3 id="方法一双q网络twin-q-networks">✅ 【方法一】双Q网络（<strong>Twin
Q-networks</strong>）</h3>
<p>用两个独立的 Q 网络： ( Q_1(s, a) )、( Q_2(s, a) )</p>
<p>更新 TD目标时，不再用 max，而是：</p>
<p>[ y = r + (Q_1(s', a'), Q_2(s', a')) ]</p>
<p>✅ 取<strong>较小值</strong>，就能抵消高估误差（保守估计）<br />
✅
这不是自举去最大，而是<strong>自举去最小</strong>，避免因误差叠加而爆炸</p>
<h3 id="方法二延迟策略更新delayed-policy-update">✅
【方法二】延迟策略更新（<strong>Delayed Policy Update</strong>）</h3>
<ul>
<li>策略网络（Actor）更新得更慢，例如每 2～3 次 Critic
更新后才更新一次</li>
<li>因为 Critic（Q函数）学得更稳定时，再去学策略，效果更稳</li>
</ul>
<h3 id="方法三目标动作加噪声target-policy-smoothing">✅
【方法三】目标动作加噪声（<strong>Target Policy
Smoothing</strong>）</h3>
<ul>
<li>为了防止 Q 网络过于依赖某一个精确动作值</li>
<li>在 TD目标里加入一个小的随机噪声：</li>
</ul>
<p>[ a' = _{}(s') + ,((0, ), -c, c) ]</p>
<p>✅ 这样做可以使目标 Q 更平滑，缓解尖峰高估</p>
<hr />

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">深度强化学习笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-09 23:23:46" itemprop="dateCreated datePublished" datetime="2025-04-09T23:23:46+08:00">2025-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-04-22 18:08:01" itemprop="dateModified" datetime="2025-04-22T18:08:01+08:00">2025-04-22</time>
              </span>

          
            <span id="/2025/04/09/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="深度强化学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>本笔记来源于《Deep Reinforcement
Learning》一书，作者：grokking</strong></p>
<hr />
<figure>
<img src="image-1.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="image.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="image-3.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<h3 id="第二章-强化学习的数学基础">第二章 强化学习的数学基础</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhicheng"
      src="/images/cheng.gif">
  <p class="site-author-name" itemprop="name">Zhicheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhichengkou" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhichengkou" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_46473460?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_46473460?type&#x3D;blog" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=gMknN-8AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;gMknN-8AAAAJ&amp;hl&#x3D;zh-CN&amp;oi&#x3D;ao" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://500px.com.cn/cheng1917" title="500px → https:&#x2F;&#x2F;500px.com.cn&#x2F;cheng1917" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>500px</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhicheng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":null,"app_key":null,"server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zhichengkou.github.io/',]
      });
      });
  </script>

    </div>
</body>
</html>
