<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhichengkou.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="On-policy 和 Off-policy 的区别         特征 On-policy (SARSA) Off-policy (Q-learning)     行为策略与目标策略的关系 行为策略与目标策略相同 行为策略和目标策略不同   策略选择依据 代理在每个状态下根据当前策略选择动作 代理根据探索性策略选择动作，但更新时使用目标策略（贪婪">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习笔记">
<meta property="og:url" content="http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Zhicheng">
<meta property="og:description" content="On-policy 和 Off-policy 的区别         特征 On-policy (SARSA) Off-policy (Q-learning)     行为策略与目标策略的关系 行为策略与目标策略相同 行为策略和目标策略不同   策略选择依据 代理在每个状态下根据当前策略选择动作 代理根据探索性策略选择动作，但更新时使用目标策略（贪婪">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-07T11:48:01.000Z">
<meta property="article:modified_time" content="2025-04-07T11:48:54.790Z">
<meta property="article:author" content="Zhicheng">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="笔记总结">
<meta property="article:tag" content="书本学习">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>强化学习笔记 | Zhicheng</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhicheng</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cheng.gif">
      <meta itemprop="name" content="Zhicheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhicheng">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          强化学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-04-07 19:48:01 / 修改时间：19:48:54" itemprop="dateCreated datePublished" datetime="2025-04-07T19:48:01+08:00">2025-04-07</time>
            </span>

          
            <span id="/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-meta-item leancloud_visitors" data-flag-title="强化学习笔记" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="on-policy-和-off-policy-的区别">On-policy 和 Off-policy
的区别</h2>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 37%" />
<col style="width: 39%" />
</colgroup>
<thead>
<tr>
<th>特征</th>
<th><strong>On-policy (SARSA)</strong></th>
<th><strong>Off-policy (Q-learning)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>行为策略与目标策略的关系</strong></td>
<td>行为策略与目标策略相同</td>
<td>行为策略和目标策略不同</td>
</tr>
<tr>
<td><strong>策略选择依据</strong></td>
<td>代理在每个状态下根据当前策略选择动作</td>
<td>代理根据探索性策略选择动作，但更新时使用目标策略（贪婪策略）</td>
</tr>
<tr>
<td><strong>学习更新</strong></td>
<td>通过选择的动作 ( A ) 和下一个动作 ( A' ) 来更新 Q 值</td>
<td>通过选择的动作 ( A ) 和下一个状态的最优动作来更新 Q 值</td>
</tr>
<tr>
<td><strong>例子</strong></td>
<td>使用 <strong>(
)-贪婪策略</strong>，代理每次选择当前策略下的动作，并在下一个状态上选择动作
( A' ) 来更新 Q 值。</td>
<td>使用 <strong>( )-贪婪策略</strong>
进行探索，但更新时始终选择最优动作来更新 Q 值。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="三种方法对比总结"><strong>三种方法对比总结</strong></h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 27%" />
<col style="width: 12%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>更新方式</th>
<th>适用场景</th>
<th>是否需要环境模型</th>
<th>计算量</th>
<th>收敛速度</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>蒙特卡洛</strong></td>
<td>只有回合结束后更新</td>
<td>适用于无模型、离线学习</td>
<td><strong>不需要</strong></td>
<td><strong>高</strong></td>
<td><strong>慢</strong></td>
</tr>
<tr>
<td><strong>时序差分（TD）</strong></td>
<td>每一步都更新</td>
<td>适用于无模型、在线学习</td>
<td><strong>不需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
<tr>
<td><strong>动态规划（DP）</strong></td>
<td>通过转移概率计算更新</td>
<td>适用于已知环境</td>
<td><strong>需要</strong></td>
<td><strong>低</strong></td>
<td><strong>快</strong></td>
</tr>
</tbody>
</table>
<ol type="1">
<li><strong>蒙特卡洛（MC）</strong>：必须 <strong>等待回合结束</strong>
才能更新，不适用于实时学习。</li>
<li><strong>时序差分（TD）</strong>：<strong>每一步都更新</strong>，比蒙特卡洛更快，适用于
<strong>在线学习</strong>。</li>
<li><strong>动态规划（DP）</strong>：需要
<strong>环境模型</strong>（转移概率），但计算最精确，适用于
<strong>已知环境</strong>。</li>
</ol>
<p>在强化学习中，<strong>TD
学习是最常用的，因为它既能在线更新，又不依赖于环境模型</strong>。蒙特卡洛主要用于
<strong>离线学习</strong>，而动态规划适用于
<strong>已知环境</strong>（如象棋等确定性环境）。</p>
<hr />
<h2 id="同轨策略-vs.-离轨策略">同轨策略 vs. 离轨策略</h2>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th></th>
<th><strong>同轨策略（On-policy）</strong></th>
<th><strong>离轨策略（Off-policy）</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>数据来源</strong></td>
<td>当前策略</td>
<td>其他策略</td>
</tr>
<tr>
<td><strong>更新的策略</strong></td>
<td>使用同一个策略更新</td>
<td>使用不同策略更新</td>
</tr>
<tr>
<td><strong>典型算法</strong></td>
<td>SARSA、Actor-Critic、REINFORCE</td>
<td>Q-learning、DQN</td>
</tr>
<tr>
<td><strong>探索方式</strong></td>
<td>需要手动引入探索（如 ε-贪心）</td>
<td>训练目标本身包含最优策略</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>相对较慢</td>
<td>相对较快</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>同轨策略（On-policy）</strong> 更像是一个人在
<strong>边学边做</strong>，不断根据自己的经验来调整行为。</li>
<li><strong>离轨策略（Off-policy）</strong> 更像是一个人
<strong>看着别人的经验</strong> 来学习，并且能更快收敛到最优解。</li>
</ul>
<hr />
<table>
<colgroup>
<col style="width: 4%" />
<col style="width: 45%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr>
<th><strong>方法</strong></th>
<th><strong>更新公式</strong></th>
<th><strong>说明</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>[ V(s_t) V(s_t) + ]</td>
<td>通过当前状态的价值和下一个状态的价值来更新，估计状态的价值。仅依赖下一个状态的值来进行更新，适用于
<strong>状态价值估计</strong>。</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>使用当前策略进行学习，依赖于当前的动作选择，<strong>当前状态和动作</strong>以及<strong>下一个状态和动作</strong>共同更新
Q 值，适用于 <strong>策略学习</strong>。</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>通过选择最大 Q
值的动作进行学习，无论当前策略如何，只关心未来的最佳动作，适用于
<strong>最优策略学习</strong>。</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>[ Q(s_t, a_t) Q(s_t, a_t) + ]</td>
<td>结合了 TD(0)
和蒙特卡洛方法，回溯多步更新，适用于长期学习，特别是在复杂任务中，考虑到
<strong>多步未来的反馈</strong>。</td>
</tr>
</tbody>
</table>
<h3
id="为什么sarsa进行的是策略学习而非状态价值估计">为什么SARSA进行的是策略学习而非状态价值估计？</h3>
<ol type="1">
<li><p><strong>动作价值函数（( Q(s, a) )）的学习</strong>：
SARSA的目标是学习动作价值函数 ( Q(s, a) )，而不是单独学习状态价值函数 (
V(s)
)。这是因为在强化学习中，我们关心的通常是<strong>在某个状态下采取某个动作后能获得的回报</strong>，而不仅仅是状态本身的“价值”。动作价值函数能够直接反映在某个状态下采取特定动作的好坏。</p></li>
<li><p><strong>SARSA是基于策略的学习</strong>：
SARSA算法通过采取当前策略下的动作，并根据当前策略的行为来估计未来的回报。这意味着，SARSA本质上是在<strong>同轨策略</strong>下学习，并通过当前策略来选择动作。因此，它不仅关注状态本身，而是通过状态-动作对（(
s_t, a_t )）来进行学习。</p>
<ul>
<li>在每个时间步，SARSA都会通过当前策略选择动作（比如 (
)-贪婪策略），然后更新相应的 ( Q(s, a) )。</li>
<li>而对于<strong>状态价值函数</strong>（( V(s)
)），它只是通过所有可能动作的加权平均来得到的，因此，如果我们只学习状态价值而不学习动作价值，我们就无法通过具体的动作决策来直接优化策略。</li>
</ul></li>
<li><p><strong>策略学习与状态价值的关系</strong>： 状态价值函数 ( V(s) )
只能在策略已知的情况下计算，而在SARSA中，策略本身是通过<strong>持续更新的动作价值函数</strong>来间接优化的。SARSA不直接估计状态价值函数，而是通过(
Q(s, a) )来间接影响策略的改进。</p></li>
<li><p><strong>为什么动作价值比状态价值更有意义？</strong>：
在强化学习中，我们的目标通常是<strong>最大化回报</strong>，而回报的最大化依赖于在每个状态下选择合适的动作。学习<strong>状态价值</strong>只是帮助我们知道一个状态本身的好坏，但我们仍然需要知道在每个状态下应该采取什么样的动作来优化回报。这就是为什么SARSA（以及其他基于值的策略学习算法）会直接学习<strong>动作价值函数</strong>。</p>
<ul>
<li>在<strong>策略迭代</strong>或<strong>值迭代</strong>中，策略是通过状态值函数来更新的，但SARSA通过动作值函数来优化策略，因为动作选择比状态值更直接影响回报。</li>
</ul></li>
</ol>
<h3 id="sarsa与状态价值估计的关系">SARSA与状态价值估计的关系</h3>
<p>尽管SARSA本身不直接估计状态价值函数 ( V(s)
)，但通过学习<strong>动作价值函数</strong>（( Q(s, a)
)），我们可以间接地得到状态价值函数 ( V(s)
)。实际上，状态价值函数可以通过对所有可能动作的加权平均来计算： [ V(s) =
_a (a|s) Q(s, a) ] 其中，( (a|s) ) 是在状态 ( s ) 下采取动作 ( a )
的概率。通过这种方式，尽管SARSA不直接估计状态价值函数，但它间接提供了可以计算状态价值函数的所有信息。</p>
<h3 id="总结">总结：</h3>
<p>SARSA算法专注于学习<strong>动作价值函数</strong>（( Q(s, a)
)），而不是直接估计状态价值函数 ( V(s)
)。这是因为在强化学习中，目标是优化策略，通过学习每个状态-动作对的价值来选择最佳动作。SARSA通过直接估计<strong>每个状态下的动作价值</strong>，而不是仅仅估计状态本身的价值，从而能够更有效地学习和改进策略。</p>
<h3 id="常用-td-方法的区别"><strong>常用 TD 方法的区别</strong></h3>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 14%" />
<col style="width: 27%" />
<col style="width: 18%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>策略类型</th>
<th>更新方式</th>
<th>目标</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD(0)</strong></td>
<td>同轨</td>
<td>仅依赖当前状态的价值和下一个状态的价值进行更新</td>
<td>估计状态价值函数</td>
<td>在线实时学习，适用于估计状态值</td>
</tr>
<tr>
<td><strong>SARSA</strong></td>
<td>同轨</td>
<td>当前动作和下一个动作共同更新 Q 值</td>
<td>学习当前策略的 Q 值</td>
<td>行为安全，如机器人、自动驾驶</td>
</tr>
<tr>
<td><strong>Q-learning</strong></td>
<td>离轨</td>
<td>最大化下一个状态的 Q 值进行更新</td>
<td>学习最优策略的 Q 值</td>
<td>追求最优策略，游戏、围棋 AI</td>
</tr>
<tr>
<td><strong>TD(λ)</strong></td>
<td>同轨</td>
<td>结合 TD(0) 和蒙特卡洛方法，回溯多步更新</td>
<td>估计状态-动作值函数</td>
<td>长期学习，适用于复杂任务</td>
</tr>
</tbody>
</table>
<h3 id="对比说明"><strong>对比说明：</strong></h3>
<ul>
<li><strong>TD(0)</strong>：只考虑一个时间步的更新，适用于简单的状态价值估计。</li>
<li><strong>SARSA</strong>：基于当前策略，更新状态-动作值 Q(s,
a)，适用于<strong>在线学习</strong>和<strong>行为安全</strong>。</li>
<li><strong>Q-learning</strong>：选择未来最大 Q
值的动作更新状态-动作值，适用于追求<strong>最优策略</strong>。</li>
<li><strong>TD(λ)</strong>：通过多个时间步回溯来更新，结合了 TD(0)
和蒙特卡洛方法，适用于 <strong>长期学习</strong>和
<strong>复杂任务</strong>。</li>
</ul>
<hr />
<p>Dyna-Q 算法家族是基于强化学习中
<strong>将真实经验与模型模拟经验结合</strong>
的一类算法。它们融合了<strong>模型学习（Model-based）</strong>与<strong>价值函数更新（如
Q-learning）</strong>，具有较高的样本效率。</p>
<p>以下是 Dyna-Q
及其常见变体的对比表格，包括其核心思路、改进点、是否探索模型变化等关键特征。</p>
<hr />
<h2 id="dyna-q-及其主要变体对比表">Dyna-Q 及其主要变体对比表</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 7%" />
<col style="width: 24%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 22%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr>
<th>算法名称</th>
<th>核心思想</th>
<th>是否使用模型</th>
<th>是否增强探索</th>
<th>是否处理非定常环境</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Dyna-Q</strong></td>
<td>基础版本，将 Q-learning 与模拟经验结合</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>❌ 否</td>
<td>学习效率高、结构简单</td>
<td>模型质量差时可能误导学习</td>
</tr>
<tr>
<td><strong>Dyna-Q+</strong></td>
<td>在模型中加入“探索奖励”，鼓励模拟冷门状态-动作对</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>增强探索能力、适应非定常性</td>
<td>增加超参数（探索奖励函数）</td>
</tr>
<tr>
<td><strong>Prioritized Dyna-Q</strong></td>
<td>使用优先级队列优先更新影响大的状态</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>❌ 否</td>
<td>更高效率、优先更新重要经验</td>
<td>实现复杂度稍高</td>
</tr>
<tr>
<td><strong>Dyna-2</strong></td>
<td>在模型中使用多个估计模型进行模拟（不确定建模）</td>
<td>✅ 多模型</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>对不确定性建模，有利于探索与适应</td>
<td>存储和计算量大</td>
</tr>
<tr>
<td><strong>Dyna-Q with Linear Function Approximation</strong></td>
<td>使用线性函数逼近代替 Q 表</td>
<td>✅ 有</td>
<td>❌ 否</td>
<td>✅ 视模型</td>
<td>可扩展到大状态空间</td>
<td>精度依赖函数逼近器质量</td>
</tr>
<tr>
<td><strong>Dyna-Q with Neural Networks</strong></td>
<td>用神经网络逼近 Q 值和环境模型</td>
<td>✅ 有</td>
<td>✅ 是</td>
<td>✅ 是</td>
<td>可用于连续空间、复杂环境</td>
<td>容易不稳定，需要技巧调参</td>
</tr>
</tbody>
</table>
<h3 id="dyna-q">1. <strong>Dyna-Q</strong></h3>
<ul>
<li>基础版本，结合真实交互 + 模拟经验。</li>
<li>每次从模型中随机选择过往 (s, a)，进行“虚拟更新”。</li>
</ul>
<h3 id="dyna-q-1">2. <strong>Dyna-Q+</strong></h3>
<ul>
<li>解决 Dyna-Q 早期探索不足的问题。</li>
<li>给模型中<strong>长时间未访问的 (s, a)</strong> 加一个额外奖励：</li>
</ul>
<p>[ = k ]</p>
<p>其中 τ 是未访问的时间步数，k 是超参数。</p>
<h3 id="prioritized-dyna-q">3. <strong>Prioritized Dyna-Q</strong></h3>
<ul>
<li>模拟更新不再随机，而是基于“状态更新的重要性”进行排序更新（如 TD
误差越大，越优先）。</li>
<li>使用优先队列实现，更高效。</li>
</ul>
<h3 id="dyna-2">4. <strong>Dyna-2</strong></h3>
<ul>
<li>使用多个不同的模型（表示环境可能的变化），每次模拟时从这些模型中采样。</li>
<li>更适合非定常或不确定环境（如博弈、对手变换等）。</li>
</ul>
<h3 id="函数逼近版本线性-神经网络">5. <strong>函数逼近版本（线性 /
神经网络）</strong></h3>
<ul>
<li>替代表格型 Q 值，用函数逼近器（如神经网络）逼近 Q
值或环境动态。</li>
<li>适用于大规模/连续状态空间的环境。</li>
</ul>
<h2 id="总结推荐不同需求">总结推荐（不同需求）</h2>
<table>
<thead>
<tr>
<th>使用场景</th>
<th>推荐算法</th>
</tr>
</thead>
<tbody>
<tr>
<td>小型离散环境</td>
<td>Dyna-Q / Dyna-Q+</td>
</tr>
<tr>
<td>希望强化探索、避免早收敛</td>
<td>Dyna-Q+</td>
</tr>
<tr>
<td>数据有限，希望学习快</td>
<td>Prioritized Dyna-Q</td>
</tr>
<tr>
<td>非定常环境（如对手策略变化）</td>
<td>Dyna-Q+ 或 Dyna-2</td>
</tr>
<tr>
<td>状态空间较大 / 连续</td>
<td>函数逼近版 Dyna-Q</td>
</tr>
<tr>
<td>有强大计算资源+深度学习</td>
<td>Dyna-Q with Neural Networks</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="第一部分总结">第一部分总结</h3>
<p>在强化学习中，<strong>时间差分（TD）</strong>方法、<strong>动态规划（DP）</strong>方法和<strong>蒙特卡洛（MC）</strong>方法是三种常见的策略评估和优化技术。每种方法都有其适用的场景，并且它们的优缺点也有所不同。下面是对这三种方法的简要概述，以及它们分别适合的场景。</p>
<table>
<colgroup>
<col style="width: 7%" />
<col style="width: 30%" />
<col style="width: 30%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>TD（时间差分）</strong></td>
<td>在线学习、实时学习、大规模状态空间、部分可观察的环境</td>
<td>能够在线更新，适合实时学习，计算效率高</td>
<td>收敛慢，对初始值敏感，可能产生偏差</td>
</tr>
<tr>
<td><strong>DP（动态规划）</strong></td>
<td>已知完整环境模型、离线学习、有限状态空间</td>
<td>能找到最优策略和最优值函数，理论精确</td>
<td>需要完整的环境模型，不适用于大规模状态空间</td>
</tr>
<tr>
<td><strong>MC（蒙特卡洛）</strong></td>
<td>完全可观察的环境、回合结束后更新、离线学习</td>
<td>不需要环境模型，只需回报进行更新，适合复杂环境</td>
<td>需要等待回合结束，不适合在线学习，计算低效（尤其状态空间大时）</td>
</tr>
</tbody>
</table>
<hr />
<p>在强化学习中，<strong>基于函数逼近的同轨策略预测</strong>（On-Policy
Prediction with Function
Approximation）是指在使用<strong>函数逼近</strong>来估计值函数时，采用与当前策略相同的轨迹来进行学习。这种方法通常是为了处理大规模或连续的状态空间，其中使用表格型方法来表示值函数变得不再可行。</p>
<h3 id="同轨策略on-policy">1.
<strong>同轨策略（On-Policy）</strong>：</h3>
<ul>
<li><strong>同轨策略</strong>意味着学习过程中使用的策略（通常是行为策略）和用于估计状态值函数的策略是相同的。在这种方法中，智能体根据当前策略与环境进行交互并收集经验，而使用这些经验来更新策略的值函数。</li>
</ul>
<h3 id="函数逼近function-approximation">2. <strong>函数逼近（Function
Approximation）</strong>：</h3>
<ul>
<li><strong>函数逼近</strong>是为了处理大规模状态空间（或者连续空间），通过使用参数化函数（如线性函数、神经网络等）来逼近值函数（如状态值函数
( V(s) ) 或动作值函数 ( Q(s, a)
)），避免使用表格形式存储所有可能的状态或动作。</li>
<li>函数逼近可以帮助将值函数从一个巨大的状态空间简化为一个相对较小的参数空间，从而能够在更大的环境中进行有效的学习。</li>
</ul>
<h3 id="同轨策略预测的内容">3.
<strong>同轨策略预测的内容</strong>：</h3>
<p>在基于函数逼近的同轨策略预测中，主要包括以下几个方面：</p>
<h4 id="使用td时间差分方法进行同轨策略预测">1.
<strong>使用TD（时间差分）方法进行同轨策略预测</strong>：</h4>
<ul>
<li><p>同轨策略预测的经典方法是使用<strong>TD方法</strong>，例如<strong>TD(0)</strong>，在这种方法中，值函数是通过逐步更新的。</p></li>
<li><p>通过<strong>TD(0)</strong> 或
<strong>SARSA</strong>（基于同轨策略的TD方法），智能体根据其当前策略进行状态转移并计算每一步的回报。随着时间的推移，智能体不断根据回报对其值函数进行更新。</p>
<p>典型的更新公式为： [ V(s_t) V(s_t) + ] 其中：</p>
<ul>
<li>( s_t ) 是当前状态，</li>
<li>( r_{t+1} ) 是当前奖励，</li>
<li>( V(s_{t+1}) ) 是下一个状态的估计值，</li>
<li>( ) 是折扣因子，</li>
<li>( ) 是学习率。</li>
</ul></li>
</ul>
<h4 id="线性函数逼近">2. <strong>线性函数逼近</strong>：</h4>
<ul>
<li>在使用<strong>线性函数逼近</strong>时，值函数 ( V(s) )
被表示为状态特征的线性组合： [ V(s) = ^T (s) ] 其中 ( ) 是参数向量，(
(s) ) 是状态 ( s ) 的特征向量。</li>
<li>通过<strong>梯度下降</strong>或其他优化方法，智能体学习合适的参数 (
) 来逼近状态的值函数。</li>
</ul>
<h4 id="非线性函数逼近例如神经网络">3.
<strong>非线性函数逼近</strong>（例如神经网络）：</h4>
<ul>
<li>对于更加复杂的任务，尤其是当状态空间非常大且不容易用线性函数表示时，可以使用<strong>神经网络</strong>作为函数逼近器。神经网络可以通过多层结构捕捉状态和价值之间的复杂关系。</li>
<li><strong>深度Q网络（DQN）</strong>是一个经典的例子，它使用深度神经网络来逼近动作值函数
( Q(s, a) )，并通过同轨策略进行学习。</li>
</ul>
<h4 id="策略评估和更新">4. <strong>策略评估和更新</strong>：</h4>
<ul>
<li>在基于函数逼近的同轨策略预测中，智能体不断评估当前策略的表现并使用TD方法更新其值函数。同时，策略也会基于这些估计进行调整，以改善智能体的表现。</li>
</ul>
<h4 id="基于梯度的更新">5. <strong>基于梯度的更新</strong>：</h4>
<ul>
<li><p>对于非线性逼近（如神经网络），值函数的更新通常通过<strong>梯度下降</strong>进行。智能体根据策略评估的结果（即对状态值函数或动作值函数的估计），通过反向传播算法来更新函数逼近器的参数。</p>
<p>例如，在使用神经网络时，参数 ( )
会根据损失函数的梯度进行调整，以最小化预测值与真实回报之间的差距。</p></li>
</ul>
<h3 id="同轨策略预测的经典算法">4.
<strong>同轨策略预测的经典算法</strong>：</h3>
<p>以下是一些常见的使用函数逼近的同轨策略预测算法：</p>
<h4 id="sarsaλ">(1) <strong>SARSA(λ)</strong>：</h4>
<ul>
<li><strong>SARSA(λ)</strong>
是一种结合了<strong>时间差分学习（TD）</strong>和<strong>跟踪误差</strong>的算法，使用λ-回溯来进行更长期的估计。</li>
<li>使用类似于TD(0)的更新规则，但通过加权历史的TD误差来改进更新过程，从而获得更快的收敛。</li>
</ul>
<h4 id="深度q学习dqn">(2) <strong>深度Q学习（DQN）</strong>：</h4>
<ul>
<li><strong>DQN</strong>
是一种结合了<strong>Q-learning</strong>和<strong>深度神经网络</strong>的方法，使用神经网络逼近动作值函数
( Q(s, a) )。</li>
<li>在DQN中，策略是基于贪心策略（即选择最大Q值的动作），并且通过训练神经网络来更新Q值。</li>
</ul>
<h4 id="a3c异步优势演员-评论家">(3)
<strong>A3C（异步优势演员-评论家）</strong>：</h4>
<ul>
<li><strong>A3C</strong>
是一种基于<strong>策略梯度</strong>的强化学习算法，使用两个网络（演员和评论家）来估计策略和状态值函数。A3C采用了多线程训练，在多个环境中同时训练，来提高学习效率。</li>
</ul>
<h3 id="优缺点分析">5. <strong>优缺点分析</strong>：</h3>
<h4 id="优点">优点：</h4>
<ul>
<li><strong>大规模问题</strong>：基于函数逼近的方法可以处理大规模或连续的状态空间，不需要每个状态都保存一个值。</li>
<li><strong>适用动态环境</strong>：与表格型方法不同，基于函数逼近的方法可以在动态和复杂的环境中工作。</li>
<li><strong>灵活性</strong>：可以使用线性和非线性函数逼近（如神经网络），提供了更大的灵活性来应对各种环境。</li>
</ul>
<h4 id="缺点">缺点：</h4>
<ul>
<li><strong>收敛性问题</strong>：当使用非线性函数逼近时（例如神经网络），可能会面临不稳定性和收敛性问题。</li>
<li><strong>过拟合</strong>：如果模型过于复杂，可能会过拟合训练数据，导致泛化能力差。</li>
<li><strong>需要大量计算</strong>：使用深度学习等方法时，计算成本和训练时间可能非常高。</li>
</ul>
<h3 id="总结-1">总结：</h3>
<p><strong>基于函数逼近的同轨策略预测</strong>利用当前策略下的经验来更新值函数或策略，适用于大规模或连续的状态空间，并使用函数逼近（如线性函数或神经网络）来解决表格型方法无法处理的问题。常见的算法包括SARSA(λ)、DQN和A3C等。</p>
<hr />
<table>
<colgroup>
<col style="width: 23%" />
<col style="width: 36%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr>
<th>对比维度</th>
<th>TD(0)（时序差分）</th>
<th>LSTD（最小二乘TD）</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>方法类型</strong></td>
<td>增量式更新算法</td>
<td>批量方法 / 解线性方程组</td>
</tr>
<tr>
<td><strong>是否使用采样</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>是否使用bootstrapping</strong></td>
<td>是</td>
<td>是</td>
</tr>
<tr>
<td><strong>收敛速度</strong></td>
<td>慢，需多次遍历</td>
<td>快，一次遍历即可</td>
</tr>
<tr>
<td><strong>收敛稳定性</strong></td>
<td>对学习率敏感，较不稳定</td>
<td>稳定性高</td>
</tr>
<tr>
<td><strong>是否需要学习率（α）</strong></td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td><strong>是否需要存储所有数据</strong></td>
<td>否（可在线更新）</td>
<td>是（需存储一批数据计算矩阵）</td>
</tr>
<tr>
<td><strong>估计误差类型</strong></td>
<td>噪声大，估计不准确</td>
<td>最小化TD误差平方，估计精确</td>
</tr>
<tr>
<td><strong>函数逼近类型</strong></td>
<td>支持线性与非线性</td>
<td>仅适用于线性函数逼近</td>
</tr>
<tr>
<td><strong>计算代价</strong></td>
<td>低（每步更新）</td>
<td>高（求解矩阵逆）</td>
</tr>
<tr>
<td><strong>是否可在线使用</strong></td>
<td>是</td>
<td>否（需收集一批样本）</td>
</tr>
<tr>
<td><strong>是否适合大规模状态空间</strong></td>
<td>是</td>
<td>否（矩阵求逆开销大，维度高时困难）</td>
</tr>
<tr>
<td><strong>常见用途</strong></td>
<td>在线学习、环境交互</td>
<td>批量预测、策略评估</td>
</tr>
<tr>
<td><strong>结果可解释性</strong></td>
<td>随机性大，结果随步长波动</td>
<td>更平滑、可解释性好</td>
</tr>
</tbody>
</table>
<h3 id="总结推荐">总结推荐：</h3>
<ul>
<li><strong>TD(0)</strong>：适合在线学习、环境交互、资源有限时使用。算法简单，适合实时任务。</li>
<li><strong>LSTD</strong>：适合批量数据分析、预测任务，或者需要快速稳定收敛的场景，但计算复杂度高，适合线性逼近小维度问题。</li>
</ul>
<hr />

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag"># 强化学习</a>
              <a href="/tags/%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93/" rel="tag"># 笔记总结</a>
              <a href="/tags/%E4%B9%A6%E6%9C%AC%E5%AD%A6%E4%B9%A0/" rel="tag"># 书本学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/03/29/KKT%E6%9D%A1%E4%BB%B6/" rel="prev" title="KKT条件">
      <i class="fa fa-chevron-left"></i> KKT条件
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#on-policy-%E5%92%8C-off-policy-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">1.</span> <span class="nav-text">On-policy 和 Off-policy
的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="nav-number">2.</span> <span class="nav-text">三种方法对比总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E7%AD%96%E7%95%A5-vs.-%E7%A6%BB%E8%BD%A8%E7%AD%96%E7%95%A5"><span class="nav-number">3.</span> <span class="nav-text">同轨策略 vs. 离轨策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88sarsa%E8%BF%9B%E8%A1%8C%E7%9A%84%E6%98%AF%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E8%80%8C%E9%9D%9E%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E4%BC%B0%E8%AE%A1"><span class="nav-number">3.1.</span> <span class="nav-text">为什么SARSA进行的是策略学习而非状态价值估计？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sarsa%E4%B8%8E%E7%8A%B6%E6%80%81%E4%BB%B7%E5%80%BC%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">3.2.</span> <span class="nav-text">SARSA与状态价值估计的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.3.</span> <span class="nav-text">总结：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8-td-%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">3.4.</span> <span class="nav-text">常用 TD 方法的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%AF%94%E8%AF%B4%E6%98%8E"><span class="nav-number">3.5.</span> <span class="nav-text">对比说明：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dyna-q-%E5%8F%8A%E5%85%B6%E4%B8%BB%E8%A6%81%E5%8F%98%E4%BD%93%E5%AF%B9%E6%AF%94%E8%A1%A8"><span class="nav-number">4.</span> <span class="nav-text">Dyna-Q 及其主要变体对比表</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dyna-q"><span class="nav-number">4.1.</span> <span class="nav-text">1. Dyna-Q</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dyna-q-1"><span class="nav-number">4.2.</span> <span class="nav-text">2. Dyna-Q+</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prioritized-dyna-q"><span class="nav-number">4.3.</span> <span class="nav-text">3. Prioritized Dyna-Q</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dyna-2"><span class="nav-number">4.4.</span> <span class="nav-text">4. Dyna-2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91%E7%89%88%E6%9C%AC%E7%BA%BF%E6%80%A7-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.5.</span> <span class="nav-text">5. 函数逼近版本（线性 &#x2F;
神经网络）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%8E%A8%E8%8D%90%E4%B8%8D%E5%90%8C%E9%9C%80%E6%B1%82"><span class="nav-number">5.</span> <span class="nav-text">总结推荐（不同需求）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E6%80%BB%E7%BB%93"><span class="nav-number">5.1.</span> <span class="nav-text">第一部分总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E7%AD%96%E7%95%A5on-policy"><span class="nav-number">5.2.</span> <span class="nav-text">1.
同轨策略（On-Policy）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91function-approximation"><span class="nav-number">5.3.</span> <span class="nav-text">2. 函数逼近（Function
Approximation）：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E7%AD%96%E7%95%A5%E9%A2%84%E6%B5%8B%E7%9A%84%E5%86%85%E5%AE%B9"><span class="nav-number">5.4.</span> <span class="nav-text">3.
同轨策略预测的内容：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8td%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%90%8C%E8%BD%A8%E7%AD%96%E7%95%A5%E9%A2%84%E6%B5%8B"><span class="nav-number">5.4.1.</span> <span class="nav-text">1.
使用TD（时间差分）方法进行同轨策略预测：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91"><span class="nav-number">5.4.2.</span> <span class="nav-text">2. 线性函数逼近：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0%E9%80%BC%E8%BF%91%E4%BE%8B%E5%A6%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.4.3.</span> <span class="nav-text">3.
非线性函数逼近（例如神经网络）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%E5%92%8C%E6%9B%B4%E6%96%B0"><span class="nav-number">5.4.4.</span> <span class="nav-text">4. 策略评估和更新：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%9B%B4%E6%96%B0"><span class="nav-number">5.4.5.</span> <span class="nav-text">5. 基于梯度的更新：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E8%BD%A8%E7%AD%96%E7%95%A5%E9%A2%84%E6%B5%8B%E7%9A%84%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95"><span class="nav-number">5.5.</span> <span class="nav-text">4.
同轨策略预测的经典算法：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#sarsa%CE%BB"><span class="nav-number">5.5.1.</span> <span class="nav-text">(1) SARSA(λ)：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6q%E5%AD%A6%E4%B9%A0dqn"><span class="nav-number">5.5.2.</span> <span class="nav-text">(2) 深度Q学习（DQN）：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#a3c%E5%BC%82%E6%AD%A5%E4%BC%98%E5%8A%BF%E6%BC%94%E5%91%98-%E8%AF%84%E8%AE%BA%E5%AE%B6"><span class="nav-number">5.5.3.</span> <span class="nav-text">(3)
A3C（异步优势演员-评论家）：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9%E5%88%86%E6%9E%90"><span class="nav-number">5.6.</span> <span class="nav-text">5. 优缺点分析：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E7%82%B9"><span class="nav-number">5.6.1.</span> <span class="nav-text">优点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-number">5.6.2.</span> <span class="nav-text">缺点：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">5.7.</span> <span class="nav-text">总结：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%8E%A8%E8%8D%90"><span class="nav-number">5.8.</span> <span class="nav-text">总结推荐：</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhicheng"
      src="/images/cheng.gif">
  <p class="site-author-name" itemprop="name">Zhicheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/zhichengkou" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zhichengkou" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/weixin_46473460?type=blog" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_46473460?type&#x3D;blog" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>CSDN</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com/citations?user=gMknN-8AAAAJ&hl=zh-CN&oi=ao" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com&#x2F;citations?user&#x3D;gMknN-8AAAAJ&amp;hl&#x3D;zh-CN&amp;oi&#x3D;ao" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://500px.com.cn/cheng1917" title="500px → https:&#x2F;&#x2F;500px.com.cn&#x2F;cheng1917" rel="noopener" target="_blank"><i class="fab fa-skype fa-fw"></i>500px</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhicheng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>






<script data-pjax>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              leancloudSelector(url).innerText = 'Counter not initialized! More info at console err msg.';
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":null,"app_key":null,"server_url":null,"security":true};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://zhichengkou.github.io/2025/04/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/',]
      });
      });
  </script>

    </div>
</body>
</html>
